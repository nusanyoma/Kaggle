{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to Sergii for this kernel:\n",
    "\n",
    "https://www.kaggle.com/criskiev/distance-is-all-you-need-lb-1-481. \n",
    "\n",
    "I just changed Folds to 5 from 3 and number of iterations to 6000 from 1500. The lb improved to -1.643 from -1.481"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Idea\n",
    "\n",
    "Despite a lot of creeping Physics and Chemistry knowledge introduced in the description, this competition is more about Geometry and pattern matching.\n",
    "\n",
    "The hypothesis of this kernel is next:\n",
    "1. If we have two similar sets of atoms with the same distances between them and the same types - the scalar coupling constant should be very close.\n",
    "2. More closest atoms to the pair of atoms under prediction have higher influence on scalar coupling constant then those with higher distance\n",
    "\n",
    "So, basically, this problem could be dealt with some kind of K-Nearest Neighbor algorithm or any tree-based - e.g. LightGBM, in case we can find some representation which would describe similar configurations with similar feature sets.\n",
    "\n",
    "Each atom is described with 3 cartesian coordinates. This representation is not stable. Each coupling pair is located in a different point in space and two similar coupling sets would have very different X,Y,Z.\n",
    "\n",
    "So, instead of using coordinates let's consider next system:\n",
    "1. Take each pair of atoms as two first core atoms\n",
    "2. Calculate the center between the pair\n",
    "3. Find all n-nearest atoms to the center (excluding first two atoms)\n",
    "4. Take two closest atoms from step 3 - they will be 3rd and 4th core atoms\n",
    "5. Calculate the distances from 4 core atoms to the rest of the atoms and to the core atoms as well\n",
    "\n",
    "Using this representation each atom position can be described by 4 distances from the core atoms. This representation is stable to rotation and translation. And it's suitable for pattern-matching. So, we can take a sequence of atoms, describe each by 4 distances + atom type(H,O,etc) and looking up for the same pattern we can find similar configurations and detect scalar coupling constant.\n",
    "\n",
    "Here I used LightGBM, because sklearn KNN can't deal with the amount of data. My blind guess is that hand-crafted KNN can outperform LightGBM.\n",
    "\n",
    "Let's code the solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import lightgbm as lgb\n",
    "import os\n",
    "# os.listdir('../input/imputed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../input'\n",
    "SUBMISSIONS_PATH = './'\n",
    "# use atomic numbers to recode atomic names\n",
    "ATOMIC_NUMBERS = {\n",
    "    'H': 1,\n",
    "    'C': 6,\n",
    "    'N': 7,\n",
    "    'O': 8,\n",
    "    'F': 9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', 120)\n",
    "pd.set_option('display.max_columns', 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default all data is read as `float64` and `int64`. We can trade this uneeded precision for memory and higher prediction speed. So, let's read with Pandas all the data in the minimal representation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/numatakouhei/.pyenv/versions/anaconda3-5.0.1/envs/py3.6/lib/python3.6/site-packages/numpy/lib/arraysetops.py:472: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_index</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.807602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.257000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.807404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.809303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "      <td>-11.254300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "      <td>84.809502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    molecule_index  atom_index_0  atom_index_1  type  scalar_coupling_constant\n",
       "id                                                                            \n",
       "0   1               1             0             1JHC  84.807602               \n",
       "1   1               1             2             2JHH -11.257000               \n",
       "2   1               1             3             2JHH -11.254800               \n",
       "3   1               1             4             2JHH -11.254300               \n",
       "4   1               2             0             1JHC  84.807404               \n",
       "5   1               2             3             2JHH -11.254100               \n",
       "6   1               2             4             2JHH -11.254800               \n",
       "7   1               3             0             1JHC  84.809303               \n",
       "8   1               3             4             2JHH -11.254300               \n",
       "9   1               4             0             1JHC  84.809502               "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dtypes = {\n",
    "    'molecule_name': 'category',\n",
    "    'atom_index_0': 'int8',\n",
    "    'atom_index_1': 'int8',\n",
    "    'type': 'category',\n",
    "    'scalar_coupling_constant': 'float32'\n",
    "}\n",
    "train_csv = pd.read_csv('champs-scalar-coupling/train.csv', index_col='id', dtype=train_dtypes)\n",
    "train_csv['molecule_index'] = train_csv.molecule_name.str.replace('dsgdb9nsd_', '').astype('int32')\n",
    "train_csv = train_csv[['molecule_index', 'atom_index_0', 'atom_index_1', 'type', 'scalar_coupling_constant']]\n",
    "train_csv.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (4658147, 5)\n",
      "Total:  88505177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index                       37265176\n",
       "molecule_index              18632588\n",
       "atom_index_0                4658147 \n",
       "atom_index_1                4658147 \n",
       "type                        4658531 \n",
       "scalar_coupling_constant    18632588\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Shape: ', train_csv.shape)\n",
    "print('Total: ', train_csv.memory_usage().sum())\n",
    "train_csv.memory_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/numatakouhei/.pyenv/versions/anaconda3-5.0.1/envs/py3.6/lib/python3.6/site-packages/numpy/lib/arraysetops.py:472: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "submission_csv = pd.read_csv('champs-scalar-coupling/sample_submission.csv', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/numatakouhei/.pyenv/versions/anaconda3-5.0.1/envs/py3.6/lib/python3.6/site-packages/numpy/lib/arraysetops.py:472: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_index</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4658147</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658148</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658149</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3JHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658150</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658151</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658152</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658153</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3JHC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658154</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2JHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658155</th>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2JHH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658156</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1JHC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         molecule_index  atom_index_0  atom_index_1  type\n",
       "id                                                       \n",
       "4658147  4               2             0             2JHC\n",
       "4658148  4               2             1             1JHC\n",
       "4658149  4               2             3             3JHH\n",
       "4658150  4               3             0             1JHC\n",
       "4658151  4               3             1             2JHC\n",
       "4658152  15              3             0             1JHC\n",
       "4658153  15              3             2             3JHC\n",
       "4658154  15              3             4             2JHH\n",
       "4658155  15              3             5             2JHH\n",
       "4658156  15              4             0             1JHC"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_csv = pd.read_csv('champs-scalar-coupling/test.csv', index_col='id', dtype=train_dtypes)\n",
    "test_csv['molecule_index'] = test_csv['molecule_name'].str.replace('dsgdb9nsd_', '').astype('int32')\n",
    "test_csv = test_csv[['molecule_index', 'atom_index_0', 'atom_index_1', 'type']]\n",
    "test_csv.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_index</th>\n",
       "      <th>atom_index</th>\n",
       "      <th>atom</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.012698</td>\n",
       "      <td>1.085804</td>\n",
       "      <td>0.008001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>-0.006031</td>\n",
       "      <td>0.001976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.011731</td>\n",
       "      <td>1.463751</td>\n",
       "      <td>0.000277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.540815</td>\n",
       "      <td>1.447527</td>\n",
       "      <td>-0.876644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.523814</td>\n",
       "      <td>1.437933</td>\n",
       "      <td>0.906397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.040426</td>\n",
       "      <td>1.024108</td>\n",
       "      <td>0.062564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017257</td>\n",
       "      <td>0.012545</td>\n",
       "      <td>-0.027377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.915789</td>\n",
       "      <td>1.358745</td>\n",
       "      <td>-0.028758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.520278</td>\n",
       "      <td>1.343532</td>\n",
       "      <td>-0.775543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.034360</td>\n",
       "      <td>0.977540</td>\n",
       "      <td>0.007602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   molecule_index  atom_index  atom         x         y         z\n",
       "0  1               0           6    -0.012698  1.085804  0.008001\n",
       "1  1               1           1     0.002150 -0.006031  0.001976\n",
       "2  1               2           1     1.011731  1.463751  0.000277\n",
       "3  1               3           1    -0.540815  1.447527 -0.876644\n",
       "4  1               4           1    -0.523814  1.437933  0.906397\n",
       "5  2               0           7    -0.040426  1.024108  0.062564\n",
       "6  2               1           1     0.017257  0.012545 -0.027377\n",
       "7  2               2           1     0.915789  1.358745 -0.028758\n",
       "8  2               3           1    -0.520278  1.343532 -0.775543\n",
       "9  3               0           8    -0.034360  0.977540  0.007602"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_dtypes = {\n",
    "    'molecule_name': 'category',\n",
    "    'atom_index': 'int8',\n",
    "    'atom': 'category',\n",
    "    'x': 'float32',\n",
    "    'y': 'float32',\n",
    "    'z': 'float32'\n",
    "}\n",
    "structures_csv = pd.read_csv('champs-scalar-coupling/structures.csv', dtype=structures_dtypes)\n",
    "structures_csv['molecule_index'] = structures_csv.molecule_name.str.replace('dsgdb9nsd_', '').astype('int32')\n",
    "structures_csv = structures_csv[['molecule_index', 'atom_index', 'atom', 'x', 'y', 'z']]\n",
    "structures_csv['atom'] = structures_csv['atom'].replace(ATOMIC_NUMBERS).astype('int8')\n",
    "structures_csv.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape:  (2358657, 6)\n",
      "Total:  42455906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index             80     \n",
       "molecule_index    9434628\n",
       "atom_index        2358657\n",
       "atom              2358657\n",
       "x                 9434628\n",
       "y                 9434628\n",
       "z                 9434628\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Shape: ', structures_csv.shape)\n",
    "print('Total: ', structures_csv.memory_usage().sum())\n",
    "structures_csv.memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Distance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_type_dataframes(base, structures, coupling_type):\n",
    "    base = base[base['type'] == coupling_type].drop('type', axis=1).copy()\n",
    "    base = base.reset_index()\n",
    "    base['id'] = base['id'].astype('int32')\n",
    "    structures = structures[structures['molecule_index'].isin(base['molecule_index'])]\n",
    "    return base, structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_coordinates(base, structures, index):\n",
    "    df = pd.merge(base, structures, how='inner',\n",
    "                  left_on=['molecule_index', f'atom_index_{index}'],\n",
    "                  right_on=['molecule_index', 'atom_index']).drop(['atom_index'], axis=1)\n",
    "    df = df.rename(columns={\n",
    "        'atom': f'atom_{index}',\n",
    "        'x': f'x_{index}',\n",
    "        'y': f'y_{index}',\n",
    "        'z': f'z_{index}'\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_atoms(base, atoms):\n",
    "    df = pd.merge(base, atoms, how='inner',\n",
    "                  on=['molecule_index', 'atom_index_0', 'atom_index_1'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_atoms(base, structures):\n",
    "    df = pd.merge(base, structures, how='left',\n",
    "                  left_on=['molecule_index'],\n",
    "                  right_on=['molecule_index'])\n",
    "    df = df[(df.atom_index_0 != df.atom_index) & (df.atom_index_1 != df.atom_index)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_center(df):\n",
    "    df['x_c'] = ((df['x_1'] + df['x_0']) * np.float32(0.5))\n",
    "    df['y_c'] = ((df['y_1'] + df['y_0']) * np.float32(0.5))\n",
    "    df['z_c'] = ((df['z_1'] + df['z_0']) * np.float32(0.5))\n",
    "\n",
    "def add_distance_to_center(df):\n",
    "    df['d_c'] = ((\n",
    "        (df['x_c'] - df['x'])**np.float32(2) +\n",
    "        (df['y_c'] - df['y'])**np.float32(2) + \n",
    "        (df['z_c'] - df['z'])**np.float32(2)\n",
    "    )**np.float32(0.5))\n",
    "\n",
    "def add_distance_between(df, suffix1, suffix2):\n",
    "    df[f'd_{suffix1}_{suffix2}'] = ((\n",
    "        (df[f'x_{suffix1}'] - df[f'x_{suffix2}'])**np.float32(2) +\n",
    "        (df[f'y_{suffix1}'] - df[f'y_{suffix2}'])**np.float32(2) + \n",
    "        (df[f'z_{suffix1}'] - df[f'z_{suffix2}'])**np.float32(2)\n",
    "    )**np.float32(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distances(df):\n",
    "    n_atoms = 1 + max([int(c.split('_')[1]) for c in df.columns if c.startswith('x_')])\n",
    "    \n",
    "    for i in range(1, n_atoms):\n",
    "        for vi in range(min(4, i)):\n",
    "            add_distance_between(df, i, vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_n_atoms(base, structures):\n",
    "    dfs = structures['molecule_index'].value_counts().rename('n_atoms').to_frame()\n",
    "    return pd.merge(base, dfs, left_on='molecule_index', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_couple_dataframe(some_csv, structures_csv, coupling_type, n_atoms=10):\n",
    "    base, structures = build_type_dataframes(some_csv, structures_csv, coupling_type)\n",
    "    base = add_coordinates(base, structures, 0)\n",
    "    base = add_coordinates(base, structures, 1)\n",
    "    \n",
    "    base = base.drop(['atom_0', 'atom_1'], axis=1)\n",
    "    atoms = base.drop('id', axis=1).copy()\n",
    "    if 'scalar_coupling_constant' in some_csv:\n",
    "        atoms = atoms.drop(['scalar_coupling_constant'], axis=1)\n",
    "        \n",
    "    add_center(atoms)\n",
    "    atoms = atoms.drop(['x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1'], axis=1)\n",
    "\n",
    "    atoms = merge_all_atoms(atoms, structures)\n",
    "    \n",
    "    add_distance_to_center(atoms)\n",
    "    \n",
    "    atoms = atoms.drop(['x_c', 'y_c', 'z_c', 'atom_index'], axis=1)\n",
    "    atoms.sort_values(['molecule_index', 'atom_index_0', 'atom_index_1', 'd_c'], inplace=True)\n",
    "    atom_groups = atoms.groupby(['molecule_index', 'atom_index_0', 'atom_index_1'])\n",
    "    atoms['num'] = atom_groups.cumcount() + 2\n",
    "    atoms = atoms.drop(['d_c'], axis=1)\n",
    "    atoms = atoms[atoms['num'] < n_atoms]\n",
    "\n",
    "    atoms = atoms.set_index(['molecule_index', 'atom_index_0', 'atom_index_1', 'num']).unstack()\n",
    "    atoms.columns = [f'{col[0]}_{col[1]}' for col in atoms.columns]\n",
    "    atoms = atoms.reset_index()\n",
    "    \n",
    "    # downcast back to int8\n",
    "    for col in atoms.columns:\n",
    "        if col.startswith('atom_'):\n",
    "            atoms[col] = atoms[col].fillna(0).astype('int8')\n",
    "            \n",
    "    atoms['molecule_index'] = atoms['molecule_index'].astype('int32')\n",
    "    \n",
    "    full = add_atoms(base, atoms)\n",
    "    add_distances(full)\n",
    "    \n",
    "    full.sort_values('id', inplace=True)\n",
    "    \n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_n_atoms(df, n_atoms, four_start=4):\n",
    "    labels = []\n",
    "    for i in range(2, n_atoms):\n",
    "        label = f'atom_{i}'\n",
    "        labels.append(label)\n",
    "\n",
    "    for i in range(n_atoms):\n",
    "        num = min(i, 4) if i < four_start else 4\n",
    "        for j in range(num):\n",
    "            labels.append(f'd_{i}_{j}')\n",
    "    if 'scalar_coupling_constant' in df:\n",
    "        labels.append('scalar_coupling_constant')\n",
    "    return df[labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check LightGBM with the smallest type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43363, 73)\n",
      "CPU times: user 2.98 s, sys: 639 ms, total: 3.61 s\n",
      "Wall time: 3.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full = build_couple_dataframe(train_csv, structures_csv, '1JHN', n_atoms=10)\n",
    "print(full.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't calculate distances for `d_0_x`, `d_1_1`, `d_2_2`, `d_2_3`, `d_3_3` because we already have them in later atoms(`d_0_1` == `d_1_0`) or they are equal to zeros(e.g. `d_1_1`, `d_2_2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'molecule_index', 'atom_index_0', 'atom_index_1',\n",
       "       'scalar_coupling_constant', 'x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1',\n",
       "       'atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8',\n",
       "       'atom_9', 'x_2', 'x_3', 'x_4', 'x_5', 'x_6', 'x_7', 'x_8', 'x_9', 'y_2',\n",
       "       'y_3', 'y_4', 'y_5', 'y_6', 'y_7', 'y_8', 'y_9', 'z_2', 'z_3', 'z_4',\n",
       "       'z_5', 'z_6', 'z_7', 'z_8', 'z_9', 'd_1_0', 'd_2_0', 'd_2_1', 'd_3_0',\n",
       "       'd_3_1', 'd_3_2', 'd_4_0', 'd_4_1', 'd_4_2', 'd_4_3', 'd_5_0', 'd_5_1',\n",
       "       'd_5_2', 'd_5_3', 'd_6_0', 'd_6_1', 'd_6_2', 'd_6_3', 'd_7_0', 'd_7_1',\n",
       "       'd_7_2', 'd_7_3', 'd_8_0', 'd_8_1', 'd_8_2', 'd_8_3', 'd_9_0', 'd_9_1',\n",
       "       'd_9_2', 'd_9_3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For experiments, full dataset can be built with higher number of atoms, and for building a training/validation sets we can trim them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'd_1_0', 'd_2_0',\n",
       "       'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0', 'd_4_1', 'd_4_2', 'd_4_3',\n",
       "       'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0', 'd_6_1', 'd_6_2', 'd_6_3',\n",
       "       'scalar_coupling_constant'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = take_n_atoms(full, 7)\n",
    "# LightGBM performs better with 0-s then with NaN-s\n",
    "df = df.fillna(0)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((34690, 23), (8673, 23), (34690,), (8673,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data = df.drop(['scalar_coupling_constant'], axis=1).values.astype('float32')\n",
    "y_data = df['scalar_coupling_constant'].values.astype('float32')\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.2, random_state=128)\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration params are copied from @artgor kernel:\n",
    "# https://www.kaggle.com/artgor/brute-force-feature-engineering\n",
    "LGB_PARAMS = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'mae',\n",
    "    'verbosity': -1,\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.2,\n",
    "    'num_leaves': 128,\n",
    "    'min_child_samples': 79,\n",
    "    'max_depth': 9,\n",
    "    'subsample_freq': 1,\n",
    "    'subsample': 0.9,\n",
    "    'bagging_seed': 11,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 0.3,\n",
    "    'colsample_bytree': 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.428523\tvalid_1's l1: 0.52994\n",
      "[200]\ttraining's l1: 0.350432\tvalid_1's l1: 0.478677\n",
      "[300]\ttraining's l1: 0.304859\tvalid_1's l1: 0.453902\n",
      "[400]\ttraining's l1: 0.271607\tvalid_1's l1: 0.437171\n",
      "[500]\ttraining's l1: 0.244988\tvalid_1's l1: 0.426053\n",
      "[600]\ttraining's l1: 0.223136\tvalid_1's l1: 0.417847\n",
      "[700]\ttraining's l1: 0.205102\tvalid_1's l1: 0.411523\n",
      "[800]\ttraining's l1: 0.189548\tvalid_1's l1: 0.406439\n",
      "[900]\ttraining's l1: 0.17607\tvalid_1's l1: 0.401809\n",
      "[1000]\ttraining's l1: 0.163495\tvalid_1's l1: 0.398227\n",
      "[1100]\ttraining's l1: 0.152994\tvalid_1's l1: 0.395416\n",
      "[1200]\ttraining's l1: 0.143626\tvalid_1's l1: 0.393002\n",
      "[1300]\ttraining's l1: 0.134844\tvalid_1's l1: 0.391189\n",
      "[1400]\ttraining's l1: 0.12696\tvalid_1's l1: 0.389281\n",
      "[1500]\ttraining's l1: 0.119718\tvalid_1's l1: 0.387558\n",
      "[1600]\ttraining's l1: 0.11317\tvalid_1's l1: 0.386156\n",
      "[1700]\ttraining's l1: 0.107148\tvalid_1's l1: 0.38502\n",
      "[1800]\ttraining's l1: 0.101517\tvalid_1's l1: 0.383982\n",
      "[1900]\ttraining's l1: 0.0959517\tvalid_1's l1: 0.382974\n",
      "[2000]\ttraining's l1: 0.0911212\tvalid_1's l1: 0.38217\n",
      "[2100]\ttraining's l1: 0.086686\tvalid_1's l1: 0.381198\n",
      "[2200]\ttraining's l1: 0.0825273\tvalid_1's l1: 0.380202\n",
      "[2300]\ttraining's l1: 0.0786831\tvalid_1's l1: 0.379606\n",
      "[2400]\ttraining's l1: 0.0751213\tvalid_1's l1: 0.378908\n",
      "[2500]\ttraining's l1: 0.0717755\tvalid_1's l1: 0.378421\n",
      "[2600]\ttraining's l1: 0.068694\tvalid_1's l1: 0.37802\n",
      "[2700]\ttraining's l1: 0.0655253\tvalid_1's l1: 0.377442\n",
      "[2800]\ttraining's l1: 0.0626971\tvalid_1's l1: 0.377105\n",
      "[2900]\ttraining's l1: 0.0599926\tvalid_1's l1: 0.376794\n",
      "[3000]\ttraining's l1: 0.0574418\tvalid_1's l1: 0.376538\n",
      "[3100]\ttraining's l1: 0.055133\tvalid_1's l1: 0.376232\n",
      "[3200]\ttraining's l1: 0.0528353\tvalid_1's l1: 0.376038\n",
      "[3300]\ttraining's l1: 0.0507569\tvalid_1's l1: 0.375846\n",
      "[3400]\ttraining's l1: 0.0487497\tvalid_1's l1: 0.375586\n",
      "[3500]\ttraining's l1: 0.0468625\tvalid_1's l1: 0.37545\n",
      "[3600]\ttraining's l1: 0.0451649\tvalid_1's l1: 0.375192\n",
      "[3700]\ttraining's l1: 0.0433444\tvalid_1's l1: 0.374991\n",
      "[3800]\ttraining's l1: 0.0417915\tvalid_1's l1: 0.374941\n",
      "[3900]\ttraining's l1: 0.0403073\tvalid_1's l1: 0.374799\n",
      "[4000]\ttraining's l1: 0.0388924\tvalid_1's l1: 0.374738\n",
      "[4100]\ttraining's l1: 0.0375189\tvalid_1's l1: 0.374657\n",
      "[4200]\ttraining's l1: 0.0361616\tvalid_1's l1: 0.374516\n",
      "[4300]\ttraining's l1: 0.0349293\tvalid_1's l1: 0.374391\n",
      "[4400]\ttraining's l1: 0.0337107\tvalid_1's l1: 0.374294\n",
      "[4500]\ttraining's l1: 0.0325878\tvalid_1's l1: 0.374255\n",
      "[4600]\ttraining's l1: 0.0314975\tvalid_1's l1: 0.374257\n",
      "[4700]\ttraining's l1: 0.0304632\tvalid_1's l1: 0.374141\n",
      "[4800]\ttraining's l1: 0.0294367\tvalid_1's l1: 0.374015\n",
      "[4900]\ttraining's l1: 0.0285251\tvalid_1's l1: 0.373936\n",
      "[5000]\ttraining's l1: 0.0275916\tvalid_1's l1: 0.373879\n",
      "[5100]\ttraining's l1: 0.0266945\tvalid_1's l1: 0.373818\n",
      "[5200]\ttraining's l1: 0.0258507\tvalid_1's l1: 0.373802\n",
      "[5300]\ttraining's l1: 0.0250646\tvalid_1's l1: 0.373818\n",
      "[5400]\ttraining's l1: 0.0243119\tvalid_1's l1: 0.373781\n",
      "[5500]\ttraining's l1: 0.0235624\tvalid_1's l1: 0.373748\n",
      "[5600]\ttraining's l1: 0.0228509\tvalid_1's l1: 0.373691\n",
      "[5700]\ttraining's l1: 0.0221661\tvalid_1's l1: 0.373663\n",
      "[5800]\ttraining's l1: 0.0215114\tvalid_1's l1: 0.373628\n",
      "[5900]\ttraining's l1: 0.0208718\tvalid_1's l1: 0.373589\n",
      "[6000]\ttraining's l1: 0.0202415\tvalid_1's l1: 0.373565\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0202415\tvalid_1's l1: 0.373565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.9846644528120927"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LGBMRegressor(**LGB_PARAMS, n_estimators=6000, n_jobs = -1)\n",
    "model.fit(X_train, y_train, \n",
    "        eval_set=[(X_train, y_train), (X_val, y_val)], eval_metric='mae',\n",
    "        verbose=100, early_stopping_rounds=200)\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "np.log(mean_absolute_error(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a bad score for such a simple set of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols = list(df.columns)\n",
    "cols.remove('scalar_coupling_constant')\n",
    "cols\n",
    "df_importance = pd.DataFrame({'feature': cols, 'importance': model.feature_importances_})\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=df_importance.sort_values('importance', ascending=False));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's funny, but looks like atom types aren't used a lot in the final decision. Quite a contrary to what a man would do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_x_y_data(some_csv, coupling_type, n_atoms):\n",
    "    full = build_couple_dataframe(some_csv, structures_csv, coupling_type, n_atoms=n_atoms)\n",
    "    \n",
    "    df = take_n_atoms(full, n_atoms)\n",
    "    df = df.fillna(0)\n",
    "    print(df.columns)\n",
    "    \n",
    "    if 'scalar_coupling_constant' in df:\n",
    "        X_data = df.drop(['scalar_coupling_constant'], axis=1).values.astype('float32')\n",
    "        y_data = df['scalar_coupling_constant'].values.astype('float32')\n",
    "    else:\n",
    "        X_data = df.values.astype('float32')\n",
    "        y_data = None\n",
    "    \n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "\n",
    "def mae_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train_csv.values)\n",
    "    logmae = np.log(mean_absolute_error())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_for_one_coupling_type(coupling_type, submission, n_atoms, n_folds=5, n_splits=5, random_state=44):\n",
    "    print(f'*** Training Model for {coupling_type} ***')\n",
    "    \n",
    "    X_data, y_data = build_x_y_data(train_csv, coupling_type, n_atoms)\n",
    "    X_test, _ = build_x_y_data(test_csv, coupling_type, n_atoms)\n",
    "    y_pred = np.zeros(X_test.shape[0], dtype='float32')\n",
    "\n",
    "    cv_score = 0\n",
    "    \n",
    "    if n_folds > n_splits:\n",
    "        n_splits = n_folds\n",
    "    \n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kfold.split(X_data, y_data)):\n",
    "        if fold >= n_folds:\n",
    "            break\n",
    "\n",
    "        X_train, X_val = X_data[train_index], X_data[val_index]\n",
    "        y_train, y_val = y_data[train_index], y_data[val_index]\n",
    "\n",
    "        model = LGBMRegressor(**LGB_PARAMS, n_estimators=6000, n_jobs = -1)\n",
    "        model.fit(X_train, y_train, \n",
    "            eval_set=[(X_train, y_train), (X_val, y_val)], eval_metric='mae',\n",
    "            verbose=100, early_stopping_rounds=200)\n",
    "\n",
    "        y_val_pred = model.predict(X_val)\n",
    "        val_score = np.log(mean_absolute_error(y_val, y_val_pred))\n",
    "        print(f'{coupling_type} Fold {fold}, logMAE: {val_score}')\n",
    "        \n",
    "        cv_score += val_score / n_folds\n",
    "        y_pred += model.predict(X_test) / n_folds\n",
    "        \n",
    "        \n",
    "    submission.loc[test_csv['type'] == coupling_type, 'scalar_coupling_constant'] = y_pred\n",
    "    return cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a separate model for each type of coupling. Dataset is split into 5 pieces and in this kernel we will use only 3 folds for speed up.\n",
    "\n",
    "Main tuning parameter is the number of atoms. I took good numbers, but accuracy can be improved a bit by tuning them for each type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Training Model for 1JHN ***\n",
      "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'd_1_0', 'd_2_0',\n",
      "       'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0', 'd_4_1', 'd_4_2', 'd_4_3',\n",
      "       'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0', 'd_6_1', 'd_6_2', 'd_6_3',\n",
      "       'scalar_coupling_constant'],\n",
      "      dtype='object')\n",
      "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'd_1_0', 'd_2_0',\n",
      "       'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0', 'd_4_1', 'd_4_2', 'd_4_3',\n",
      "       'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0', 'd_6_1', 'd_6_2', 'd_6_3'],\n",
      "      dtype='object')\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.429627\tvalid_1's l1: 0.532369\n",
      "[200]\ttraining's l1: 0.350126\tvalid_1's l1: 0.478463\n",
      "[300]\ttraining's l1: 0.306173\tvalid_1's l1: 0.454657\n",
      "[400]\ttraining's l1: 0.272005\tvalid_1's l1: 0.438144\n",
      "[500]\ttraining's l1: 0.246723\tvalid_1's l1: 0.427249\n",
      "[600]\ttraining's l1: 0.224672\tvalid_1's l1: 0.419068\n",
      "[700]\ttraining's l1: 0.206798\tvalid_1's l1: 0.412317\n",
      "[800]\ttraining's l1: 0.189642\tvalid_1's l1: 0.406248\n",
      "[900]\ttraining's l1: 0.174627\tvalid_1's l1: 0.402042\n",
      "[1000]\ttraining's l1: 0.162879\tvalid_1's l1: 0.398678\n",
      "[1100]\ttraining's l1: 0.152282\tvalid_1's l1: 0.395597\n",
      "[1200]\ttraining's l1: 0.142648\tvalid_1's l1: 0.393392\n",
      "[1300]\ttraining's l1: 0.133471\tvalid_1's l1: 0.391623\n",
      "[1400]\ttraining's l1: 0.125501\tvalid_1's l1: 0.389912\n",
      "[1500]\ttraining's l1: 0.118313\tvalid_1's l1: 0.388818\n",
      "[1600]\ttraining's l1: 0.1118\tvalid_1's l1: 0.387335\n",
      "[1700]\ttraining's l1: 0.105686\tvalid_1's l1: 0.386319\n",
      "[1800]\ttraining's l1: 0.100187\tvalid_1's l1: 0.385171\n",
      "[1900]\ttraining's l1: 0.0952484\tvalid_1's l1: 0.384215\n",
      "[2000]\ttraining's l1: 0.0904961\tvalid_1's l1: 0.38337\n",
      "[2100]\ttraining's l1: 0.085993\tvalid_1's l1: 0.38262\n",
      "[2200]\ttraining's l1: 0.081754\tvalid_1's l1: 0.38218\n",
      "[2300]\ttraining's l1: 0.0781111\tvalid_1's l1: 0.3816\n",
      "[2400]\ttraining's l1: 0.0743041\tvalid_1's l1: 0.381088\n",
      "[2500]\ttraining's l1: 0.0709992\tvalid_1's l1: 0.380535\n",
      "[2600]\ttraining's l1: 0.0679912\tvalid_1's l1: 0.380062\n",
      "[2700]\ttraining's l1: 0.0651108\tvalid_1's l1: 0.379612\n",
      "[2800]\ttraining's l1: 0.0624077\tvalid_1's l1: 0.379252\n",
      "[2900]\ttraining's l1: 0.0597253\tvalid_1's l1: 0.378911\n",
      "[3000]\ttraining's l1: 0.0572364\tvalid_1's l1: 0.378723\n",
      "[3100]\ttraining's l1: 0.0548926\tvalid_1's l1: 0.378349\n",
      "[3200]\ttraining's l1: 0.0527835\tvalid_1's l1: 0.378067\n",
      "[3300]\ttraining's l1: 0.050697\tvalid_1's l1: 0.377967\n",
      "[3400]\ttraining's l1: 0.0485959\tvalid_1's l1: 0.377737\n",
      "[3500]\ttraining's l1: 0.0467054\tvalid_1's l1: 0.377611\n",
      "[3600]\ttraining's l1: 0.0449837\tvalid_1's l1: 0.377394\n",
      "[3700]\ttraining's l1: 0.0431739\tvalid_1's l1: 0.37727\n",
      "[3800]\ttraining's l1: 0.0415156\tvalid_1's l1: 0.377086\n",
      "[3900]\ttraining's l1: 0.0400489\tvalid_1's l1: 0.376952\n",
      "[4000]\ttraining's l1: 0.0385949\tvalid_1's l1: 0.37685\n",
      "[4100]\ttraining's l1: 0.0372755\tvalid_1's l1: 0.376722\n",
      "[4200]\ttraining's l1: 0.0360195\tvalid_1's l1: 0.376549\n",
      "[4300]\ttraining's l1: 0.0348151\tvalid_1's l1: 0.376499\n",
      "[4400]\ttraining's l1: 0.033561\tvalid_1's l1: 0.376377\n",
      "[4500]\ttraining's l1: 0.0324156\tvalid_1's l1: 0.376351\n",
      "[4600]\ttraining's l1: 0.0312722\tvalid_1's l1: 0.376295\n",
      "[4700]\ttraining's l1: 0.0301431\tvalid_1's l1: 0.376145\n",
      "[4800]\ttraining's l1: 0.0292031\tvalid_1's l1: 0.376196\n",
      "[4900]\ttraining's l1: 0.0282784\tvalid_1's l1: 0.37613\n",
      "Early stopping, best iteration is:\n",
      "[4716]\ttraining's l1: 0.03\tvalid_1's l1: 0.376124\n",
      "1JHN Fold 0, logMAE: -0.9778369873444822\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.421223\tvalid_1's l1: 0.510444\n",
      "[200]\ttraining's l1: 0.346247\tvalid_1's l1: 0.465047\n",
      "[300]\ttraining's l1: 0.297463\tvalid_1's l1: 0.439799\n",
      "[400]\ttraining's l1: 0.265651\tvalid_1's l1: 0.426714\n",
      "[500]\ttraining's l1: 0.240292\tvalid_1's l1: 0.416476\n",
      "[600]\ttraining's l1: 0.218757\tvalid_1's l1: 0.409478\n",
      "[700]\ttraining's l1: 0.201557\tvalid_1's l1: 0.403974\n",
      "[800]\ttraining's l1: 0.186355\tvalid_1's l1: 0.398701\n",
      "[900]\ttraining's l1: 0.173049\tvalid_1's l1: 0.394715\n",
      "[1000]\ttraining's l1: 0.160559\tvalid_1's l1: 0.391703\n",
      "[1100]\ttraining's l1: 0.149921\tvalid_1's l1: 0.389289\n",
      "[1200]\ttraining's l1: 0.140959\tvalid_1's l1: 0.387101\n",
      "[1300]\ttraining's l1: 0.132305\tvalid_1's l1: 0.384923\n",
      "[1400]\ttraining's l1: 0.124316\tvalid_1's l1: 0.383308\n",
      "[1500]\ttraining's l1: 0.117612\tvalid_1's l1: 0.382001\n",
      "[1600]\ttraining's l1: 0.111214\tvalid_1's l1: 0.380885\n",
      "[1700]\ttraining's l1: 0.105566\tvalid_1's l1: 0.380014\n",
      "[1800]\ttraining's l1: 0.100228\tvalid_1's l1: 0.379151\n",
      "[1900]\ttraining's l1: 0.0953539\tvalid_1's l1: 0.378363\n",
      "[2000]\ttraining's l1: 0.0905192\tvalid_1's l1: 0.377364\n",
      "[2100]\ttraining's l1: 0.0856974\tvalid_1's l1: 0.376546\n",
      "[2200]\ttraining's l1: 0.0818169\tvalid_1's l1: 0.376044\n",
      "[2300]\ttraining's l1: 0.0779446\tvalid_1's l1: 0.375521\n",
      "[2400]\ttraining's l1: 0.0744084\tvalid_1's l1: 0.375086\n",
      "[2500]\ttraining's l1: 0.0710683\tvalid_1's l1: 0.374688\n",
      "[2600]\ttraining's l1: 0.067878\tvalid_1's l1: 0.374279\n",
      "[2700]\ttraining's l1: 0.0649514\tvalid_1's l1: 0.373785\n",
      "[2800]\ttraining's l1: 0.0622408\tvalid_1's l1: 0.373385\n",
      "[2900]\ttraining's l1: 0.0597377\tvalid_1's l1: 0.373066\n",
      "[3000]\ttraining's l1: 0.0573188\tvalid_1's l1: 0.372742\n",
      "[3100]\ttraining's l1: 0.0548614\tvalid_1's l1: 0.372462\n",
      "[3200]\ttraining's l1: 0.0526672\tvalid_1's l1: 0.37214\n",
      "[3300]\ttraining's l1: 0.0506384\tvalid_1's l1: 0.371883\n",
      "[3400]\ttraining's l1: 0.0485688\tvalid_1's l1: 0.371597\n",
      "[3500]\ttraining's l1: 0.0466935\tvalid_1's l1: 0.371423\n",
      "[3600]\ttraining's l1: 0.0448895\tvalid_1's l1: 0.371155\n",
      "[3700]\ttraining's l1: 0.0431605\tvalid_1's l1: 0.370896\n",
      "[3800]\ttraining's l1: 0.0415203\tvalid_1's l1: 0.370789\n",
      "[3900]\ttraining's l1: 0.039944\tvalid_1's l1: 0.370703\n",
      "[4000]\ttraining's l1: 0.0385114\tvalid_1's l1: 0.370666\n",
      "[4100]\ttraining's l1: 0.0371846\tvalid_1's l1: 0.370559\n",
      "[4200]\ttraining's l1: 0.0358918\tvalid_1's l1: 0.37043\n",
      "[4300]\ttraining's l1: 0.0347188\tvalid_1's l1: 0.370261\n",
      "[4400]\ttraining's l1: 0.0334818\tvalid_1's l1: 0.370218\n",
      "[4500]\ttraining's l1: 0.0323888\tvalid_1's l1: 0.370047\n",
      "[4600]\ttraining's l1: 0.0312976\tvalid_1's l1: 0.370024\n",
      "[4700]\ttraining's l1: 0.0303148\tvalid_1's l1: 0.369965\n",
      "[4800]\ttraining's l1: 0.0293419\tvalid_1's l1: 0.369898\n",
      "[4900]\ttraining's l1: 0.028403\tvalid_1's l1: 0.369796\n",
      "[5000]\ttraining's l1: 0.0274677\tvalid_1's l1: 0.369746\n",
      "[5100]\ttraining's l1: 0.0265932\tvalid_1's l1: 0.369676\n",
      "[5200]\ttraining's l1: 0.0257928\tvalid_1's l1: 0.369609\n",
      "[5300]\ttraining's l1: 0.0249894\tvalid_1's l1: 0.369579\n",
      "[5400]\ttraining's l1: 0.0242589\tvalid_1's l1: 0.369501\n",
      "[5500]\ttraining's l1: 0.0235097\tvalid_1's l1: 0.369461\n",
      "[5600]\ttraining's l1: 0.0228203\tvalid_1's l1: 0.369431\n",
      "[5700]\ttraining's l1: 0.0221389\tvalid_1's l1: 0.369396\n",
      "[5800]\ttraining's l1: 0.0214825\tvalid_1's l1: 0.369314\n",
      "[5900]\ttraining's l1: 0.0209058\tvalid_1's l1: 0.369271\n",
      "[6000]\ttraining's l1: 0.0203343\tvalid_1's l1: 0.36923\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0203343\tvalid_1's l1: 0.36923\n",
      "1JHN Fold 1, logMAE: -0.9963350535659782\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.427891\tvalid_1's l1: 0.517766\n",
      "[200]\ttraining's l1: 0.352724\tvalid_1's l1: 0.46904\n",
      "[300]\ttraining's l1: 0.310797\tvalid_1's l1: 0.445945\n",
      "[400]\ttraining's l1: 0.275858\tvalid_1's l1: 0.431889\n",
      "[500]\ttraining's l1: 0.248381\tvalid_1's l1: 0.420789\n",
      "[600]\ttraining's l1: 0.227797\tvalid_1's l1: 0.413268\n",
      "[700]\ttraining's l1: 0.209144\tvalid_1's l1: 0.407321\n",
      "[800]\ttraining's l1: 0.192529\tvalid_1's l1: 0.403412\n",
      "[900]\ttraining's l1: 0.177982\tvalid_1's l1: 0.398898\n",
      "[1000]\ttraining's l1: 0.165585\tvalid_1's l1: 0.395693\n",
      "[1100]\ttraining's l1: 0.154779\tvalid_1's l1: 0.393132\n",
      "[1200]\ttraining's l1: 0.14474\tvalid_1's l1: 0.390403\n",
      "[1300]\ttraining's l1: 0.135846\tvalid_1's l1: 0.388323\n",
      "[1400]\ttraining's l1: 0.127712\tvalid_1's l1: 0.386401\n",
      "[1500]\ttraining's l1: 0.120617\tvalid_1's l1: 0.384962\n",
      "[1600]\ttraining's l1: 0.113301\tvalid_1's l1: 0.383591\n",
      "[1700]\ttraining's l1: 0.107066\tvalid_1's l1: 0.382124\n",
      "[1800]\ttraining's l1: 0.101339\tvalid_1's l1: 0.380795\n",
      "[1900]\ttraining's l1: 0.0962073\tvalid_1's l1: 0.380125\n",
      "[2000]\ttraining's l1: 0.0915934\tvalid_1's l1: 0.379427\n",
      "[2100]\ttraining's l1: 0.0870533\tvalid_1's l1: 0.378367\n",
      "[2200]\ttraining's l1: 0.0829658\tvalid_1's l1: 0.377819\n",
      "[2300]\ttraining's l1: 0.0790227\tvalid_1's l1: 0.376988\n",
      "[2400]\ttraining's l1: 0.0753824\tvalid_1's l1: 0.376376\n",
      "[2500]\ttraining's l1: 0.0718047\tvalid_1's l1: 0.375979\n",
      "[2600]\ttraining's l1: 0.0686997\tvalid_1's l1: 0.375469\n",
      "[2700]\ttraining's l1: 0.0656265\tvalid_1's l1: 0.375055\n",
      "[2800]\ttraining's l1: 0.0628833\tvalid_1's l1: 0.374606\n",
      "[2900]\ttraining's l1: 0.0602417\tvalid_1's l1: 0.37398\n",
      "[3000]\ttraining's l1: 0.0576874\tvalid_1's l1: 0.37376\n",
      "[3100]\ttraining's l1: 0.0552652\tvalid_1's l1: 0.373637\n",
      "[3200]\ttraining's l1: 0.0529737\tvalid_1's l1: 0.373327\n",
      "[3300]\ttraining's l1: 0.0509312\tvalid_1's l1: 0.373062\n",
      "[3400]\ttraining's l1: 0.0489159\tvalid_1's l1: 0.372846\n",
      "[3500]\ttraining's l1: 0.0470478\tvalid_1's l1: 0.372568\n",
      "[3600]\ttraining's l1: 0.0451725\tvalid_1's l1: 0.372236\n",
      "[3700]\ttraining's l1: 0.0433081\tvalid_1's l1: 0.371998\n",
      "[3800]\ttraining's l1: 0.0417008\tvalid_1's l1: 0.371833\n",
      "[3900]\ttraining's l1: 0.0402191\tvalid_1's l1: 0.371613\n",
      "[4000]\ttraining's l1: 0.038829\tvalid_1's l1: 0.371528\n",
      "[4100]\ttraining's l1: 0.0373816\tvalid_1's l1: 0.371327\n",
      "[4200]\ttraining's l1: 0.0360764\tvalid_1's l1: 0.371286\n",
      "[4300]\ttraining's l1: 0.0348423\tvalid_1's l1: 0.371113\n",
      "[4400]\ttraining's l1: 0.0336239\tvalid_1's l1: 0.371078\n",
      "[4500]\ttraining's l1: 0.0324745\tvalid_1's l1: 0.370933\n",
      "[4600]\ttraining's l1: 0.0314283\tvalid_1's l1: 0.370861\n",
      "[4700]\ttraining's l1: 0.0303643\tvalid_1's l1: 0.370762\n",
      "[4800]\ttraining's l1: 0.029381\tvalid_1's l1: 0.370705\n",
      "[4900]\ttraining's l1: 0.0284292\tvalid_1's l1: 0.370617\n",
      "[5000]\ttraining's l1: 0.0275007\tvalid_1's l1: 0.370541\n",
      "[5100]\ttraining's l1: 0.0266363\tvalid_1's l1: 0.37047\n",
      "[5200]\ttraining's l1: 0.0257438\tvalid_1's l1: 0.370392\n",
      "[5300]\ttraining's l1: 0.0249033\tvalid_1's l1: 0.370389\n",
      "[5400]\ttraining's l1: 0.0241502\tvalid_1's l1: 0.370348\n",
      "[5500]\ttraining's l1: 0.023463\tvalid_1's l1: 0.370324\n",
      "[5600]\ttraining's l1: 0.0227391\tvalid_1's l1: 0.370253\n",
      "[5700]\ttraining's l1: 0.022043\tvalid_1's l1: 0.37027\n",
      "[5800]\ttraining's l1: 0.0214316\tvalid_1's l1: 0.370199\n",
      "[5900]\ttraining's l1: 0.0207906\tvalid_1's l1: 0.370185\n",
      "[6000]\ttraining's l1: 0.0201999\tvalid_1's l1: 0.370119\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0201999\tvalid_1's l1: 0.370119\n",
      "1JHN Fold 2, logMAE: -0.9939296001678858\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.425908\tvalid_1's l1: 0.509145\n",
      "[200]\ttraining's l1: 0.351919\tvalid_1's l1: 0.464813\n",
      "[300]\ttraining's l1: 0.303712\tvalid_1's l1: 0.439847\n",
      "[400]\ttraining's l1: 0.272369\tvalid_1's l1: 0.426963\n",
      "[500]\ttraining's l1: 0.247977\tvalid_1's l1: 0.416528\n",
      "[600]\ttraining's l1: 0.225899\tvalid_1's l1: 0.40791\n",
      "[700]\ttraining's l1: 0.207105\tvalid_1's l1: 0.401632\n",
      "[800]\ttraining's l1: 0.191352\tvalid_1's l1: 0.396024\n",
      "[900]\ttraining's l1: 0.177566\tvalid_1's l1: 0.392077\n",
      "[1000]\ttraining's l1: 0.16505\tvalid_1's l1: 0.3886\n",
      "[1100]\ttraining's l1: 0.153685\tvalid_1's l1: 0.385382\n",
      "[1200]\ttraining's l1: 0.14395\tvalid_1's l1: 0.383382\n",
      "[1300]\ttraining's l1: 0.135109\tvalid_1's l1: 0.381397\n",
      "[1400]\ttraining's l1: 0.126635\tvalid_1's l1: 0.379282\n",
      "[1500]\ttraining's l1: 0.119601\tvalid_1's l1: 0.377805\n",
      "[1600]\ttraining's l1: 0.112817\tvalid_1's l1: 0.376562\n",
      "[1700]\ttraining's l1: 0.10689\tvalid_1's l1: 0.375645\n",
      "[1800]\ttraining's l1: 0.101254\tvalid_1's l1: 0.374403\n",
      "[1900]\ttraining's l1: 0.095847\tvalid_1's l1: 0.37339\n",
      "[2000]\ttraining's l1: 0.0908712\tvalid_1's l1: 0.372635\n",
      "[2100]\ttraining's l1: 0.0865478\tvalid_1's l1: 0.371979\n",
      "[2200]\ttraining's l1: 0.0822803\tvalid_1's l1: 0.371175\n",
      "[2300]\ttraining's l1: 0.0786133\tvalid_1's l1: 0.370539\n",
      "[2400]\ttraining's l1: 0.0747604\tvalid_1's l1: 0.370036\n",
      "[2500]\ttraining's l1: 0.0714743\tvalid_1's l1: 0.36955\n",
      "[2600]\ttraining's l1: 0.0679647\tvalid_1's l1: 0.368907\n",
      "[2700]\ttraining's l1: 0.0649566\tvalid_1's l1: 0.368546\n",
      "[2800]\ttraining's l1: 0.0621803\tvalid_1's l1: 0.368157\n",
      "[2900]\ttraining's l1: 0.0595324\tvalid_1's l1: 0.367676\n",
      "[3000]\ttraining's l1: 0.05707\tvalid_1's l1: 0.367488\n",
      "[3100]\ttraining's l1: 0.0547386\tvalid_1's l1: 0.367216\n",
      "[3200]\ttraining's l1: 0.0525964\tvalid_1's l1: 0.367105\n",
      "[3300]\ttraining's l1: 0.0505075\tvalid_1's l1: 0.366903\n",
      "[3400]\ttraining's l1: 0.0485605\tvalid_1's l1: 0.366681\n",
      "[3500]\ttraining's l1: 0.0466965\tvalid_1's l1: 0.366421\n",
      "[3600]\ttraining's l1: 0.0449458\tvalid_1's l1: 0.366233\n",
      "[3700]\ttraining's l1: 0.0432485\tvalid_1's l1: 0.366146\n",
      "[3800]\ttraining's l1: 0.0417032\tvalid_1's l1: 0.365946\n",
      "[3900]\ttraining's l1: 0.040133\tvalid_1's l1: 0.365851\n",
      "[4000]\ttraining's l1: 0.0386951\tvalid_1's l1: 0.365684\n",
      "[4100]\ttraining's l1: 0.0373998\tvalid_1's l1: 0.365578\n",
      "[4200]\ttraining's l1: 0.0361518\tvalid_1's l1: 0.365438\n",
      "[4300]\ttraining's l1: 0.0349693\tvalid_1's l1: 0.365336\n",
      "[4400]\ttraining's l1: 0.0337531\tvalid_1's l1: 0.365201\n",
      "[4500]\ttraining's l1: 0.0326391\tvalid_1's l1: 0.365103\n",
      "[4600]\ttraining's l1: 0.0315167\tvalid_1's l1: 0.365058\n",
      "[4700]\ttraining's l1: 0.030505\tvalid_1's l1: 0.364975\n",
      "[4800]\ttraining's l1: 0.029541\tvalid_1's l1: 0.364849\n",
      "[4900]\ttraining's l1: 0.0285435\tvalid_1's l1: 0.364758\n",
      "[5000]\ttraining's l1: 0.0276403\tvalid_1's l1: 0.364677\n",
      "[5100]\ttraining's l1: 0.0267722\tvalid_1's l1: 0.364603\n",
      "[5200]\ttraining's l1: 0.0259007\tvalid_1's l1: 0.36453\n",
      "[5300]\ttraining's l1: 0.0251206\tvalid_1's l1: 0.364437\n",
      "[5400]\ttraining's l1: 0.0243402\tvalid_1's l1: 0.364382\n",
      "[5500]\ttraining's l1: 0.023627\tvalid_1's l1: 0.36428\n",
      "[5600]\ttraining's l1: 0.022909\tvalid_1's l1: 0.364225\n",
      "[5700]\ttraining's l1: 0.0222878\tvalid_1's l1: 0.364193\n",
      "[5800]\ttraining's l1: 0.0216178\tvalid_1's l1: 0.364183\n",
      "[5900]\ttraining's l1: 0.0210095\tvalid_1's l1: 0.364073\n",
      "[6000]\ttraining's l1: 0.0204129\tvalid_1's l1: 0.364046\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0204129\tvalid_1's l1: 0.364046\n",
      "1JHN Fold 3, logMAE: -1.0104738211142175\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.421934\tvalid_1's l1: 0.517766\n",
      "[200]\ttraining's l1: 0.344277\tvalid_1's l1: 0.466508\n",
      "[300]\ttraining's l1: 0.305162\tvalid_1's l1: 0.446242\n",
      "[400]\ttraining's l1: 0.273161\tvalid_1's l1: 0.430535\n",
      "[500]\ttraining's l1: 0.24539\tvalid_1's l1: 0.41931\n",
      "[600]\ttraining's l1: 0.223253\tvalid_1's l1: 0.41128\n",
      "[700]\ttraining's l1: 0.204976\tvalid_1's l1: 0.405324\n",
      "[800]\ttraining's l1: 0.18925\tvalid_1's l1: 0.400619\n",
      "[900]\ttraining's l1: 0.175935\tvalid_1's l1: 0.396664\n",
      "[1000]\ttraining's l1: 0.1642\tvalid_1's l1: 0.393315\n",
      "[1100]\ttraining's l1: 0.153902\tvalid_1's l1: 0.39044\n",
      "[1200]\ttraining's l1: 0.144251\tvalid_1's l1: 0.38828\n",
      "[1300]\ttraining's l1: 0.135437\tvalid_1's l1: 0.38611\n",
      "[1400]\ttraining's l1: 0.127318\tvalid_1's l1: 0.384459\n",
      "[1500]\ttraining's l1: 0.120102\tvalid_1's l1: 0.383116\n",
      "[1600]\ttraining's l1: 0.113158\tvalid_1's l1: 0.381805\n",
      "[1700]\ttraining's l1: 0.10686\tvalid_1's l1: 0.380323\n",
      "[1800]\ttraining's l1: 0.101401\tvalid_1's l1: 0.379596\n",
      "[1900]\ttraining's l1: 0.0960861\tvalid_1's l1: 0.378622\n",
      "[2000]\ttraining's l1: 0.091383\tvalid_1's l1: 0.378091\n",
      "[2100]\ttraining's l1: 0.0868208\tvalid_1's l1: 0.377341\n",
      "[2200]\ttraining's l1: 0.0825886\tvalid_1's l1: 0.376833\n",
      "[2300]\ttraining's l1: 0.0789284\tvalid_1's l1: 0.376268\n",
      "[2400]\ttraining's l1: 0.0750733\tvalid_1's l1: 0.375726\n",
      "[2500]\ttraining's l1: 0.071667\tvalid_1's l1: 0.37526\n",
      "[2600]\ttraining's l1: 0.0683344\tvalid_1's l1: 0.374903\n",
      "[2700]\ttraining's l1: 0.0652396\tvalid_1's l1: 0.374557\n",
      "[2800]\ttraining's l1: 0.0624728\tvalid_1's l1: 0.374224\n",
      "[2900]\ttraining's l1: 0.0598343\tvalid_1's l1: 0.373751\n",
      "[3000]\ttraining's l1: 0.0573892\tvalid_1's l1: 0.373453\n",
      "[3100]\ttraining's l1: 0.0549769\tvalid_1's l1: 0.373345\n",
      "[3200]\ttraining's l1: 0.0527604\tvalid_1's l1: 0.373217\n",
      "[3300]\ttraining's l1: 0.0507297\tvalid_1's l1: 0.373066\n",
      "[3400]\ttraining's l1: 0.0488036\tvalid_1's l1: 0.372842\n",
      "[3500]\ttraining's l1: 0.0469666\tvalid_1's l1: 0.372645\n",
      "[3600]\ttraining's l1: 0.0451915\tvalid_1's l1: 0.372507\n",
      "[3700]\ttraining's l1: 0.0435006\tvalid_1's l1: 0.372425\n",
      "[3800]\ttraining's l1: 0.0417838\tvalid_1's l1: 0.372271\n",
      "[3900]\ttraining's l1: 0.0402537\tvalid_1's l1: 0.372108\n",
      "[4000]\ttraining's l1: 0.0387165\tvalid_1's l1: 0.371989\n",
      "[4100]\ttraining's l1: 0.0373527\tvalid_1's l1: 0.371897\n",
      "[4200]\ttraining's l1: 0.0360716\tvalid_1's l1: 0.371882\n",
      "[4300]\ttraining's l1: 0.0348166\tvalid_1's l1: 0.37177\n",
      "[4400]\ttraining's l1: 0.0335379\tvalid_1's l1: 0.371643\n",
      "[4500]\ttraining's l1: 0.0323928\tvalid_1's l1: 0.371588\n",
      "[4600]\ttraining's l1: 0.0312642\tvalid_1's l1: 0.371414\n",
      "[4700]\ttraining's l1: 0.0301804\tvalid_1's l1: 0.371273\n",
      "[4800]\ttraining's l1: 0.0291926\tvalid_1's l1: 0.371148\n",
      "[4900]\ttraining's l1: 0.0282837\tvalid_1's l1: 0.37108\n",
      "[5000]\ttraining's l1: 0.0273724\tvalid_1's l1: 0.371045\n",
      "[5100]\ttraining's l1: 0.0264498\tvalid_1's l1: 0.370981\n",
      "[5200]\ttraining's l1: 0.0255847\tvalid_1's l1: 0.370873\n",
      "[5300]\ttraining's l1: 0.024757\tvalid_1's l1: 0.37083\n",
      "[5400]\ttraining's l1: 0.0239823\tvalid_1's l1: 0.370754\n",
      "[5500]\ttraining's l1: 0.0232527\tvalid_1's l1: 0.370727\n",
      "[5600]\ttraining's l1: 0.0225482\tvalid_1's l1: 0.370636\n",
      "[5700]\ttraining's l1: 0.0218385\tvalid_1's l1: 0.370619\n",
      "[5800]\ttraining's l1: 0.021161\tvalid_1's l1: 0.370541\n",
      "[5900]\ttraining's l1: 0.0205744\tvalid_1's l1: 0.370471\n",
      "[6000]\ttraining's l1: 0.0199902\tvalid_1's l1: 0.370456\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0199902\tvalid_1's l1: 0.370456\n",
      "1JHN Fold 4, logMAE: -0.9930204353661812\n",
      "*** Training Model for 1JHC ***\n",
      "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8',\n",
      "       'atom_9', 'd_1_0', 'd_2_0', 'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0',\n",
      "       'd_4_1', 'd_4_2', 'd_4_3', 'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0',\n",
      "       'd_6_1', 'd_6_2', 'd_6_3', 'd_7_0', 'd_7_1', 'd_7_2', 'd_7_3', 'd_8_0',\n",
      "       'd_8_1', 'd_8_2', 'd_8_3', 'd_9_0', 'd_9_1', 'd_9_2', 'd_9_3',\n",
      "       'scalar_coupling_constant'],\n",
      "      dtype='object')\n",
      "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8',\n",
      "       'atom_9', 'd_1_0', 'd_2_0', 'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0',\n",
      "       'd_4_1', 'd_4_2', 'd_4_3', 'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0',\n",
      "       'd_6_1', 'd_6_2', 'd_6_3', 'd_7_0', 'd_7_1', 'd_7_2', 'd_7_3', 'd_8_0',\n",
      "       'd_8_1', 'd_8_2', 'd_8_3', 'd_9_0', 'd_9_1', 'd_9_2', 'd_9_3'],\n",
      "      dtype='object')\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 1.16259\tvalid_1's l1: 1.21513\n",
      "[200]\ttraining's l1: 0.989651\tvalid_1's l1: 1.0707\n",
      "[300]\ttraining's l1: 0.891694\tvalid_1's l1: 0.996913\n",
      "[400]\ttraining's l1: 0.823256\tvalid_1's l1: 0.949467\n",
      "[500]\ttraining's l1: 0.770377\tvalid_1's l1: 0.914639\n",
      "[600]\ttraining's l1: 0.72703\tvalid_1's l1: 0.888618\n",
      "[700]\ttraining's l1: 0.690971\tvalid_1's l1: 0.868259\n",
      "[800]\ttraining's l1: 0.660469\tvalid_1's l1: 0.851818\n",
      "[900]\ttraining's l1: 0.63275\tvalid_1's l1: 0.836773\n",
      "[1000]\ttraining's l1: 0.608024\tvalid_1's l1: 0.82415\n",
      "[1100]\ttraining's l1: 0.585514\tvalid_1's l1: 0.8129\n",
      "[1200]\ttraining's l1: 0.56445\tvalid_1's l1: 0.803184\n",
      "[1300]\ttraining's l1: 0.546221\tvalid_1's l1: 0.794961\n",
      "[1400]\ttraining's l1: 0.528749\tvalid_1's l1: 0.787475\n",
      "[1500]\ttraining's l1: 0.512616\tvalid_1's l1: 0.780651\n",
      "[1600]\ttraining's l1: 0.497371\tvalid_1's l1: 0.774338\n",
      "[1700]\ttraining's l1: 0.483145\tvalid_1's l1: 0.768527\n",
      "[1800]\ttraining's l1: 0.469988\tvalid_1's l1: 0.763246\n",
      "[1900]\ttraining's l1: 0.457458\tvalid_1's l1: 0.758425\n",
      "[2000]\ttraining's l1: 0.445623\tvalid_1's l1: 0.754034\n",
      "[2100]\ttraining's l1: 0.434742\tvalid_1's l1: 0.750008\n",
      "[2200]\ttraining's l1: 0.42438\tvalid_1's l1: 0.746365\n",
      "[2300]\ttraining's l1: 0.414159\tvalid_1's l1: 0.742541\n",
      "[2400]\ttraining's l1: 0.404227\tvalid_1's l1: 0.739112\n",
      "[2500]\ttraining's l1: 0.395045\tvalid_1's l1: 0.735903\n",
      "[2600]\ttraining's l1: 0.386082\tvalid_1's l1: 0.733105\n",
      "[2700]\ttraining's l1: 0.37776\tvalid_1's l1: 0.730261\n",
      "[2800]\ttraining's l1: 0.369509\tvalid_1's l1: 0.727517\n",
      "[2900]\ttraining's l1: 0.361605\tvalid_1's l1: 0.72482\n",
      "[3000]\ttraining's l1: 0.353877\tvalid_1's l1: 0.72234\n",
      "[3100]\ttraining's l1: 0.346521\tvalid_1's l1: 0.72004\n",
      "[3200]\ttraining's l1: 0.339562\tvalid_1's l1: 0.717873\n",
      "[3300]\ttraining's l1: 0.332857\tvalid_1's l1: 0.715932\n",
      "[3400]\ttraining's l1: 0.326307\tvalid_1's l1: 0.714021\n",
      "[3500]\ttraining's l1: 0.320133\tvalid_1's l1: 0.712194\n",
      "[3600]\ttraining's l1: 0.314144\tvalid_1's l1: 0.710532\n",
      "[3700]\ttraining's l1: 0.308229\tvalid_1's l1: 0.708774\n",
      "[3800]\ttraining's l1: 0.302478\tvalid_1's l1: 0.706986\n",
      "[3900]\ttraining's l1: 0.296967\tvalid_1's l1: 0.705468\n",
      "[4000]\ttraining's l1: 0.291624\tvalid_1's l1: 0.703892\n",
      "[4100]\ttraining's l1: 0.286498\tvalid_1's l1: 0.702469\n",
      "[4200]\ttraining's l1: 0.281468\tvalid_1's l1: 0.701045\n",
      "[4300]\ttraining's l1: 0.276631\tvalid_1's l1: 0.699853\n",
      "[4400]\ttraining's l1: 0.27189\tvalid_1's l1: 0.698612\n",
      "[4500]\ttraining's l1: 0.267494\tvalid_1's l1: 0.697498\n",
      "[4600]\ttraining's l1: 0.262901\tvalid_1's l1: 0.696239\n",
      "[4700]\ttraining's l1: 0.258759\tvalid_1's l1: 0.695127\n",
      "[4800]\ttraining's l1: 0.254632\tvalid_1's l1: 0.69405\n",
      "[4900]\ttraining's l1: 0.250535\tvalid_1's l1: 0.693104\n",
      "[5000]\ttraining's l1: 0.246636\tvalid_1's l1: 0.69206\n",
      "[5100]\ttraining's l1: 0.242674\tvalid_1's l1: 0.691091\n",
      "[5200]\ttraining's l1: 0.238885\tvalid_1's l1: 0.690122\n",
      "[5300]\ttraining's l1: 0.235146\tvalid_1's l1: 0.689095\n",
      "[5400]\ttraining's l1: 0.231543\tvalid_1's l1: 0.688176\n",
      "[5500]\ttraining's l1: 0.2279\tvalid_1's l1: 0.68732\n",
      "[5600]\ttraining's l1: 0.224457\tvalid_1's l1: 0.686468\n",
      "[5700]\ttraining's l1: 0.221049\tvalid_1's l1: 0.68571\n",
      "[5800]\ttraining's l1: 0.21786\tvalid_1's l1: 0.684983\n",
      "[5900]\ttraining's l1: 0.214652\tvalid_1's l1: 0.684213\n",
      "[6000]\ttraining's l1: 0.211682\tvalid_1's l1: 0.683585\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.211682\tvalid_1's l1: 0.683585\n",
      "1JHC Fold 0, logMAE: -0.38040396394442866\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 1.16997\tvalid_1's l1: 1.21509\n",
      "[200]\ttraining's l1: 0.990809\tvalid_1's l1: 1.06696\n",
      "[300]\ttraining's l1: 0.891961\tvalid_1's l1: 0.992757\n",
      "[400]\ttraining's l1: 0.823608\tvalid_1's l1: 0.944754\n",
      "[500]\ttraining's l1: 0.770514\tvalid_1's l1: 0.910272\n",
      "[600]\ttraining's l1: 0.728835\tvalid_1's l1: 0.884765\n",
      "[700]\ttraining's l1: 0.691895\tvalid_1's l1: 0.86278\n",
      "[800]\ttraining's l1: 0.66026\tvalid_1's l1: 0.845505\n",
      "[900]\ttraining's l1: 0.632119\tvalid_1's l1: 0.830607\n",
      "[1000]\ttraining's l1: 0.607752\tvalid_1's l1: 0.818353\n",
      "[1100]\ttraining's l1: 0.585387\tvalid_1's l1: 0.807892\n",
      "[1200]\ttraining's l1: 0.565022\tvalid_1's l1: 0.79832\n",
      "[1300]\ttraining's l1: 0.546239\tvalid_1's l1: 0.789404\n",
      "[1400]\ttraining's l1: 0.52902\tvalid_1's l1: 0.781444\n",
      "[1500]\ttraining's l1: 0.5127\tvalid_1's l1: 0.774278\n",
      "[1600]\ttraining's l1: 0.497699\tvalid_1's l1: 0.768011\n",
      "[1700]\ttraining's l1: 0.483596\tvalid_1's l1: 0.762002\n",
      "[1800]\ttraining's l1: 0.470708\tvalid_1's l1: 0.756977\n",
      "[1900]\ttraining's l1: 0.45829\tvalid_1's l1: 0.752146\n",
      "[2000]\ttraining's l1: 0.446196\tvalid_1's l1: 0.747626\n",
      "[2100]\ttraining's l1: 0.434644\tvalid_1's l1: 0.743519\n",
      "[2200]\ttraining's l1: 0.424191\tvalid_1's l1: 0.739522\n",
      "[2300]\ttraining's l1: 0.414052\tvalid_1's l1: 0.735844\n",
      "[2400]\ttraining's l1: 0.404302\tvalid_1's l1: 0.732587\n",
      "[2500]\ttraining's l1: 0.395305\tvalid_1's l1: 0.729248\n",
      "[2600]\ttraining's l1: 0.386264\tvalid_1's l1: 0.72618\n",
      "[2700]\ttraining's l1: 0.377884\tvalid_1's l1: 0.723544\n",
      "[2800]\ttraining's l1: 0.369817\tvalid_1's l1: 0.720932\n",
      "[2900]\ttraining's l1: 0.36192\tvalid_1's l1: 0.71827\n",
      "[3000]\ttraining's l1: 0.354065\tvalid_1's l1: 0.715795\n",
      "[3100]\ttraining's l1: 0.346758\tvalid_1's l1: 0.713707\n",
      "[3200]\ttraining's l1: 0.3399\tvalid_1's l1: 0.711452\n",
      "[3300]\ttraining's l1: 0.333145\tvalid_1's l1: 0.709362\n",
      "[3400]\ttraining's l1: 0.326597\tvalid_1's l1: 0.707219\n",
      "[3500]\ttraining's l1: 0.320254\tvalid_1's l1: 0.705392\n",
      "[3600]\ttraining's l1: 0.314181\tvalid_1's l1: 0.703726\n",
      "[3700]\ttraining's l1: 0.30839\tvalid_1's l1: 0.702138\n",
      "[3800]\ttraining's l1: 0.302787\tvalid_1's l1: 0.700579\n",
      "[3900]\ttraining's l1: 0.297283\tvalid_1's l1: 0.699122\n",
      "[4000]\ttraining's l1: 0.29188\tvalid_1's l1: 0.69757\n",
      "[4100]\ttraining's l1: 0.286589\tvalid_1's l1: 0.696074\n",
      "[4200]\ttraining's l1: 0.281515\tvalid_1's l1: 0.694596\n",
      "[4300]\ttraining's l1: 0.2767\tvalid_1's l1: 0.693255\n",
      "[4400]\ttraining's l1: 0.271981\tvalid_1's l1: 0.692066\n",
      "[4500]\ttraining's l1: 0.267365\tvalid_1's l1: 0.690768\n",
      "[4600]\ttraining's l1: 0.262885\tvalid_1's l1: 0.689745\n",
      "[4700]\ttraining's l1: 0.258479\tvalid_1's l1: 0.688496\n",
      "[4800]\ttraining's l1: 0.254404\tvalid_1's l1: 0.687386\n",
      "[4900]\ttraining's l1: 0.250346\tvalid_1's l1: 0.686472\n",
      "[5000]\ttraining's l1: 0.246426\tvalid_1's l1: 0.685566\n",
      "[5100]\ttraining's l1: 0.242578\tvalid_1's l1: 0.684589\n",
      "[5200]\ttraining's l1: 0.23881\tvalid_1's l1: 0.683654\n",
      "[5300]\ttraining's l1: 0.23505\tvalid_1's l1: 0.682734\n",
      "[5400]\ttraining's l1: 0.231551\tvalid_1's l1: 0.681891\n",
      "[5500]\ttraining's l1: 0.228133\tvalid_1's l1: 0.68101\n",
      "[5600]\ttraining's l1: 0.224759\tvalid_1's l1: 0.680159\n",
      "[5700]\ttraining's l1: 0.221458\tvalid_1's l1: 0.679446\n",
      "[5800]\ttraining's l1: 0.21819\tvalid_1's l1: 0.67873\n",
      "[5900]\ttraining's l1: 0.214996\tvalid_1's l1: 0.677884\n",
      "[6000]\ttraining's l1: 0.211868\tvalid_1's l1: 0.677107\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.211868\tvalid_1's l1: 0.677107\n",
      "1JHC Fold 1, logMAE: -0.38992669951004144\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 1.16739\tvalid_1's l1: 1.21739\n",
      "[200]\ttraining's l1: 0.993582\tvalid_1's l1: 1.07359\n",
      "[300]\ttraining's l1: 0.896453\tvalid_1's l1: 0.999558\n",
      "[400]\ttraining's l1: 0.825722\tvalid_1's l1: 0.95083\n",
      "[500]\ttraining's l1: 0.771544\tvalid_1's l1: 0.915797\n",
      "[600]\ttraining's l1: 0.729068\tvalid_1's l1: 0.890586\n",
      "[700]\ttraining's l1: 0.692057\tvalid_1's l1: 0.86854\n",
      "[800]\ttraining's l1: 0.660854\tvalid_1's l1: 0.851959\n",
      "[900]\ttraining's l1: 0.633687\tvalid_1's l1: 0.837028\n",
      "[1000]\ttraining's l1: 0.608419\tvalid_1's l1: 0.824158\n",
      "[1100]\ttraining's l1: 0.586303\tvalid_1's l1: 0.813264\n",
      "[1200]\ttraining's l1: 0.565501\tvalid_1's l1: 0.803356\n",
      "[1300]\ttraining's l1: 0.54609\tvalid_1's l1: 0.794455\n",
      "[1400]\ttraining's l1: 0.529096\tvalid_1's l1: 0.787203\n",
      "[1500]\ttraining's l1: 0.513046\tvalid_1's l1: 0.780247\n",
      "[1600]\ttraining's l1: 0.49822\tvalid_1's l1: 0.774073\n",
      "[1700]\ttraining's l1: 0.484049\tvalid_1's l1: 0.768428\n",
      "[1800]\ttraining's l1: 0.47023\tvalid_1's l1: 0.762685\n",
      "[1900]\ttraining's l1: 0.457247\tvalid_1's l1: 0.757948\n",
      "[2000]\ttraining's l1: 0.445507\tvalid_1's l1: 0.753522\n",
      "[2100]\ttraining's l1: 0.434229\tvalid_1's l1: 0.749526\n",
      "[2200]\ttraining's l1: 0.423638\tvalid_1's l1: 0.745687\n",
      "[2300]\ttraining's l1: 0.413349\tvalid_1's l1: 0.742042\n",
      "[2400]\ttraining's l1: 0.403434\tvalid_1's l1: 0.738561\n",
      "[2500]\ttraining's l1: 0.394479\tvalid_1's l1: 0.735437\n",
      "[2600]\ttraining's l1: 0.385295\tvalid_1's l1: 0.732488\n",
      "[2700]\ttraining's l1: 0.376884\tvalid_1's l1: 0.729766\n",
      "[2800]\ttraining's l1: 0.36873\tvalid_1's l1: 0.727082\n",
      "[2900]\ttraining's l1: 0.360962\tvalid_1's l1: 0.724353\n",
      "[3000]\ttraining's l1: 0.353333\tvalid_1's l1: 0.722197\n",
      "[3100]\ttraining's l1: 0.345985\tvalid_1's l1: 0.719854\n",
      "[3200]\ttraining's l1: 0.338888\tvalid_1's l1: 0.717777\n",
      "[3300]\ttraining's l1: 0.332284\tvalid_1's l1: 0.715902\n",
      "[3400]\ttraining's l1: 0.326018\tvalid_1's l1: 0.713846\n",
      "[3500]\ttraining's l1: 0.319772\tvalid_1's l1: 0.712067\n",
      "[3600]\ttraining's l1: 0.313708\tvalid_1's l1: 0.710406\n",
      "[3700]\ttraining's l1: 0.307742\tvalid_1's l1: 0.708677\n",
      "[3800]\ttraining's l1: 0.301945\tvalid_1's l1: 0.707042\n",
      "[3900]\ttraining's l1: 0.296535\tvalid_1's l1: 0.705506\n",
      "[4000]\ttraining's l1: 0.29136\tvalid_1's l1: 0.703999\n",
      "[4100]\ttraining's l1: 0.286143\tvalid_1's l1: 0.702649\n",
      "[4200]\ttraining's l1: 0.281275\tvalid_1's l1: 0.701194\n",
      "[4300]\ttraining's l1: 0.276331\tvalid_1's l1: 0.699962\n",
      "[4400]\ttraining's l1: 0.271563\tvalid_1's l1: 0.698699\n",
      "[4500]\ttraining's l1: 0.26708\tvalid_1's l1: 0.697536\n",
      "[4600]\ttraining's l1: 0.262617\tvalid_1's l1: 0.696345\n",
      "[4700]\ttraining's l1: 0.258269\tvalid_1's l1: 0.695239\n",
      "[4800]\ttraining's l1: 0.254153\tvalid_1's l1: 0.694196\n",
      "[4900]\ttraining's l1: 0.250073\tvalid_1's l1: 0.693152\n",
      "[5000]\ttraining's l1: 0.246073\tvalid_1's l1: 0.69211\n",
      "[5100]\ttraining's l1: 0.242307\tvalid_1's l1: 0.691145\n",
      "[5200]\ttraining's l1: 0.238523\tvalid_1's l1: 0.690341\n",
      "[5300]\ttraining's l1: 0.234677\tvalid_1's l1: 0.689367\n",
      "[5400]\ttraining's l1: 0.23122\tvalid_1's l1: 0.688597\n",
      "[5500]\ttraining's l1: 0.227788\tvalid_1's l1: 0.687774\n",
      "[5600]\ttraining's l1: 0.22429\tvalid_1's l1: 0.686871\n",
      "[5700]\ttraining's l1: 0.221004\tvalid_1's l1: 0.686003\n",
      "[5800]\ttraining's l1: 0.217788\tvalid_1's l1: 0.685174\n",
      "[5900]\ttraining's l1: 0.214581\tvalid_1's l1: 0.684466\n",
      "[6000]\ttraining's l1: 0.211539\tvalid_1's l1: 0.683772\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.211539\tvalid_1's l1: 0.683772\n",
      "1JHC Fold 2, logMAE: -0.38013100890485857\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 1.16171\tvalid_1's l1: 1.21508\n",
      "[200]\ttraining's l1: 0.991849\tvalid_1's l1: 1.07313\n",
      "[300]\ttraining's l1: 0.894262\tvalid_1's l1: 0.999549\n",
      "[400]\ttraining's l1: 0.825166\tvalid_1's l1: 0.950857\n",
      "[500]\ttraining's l1: 0.772155\tvalid_1's l1: 0.915532\n",
      "[600]\ttraining's l1: 0.728559\tvalid_1's l1: 0.888326\n",
      "[700]\ttraining's l1: 0.692728\tvalid_1's l1: 0.866893\n",
      "[800]\ttraining's l1: 0.661101\tvalid_1's l1: 0.848831\n",
      "[900]\ttraining's l1: 0.632955\tvalid_1's l1: 0.833656\n",
      "[1000]\ttraining's l1: 0.608127\tvalid_1's l1: 0.821111\n",
      "[1100]\ttraining's l1: 0.585905\tvalid_1's l1: 0.81002\n",
      "[1200]\ttraining's l1: 0.565815\tvalid_1's l1: 0.80051\n",
      "[1300]\ttraining's l1: 0.546552\tvalid_1's l1: 0.791578\n",
      "[1400]\ttraining's l1: 0.529477\tvalid_1's l1: 0.784193\n",
      "[1500]\ttraining's l1: 0.513325\tvalid_1's l1: 0.776947\n",
      "[1600]\ttraining's l1: 0.498409\tvalid_1's l1: 0.771086\n",
      "[1700]\ttraining's l1: 0.483954\tvalid_1's l1: 0.765037\n",
      "[1800]\ttraining's l1: 0.471142\tvalid_1's l1: 0.759574\n",
      "[1900]\ttraining's l1: 0.458562\tvalid_1's l1: 0.754468\n",
      "[2000]\ttraining's l1: 0.446753\tvalid_1's l1: 0.749744\n",
      "[2100]\ttraining's l1: 0.435297\tvalid_1's l1: 0.745331\n",
      "[2200]\ttraining's l1: 0.424797\tvalid_1's l1: 0.741374\n",
      "[2300]\ttraining's l1: 0.414421\tvalid_1's l1: 0.737444\n",
      "[2400]\ttraining's l1: 0.404756\tvalid_1's l1: 0.733781\n",
      "[2500]\ttraining's l1: 0.395639\tvalid_1's l1: 0.730753\n",
      "[2600]\ttraining's l1: 0.386849\tvalid_1's l1: 0.727687\n",
      "[2700]\ttraining's l1: 0.378322\tvalid_1's l1: 0.724682\n",
      "[2800]\ttraining's l1: 0.370199\tvalid_1's l1: 0.721982\n",
      "[2900]\ttraining's l1: 0.362352\tvalid_1's l1: 0.719386\n",
      "[3000]\ttraining's l1: 0.354694\tvalid_1's l1: 0.717082\n",
      "[3100]\ttraining's l1: 0.347414\tvalid_1's l1: 0.714597\n",
      "[3200]\ttraining's l1: 0.340338\tvalid_1's l1: 0.712257\n",
      "[3300]\ttraining's l1: 0.333395\tvalid_1's l1: 0.709998\n",
      "[3400]\ttraining's l1: 0.3268\tvalid_1's l1: 0.708007\n",
      "[3500]\ttraining's l1: 0.320283\tvalid_1's l1: 0.706156\n",
      "[3600]\ttraining's l1: 0.314391\tvalid_1's l1: 0.70436\n",
      "[3700]\ttraining's l1: 0.308601\tvalid_1's l1: 0.702667\n",
      "[3800]\ttraining's l1: 0.302876\tvalid_1's l1: 0.701034\n",
      "[3900]\ttraining's l1: 0.297425\tvalid_1's l1: 0.699429\n",
      "[4000]\ttraining's l1: 0.292089\tvalid_1's l1: 0.697928\n",
      "[4100]\ttraining's l1: 0.287013\tvalid_1's l1: 0.696625\n",
      "[4200]\ttraining's l1: 0.281994\tvalid_1's l1: 0.695148\n",
      "[4300]\ttraining's l1: 0.276993\tvalid_1's l1: 0.693719\n",
      "[4400]\ttraining's l1: 0.272302\tvalid_1's l1: 0.692457\n",
      "[4500]\ttraining's l1: 0.267608\tvalid_1's l1: 0.691153\n",
      "[4600]\ttraining's l1: 0.263018\tvalid_1's l1: 0.689854\n",
      "[4700]\ttraining's l1: 0.258658\tvalid_1's l1: 0.688857\n",
      "[4800]\ttraining's l1: 0.254577\tvalid_1's l1: 0.687704\n",
      "[4900]\ttraining's l1: 0.250566\tvalid_1's l1: 0.686647\n",
      "[5000]\ttraining's l1: 0.246509\tvalid_1's l1: 0.685594\n",
      "[5100]\ttraining's l1: 0.242568\tvalid_1's l1: 0.684714\n",
      "[5200]\ttraining's l1: 0.238751\tvalid_1's l1: 0.683807\n",
      "[5300]\ttraining's l1: 0.235024\tvalid_1's l1: 0.682845\n",
      "[5400]\ttraining's l1: 0.231387\tvalid_1's l1: 0.682003\n",
      "[5500]\ttraining's l1: 0.227878\tvalid_1's l1: 0.681161\n",
      "[5600]\ttraining's l1: 0.224406\tvalid_1's l1: 0.680431\n",
      "[5700]\ttraining's l1: 0.221054\tvalid_1's l1: 0.679584\n",
      "[5800]\ttraining's l1: 0.217715\tvalid_1's l1: 0.678824\n",
      "[5900]\ttraining's l1: 0.214558\tvalid_1's l1: 0.677998\n",
      "[6000]\ttraining's l1: 0.21145\tvalid_1's l1: 0.67727\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.21145\tvalid_1's l1: 0.67727\n",
      "1JHC Fold 3, logMAE: -0.3896849207641081\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 1.16025\tvalid_1's l1: 1.21555\n",
      "[200]\ttraining's l1: 0.989993\tvalid_1's l1: 1.07381\n",
      "[300]\ttraining's l1: 0.891776\tvalid_1's l1: 0.999423\n",
      "[400]\ttraining's l1: 0.82344\tvalid_1's l1: 0.951069\n",
      "[500]\ttraining's l1: 0.769911\tvalid_1's l1: 0.915286\n",
      "[600]\ttraining's l1: 0.726172\tvalid_1's l1: 0.888168\n",
      "[700]\ttraining's l1: 0.69074\tvalid_1's l1: 0.867306\n",
      "[800]\ttraining's l1: 0.659311\tvalid_1's l1: 0.84984\n",
      "[900]\ttraining's l1: 0.631091\tvalid_1's l1: 0.834764\n",
      "[1000]\ttraining's l1: 0.606601\tvalid_1's l1: 0.822818\n",
      "[1100]\ttraining's l1: 0.584511\tvalid_1's l1: 0.811627\n",
      "[1200]\ttraining's l1: 0.563735\tvalid_1's l1: 0.801672\n",
      "[1300]\ttraining's l1: 0.545225\tvalid_1's l1: 0.793054\n",
      "[1400]\ttraining's l1: 0.528106\tvalid_1's l1: 0.785056\n",
      "[1500]\ttraining's l1: 0.512235\tvalid_1's l1: 0.777956\n",
      "[1600]\ttraining's l1: 0.49708\tvalid_1's l1: 0.771584\n",
      "[1700]\ttraining's l1: 0.483292\tvalid_1's l1: 0.765997\n",
      "[1800]\ttraining's l1: 0.470433\tvalid_1's l1: 0.760541\n",
      "[1900]\ttraining's l1: 0.458093\tvalid_1's l1: 0.755373\n",
      "[2000]\ttraining's l1: 0.44615\tvalid_1's l1: 0.750564\n",
      "[2100]\ttraining's l1: 0.434785\tvalid_1's l1: 0.746459\n",
      "[2200]\ttraining's l1: 0.424364\tvalid_1's l1: 0.742706\n",
      "[2300]\ttraining's l1: 0.414312\tvalid_1's l1: 0.739025\n",
      "[2400]\ttraining's l1: 0.404413\tvalid_1's l1: 0.73527\n",
      "[2500]\ttraining's l1: 0.39517\tvalid_1's l1: 0.731778\n",
      "[2600]\ttraining's l1: 0.386591\tvalid_1's l1: 0.72882\n",
      "[2700]\ttraining's l1: 0.378117\tvalid_1's l1: 0.726074\n",
      "[2800]\ttraining's l1: 0.369743\tvalid_1's l1: 0.723272\n",
      "[2900]\ttraining's l1: 0.361592\tvalid_1's l1: 0.720785\n",
      "[3000]\ttraining's l1: 0.354048\tvalid_1's l1: 0.718465\n",
      "[3100]\ttraining's l1: 0.34672\tvalid_1's l1: 0.716167\n",
      "[3200]\ttraining's l1: 0.339593\tvalid_1's l1: 0.714119\n",
      "[3300]\ttraining's l1: 0.332931\tvalid_1's l1: 0.711922\n",
      "[3400]\ttraining's l1: 0.326369\tvalid_1's l1: 0.710099\n",
      "[3500]\ttraining's l1: 0.320001\tvalid_1's l1: 0.708194\n",
      "[3600]\ttraining's l1: 0.313911\tvalid_1's l1: 0.706466\n",
      "[3700]\ttraining's l1: 0.308154\tvalid_1's l1: 0.704814\n",
      "[3800]\ttraining's l1: 0.302439\tvalid_1's l1: 0.703106\n",
      "[3900]\ttraining's l1: 0.297\tvalid_1's l1: 0.701478\n",
      "[4000]\ttraining's l1: 0.291593\tvalid_1's l1: 0.699942\n",
      "[4100]\ttraining's l1: 0.286521\tvalid_1's l1: 0.698515\n",
      "[4200]\ttraining's l1: 0.281417\tvalid_1's l1: 0.697076\n",
      "[4300]\ttraining's l1: 0.276644\tvalid_1's l1: 0.695837\n",
      "[4400]\ttraining's l1: 0.271795\tvalid_1's l1: 0.694665\n",
      "[4500]\ttraining's l1: 0.267251\tvalid_1's l1: 0.693545\n",
      "[4600]\ttraining's l1: 0.262765\tvalid_1's l1: 0.692251\n",
      "[4700]\ttraining's l1: 0.258466\tvalid_1's l1: 0.69106\n",
      "[4800]\ttraining's l1: 0.254292\tvalid_1's l1: 0.689977\n",
      "[4900]\ttraining's l1: 0.25022\tvalid_1's l1: 0.689075\n",
      "[5000]\ttraining's l1: 0.246242\tvalid_1's l1: 0.688052\n",
      "[5100]\ttraining's l1: 0.242349\tvalid_1's l1: 0.687098\n",
      "[5200]\ttraining's l1: 0.238595\tvalid_1's l1: 0.68605\n",
      "[5300]\ttraining's l1: 0.234844\tvalid_1's l1: 0.685023\n",
      "[5400]\ttraining's l1: 0.231268\tvalid_1's l1: 0.684231\n",
      "[5500]\ttraining's l1: 0.227725\tvalid_1's l1: 0.683393\n",
      "[5600]\ttraining's l1: 0.224319\tvalid_1's l1: 0.682548\n",
      "[5700]\ttraining's l1: 0.220996\tvalid_1's l1: 0.681805\n",
      "[5800]\ttraining's l1: 0.217724\tvalid_1's l1: 0.681125\n",
      "[5900]\ttraining's l1: 0.214566\tvalid_1's l1: 0.680401\n",
      "[6000]\ttraining's l1: 0.211463\tvalid_1's l1: 0.679645\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.211463\tvalid_1's l1: 0.679645\n",
      "1JHC Fold 4, logMAE: -0.38618496875447456\n",
      "*** Training Model for 2JHH ***\n",
      "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8',\n",
      "       'd_1_0', 'd_2_0', 'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0', 'd_4_1',\n",
      "       'd_4_2', 'd_4_3', 'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0', 'd_6_1',\n",
      "       'd_6_2', 'd_6_3', 'd_7_0', 'd_7_1', 'd_7_2', 'd_7_3', 'd_8_0', 'd_8_1',\n",
      "       'd_8_2', 'd_8_3', 'scalar_coupling_constant'],\n",
      "      dtype='object')\n",
      "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8',\n",
      "       'd_1_0', 'd_2_0', 'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0', 'd_4_1',\n",
      "       'd_4_2', 'd_4_3', 'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0', 'd_6_1',\n",
      "       'd_6_2', 'd_6_3', 'd_7_0', 'd_7_1', 'd_7_2', 'd_7_3', 'd_8_0', 'd_8_1',\n",
      "       'd_8_2', 'd_8_3'],\n",
      "      dtype='object')\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.255385\tvalid_1's l1: 0.278547\n",
      "[200]\ttraining's l1: 0.208479\tvalid_1's l1: 0.242561\n",
      "[300]\ttraining's l1: 0.183296\tvalid_1's l1: 0.225212\n",
      "[400]\ttraining's l1: 0.166595\tvalid_1's l1: 0.215154\n",
      "[500]\ttraining's l1: 0.153303\tvalid_1's l1: 0.207568\n",
      "[600]\ttraining's l1: 0.143012\tvalid_1's l1: 0.202187\n",
      "[700]\ttraining's l1: 0.133992\tvalid_1's l1: 0.197861\n",
      "[800]\ttraining's l1: 0.12642\tvalid_1's l1: 0.19433\n",
      "[900]\ttraining's l1: 0.120133\tvalid_1's l1: 0.191583\n",
      "[1000]\ttraining's l1: 0.114048\tvalid_1's l1: 0.188998\n",
      "[1100]\ttraining's l1: 0.108589\tvalid_1's l1: 0.186693\n",
      "[1200]\ttraining's l1: 0.103855\tvalid_1's l1: 0.184901\n",
      "[1300]\ttraining's l1: 0.0996271\tvalid_1's l1: 0.18338\n",
      "[1400]\ttraining's l1: 0.095422\tvalid_1's l1: 0.181834\n",
      "[1500]\ttraining's l1: 0.0916825\tvalid_1's l1: 0.180484\n",
      "[1600]\ttraining's l1: 0.0879755\tvalid_1's l1: 0.17907\n",
      "[1700]\ttraining's l1: 0.0846953\tvalid_1's l1: 0.177977\n",
      "[1800]\ttraining's l1: 0.0815656\tvalid_1's l1: 0.177008\n",
      "[1900]\ttraining's l1: 0.0787139\tvalid_1's l1: 0.176131\n",
      "[2000]\ttraining's l1: 0.0759218\tvalid_1's l1: 0.175221\n",
      "[2100]\ttraining's l1: 0.0734605\tvalid_1's l1: 0.174514\n",
      "[2200]\ttraining's l1: 0.0711171\tvalid_1's l1: 0.173811\n",
      "[2300]\ttraining's l1: 0.0688951\tvalid_1's l1: 0.173169\n",
      "[2400]\ttraining's l1: 0.0668011\tvalid_1's l1: 0.172584\n",
      "[2500]\ttraining's l1: 0.064727\tvalid_1's l1: 0.172025\n",
      "[2600]\ttraining's l1: 0.0627557\tvalid_1's l1: 0.171505\n",
      "[2700]\ttraining's l1: 0.060827\tvalid_1's l1: 0.170979\n",
      "[2800]\ttraining's l1: 0.0590659\tvalid_1's l1: 0.170507\n",
      "[2900]\ttraining's l1: 0.0572947\tvalid_1's l1: 0.170082\n",
      "[3000]\ttraining's l1: 0.0556421\tvalid_1's l1: 0.16966\n",
      "[3100]\ttraining's l1: 0.0541116\tvalid_1's l1: 0.16928\n",
      "[3200]\ttraining's l1: 0.0526268\tvalid_1's l1: 0.168902\n",
      "[3300]\ttraining's l1: 0.0511852\tvalid_1's l1: 0.168582\n",
      "[3400]\ttraining's l1: 0.0498394\tvalid_1's l1: 0.168265\n",
      "[3500]\ttraining's l1: 0.0485935\tvalid_1's l1: 0.167977\n",
      "[3600]\ttraining's l1: 0.0473494\tvalid_1's l1: 0.167696\n",
      "[3700]\ttraining's l1: 0.0461838\tvalid_1's l1: 0.16742\n",
      "[3800]\ttraining's l1: 0.0450355\tvalid_1's l1: 0.167165\n",
      "[3900]\ttraining's l1: 0.0439087\tvalid_1's l1: 0.166919\n",
      "[4000]\ttraining's l1: 0.0428534\tvalid_1's l1: 0.166665\n",
      "[4100]\ttraining's l1: 0.0418792\tvalid_1's l1: 0.166469\n",
      "[4200]\ttraining's l1: 0.0408844\tvalid_1's l1: 0.166281\n",
      "[4300]\ttraining's l1: 0.0399532\tvalid_1's l1: 0.166075\n",
      "[4400]\ttraining's l1: 0.0390241\tvalid_1's l1: 0.165875\n",
      "[4500]\ttraining's l1: 0.038136\tvalid_1's l1: 0.165712\n",
      "[4600]\ttraining's l1: 0.0373105\tvalid_1's l1: 0.165533\n",
      "[4700]\ttraining's l1: 0.036452\tvalid_1's l1: 0.165376\n",
      "[4800]\ttraining's l1: 0.0356259\tvalid_1's l1: 0.165217\n",
      "[4900]\ttraining's l1: 0.0348255\tvalid_1's l1: 0.165073\n",
      "[5000]\ttraining's l1: 0.034059\tvalid_1's l1: 0.16494\n",
      "[5100]\ttraining's l1: 0.0333368\tvalid_1's l1: 0.164796\n",
      "[5200]\ttraining's l1: 0.0326312\tvalid_1's l1: 0.164676\n",
      "[5300]\ttraining's l1: 0.0319528\tvalid_1's l1: 0.164527\n",
      "[5400]\ttraining's l1: 0.0313009\tvalid_1's l1: 0.16442\n",
      "[5500]\ttraining's l1: 0.0306787\tvalid_1's l1: 0.164301\n",
      "[5600]\ttraining's l1: 0.030051\tvalid_1's l1: 0.164185\n",
      "[5700]\ttraining's l1: 0.0294696\tvalid_1's l1: 0.164072\n",
      "[5800]\ttraining's l1: 0.0288993\tvalid_1's l1: 0.163969\n",
      "[5900]\ttraining's l1: 0.0283395\tvalid_1's l1: 0.163861\n",
      "[6000]\ttraining's l1: 0.0277745\tvalid_1's l1: 0.163749\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0277745\tvalid_1's l1: 0.163749\n",
      "2JHH Fold 0, logMAE: -1.809418539098965\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.255796\tvalid_1's l1: 0.27512\n",
      "[200]\ttraining's l1: 0.209294\tvalid_1's l1: 0.239791\n",
      "[300]\ttraining's l1: 0.183879\tvalid_1's l1: 0.22304\n",
      "[400]\ttraining's l1: 0.16764\tvalid_1's l1: 0.213396\n",
      "[500]\ttraining's l1: 0.154595\tvalid_1's l1: 0.206275\n",
      "[600]\ttraining's l1: 0.143645\tvalid_1's l1: 0.200704\n",
      "[700]\ttraining's l1: 0.134408\tvalid_1's l1: 0.196139\n",
      "[800]\ttraining's l1: 0.126721\tvalid_1's l1: 0.192707\n",
      "[900]\ttraining's l1: 0.120042\tvalid_1's l1: 0.189753\n",
      "[1000]\ttraining's l1: 0.114114\tvalid_1's l1: 0.187254\n",
      "[1100]\ttraining's l1: 0.108829\tvalid_1's l1: 0.185118\n",
      "[1200]\ttraining's l1: 0.103944\tvalid_1's l1: 0.183247\n",
      "[1300]\ttraining's l1: 0.0994754\tvalid_1's l1: 0.181604\n",
      "[1400]\ttraining's l1: 0.0952166\tvalid_1's l1: 0.179945\n",
      "[1500]\ttraining's l1: 0.0913572\tvalid_1's l1: 0.178549\n",
      "[1600]\ttraining's l1: 0.0879594\tvalid_1's l1: 0.177288\n",
      "[1700]\ttraining's l1: 0.0848339\tvalid_1's l1: 0.176265\n",
      "[1800]\ttraining's l1: 0.0817299\tvalid_1's l1: 0.175129\n",
      "[1900]\ttraining's l1: 0.0788306\tvalid_1's l1: 0.174207\n",
      "[2000]\ttraining's l1: 0.0761282\tvalid_1's l1: 0.173333\n",
      "[2100]\ttraining's l1: 0.0736059\tvalid_1's l1: 0.172599\n",
      "[2200]\ttraining's l1: 0.0711506\tvalid_1's l1: 0.171827\n",
      "[2300]\ttraining's l1: 0.0689676\tvalid_1's l1: 0.171173\n",
      "[2400]\ttraining's l1: 0.0667556\tvalid_1's l1: 0.170521\n",
      "[2500]\ttraining's l1: 0.0646695\tvalid_1's l1: 0.169867\n",
      "[2600]\ttraining's l1: 0.0627139\tvalid_1's l1: 0.169347\n",
      "[2700]\ttraining's l1: 0.0608561\tvalid_1's l1: 0.168829\n",
      "[2800]\ttraining's l1: 0.0590901\tvalid_1's l1: 0.16838\n",
      "[2900]\ttraining's l1: 0.0574208\tvalid_1's l1: 0.167886\n",
      "[3000]\ttraining's l1: 0.0557773\tvalid_1's l1: 0.167408\n",
      "[3100]\ttraining's l1: 0.0542038\tvalid_1's l1: 0.167041\n",
      "[3200]\ttraining's l1: 0.0527197\tvalid_1's l1: 0.166697\n",
      "[3300]\ttraining's l1: 0.0513154\tvalid_1's l1: 0.166348\n",
      "[3400]\ttraining's l1: 0.0499812\tvalid_1's l1: 0.166042\n",
      "[3500]\ttraining's l1: 0.0486634\tvalid_1's l1: 0.165724\n",
      "[3600]\ttraining's l1: 0.0474401\tvalid_1's l1: 0.165426\n",
      "[3700]\ttraining's l1: 0.0462154\tvalid_1's l1: 0.165163\n",
      "[3800]\ttraining's l1: 0.0451225\tvalid_1's l1: 0.164935\n",
      "[3900]\ttraining's l1: 0.0440334\tvalid_1's l1: 0.164671\n",
      "[4000]\ttraining's l1: 0.043035\tvalid_1's l1: 0.164492\n",
      "[4100]\ttraining's l1: 0.0420161\tvalid_1's l1: 0.164257\n",
      "[4200]\ttraining's l1: 0.0409815\tvalid_1's l1: 0.164025\n",
      "[4300]\ttraining's l1: 0.0400549\tvalid_1's l1: 0.163826\n",
      "[4400]\ttraining's l1: 0.03912\tvalid_1's l1: 0.163641\n",
      "[4500]\ttraining's l1: 0.0382087\tvalid_1's l1: 0.16347\n",
      "[4600]\ttraining's l1: 0.0373702\tvalid_1's l1: 0.163279\n",
      "[4700]\ttraining's l1: 0.0365456\tvalid_1's l1: 0.16313\n",
      "[4800]\ttraining's l1: 0.0357386\tvalid_1's l1: 0.162993\n",
      "[4900]\ttraining's l1: 0.0349675\tvalid_1's l1: 0.162836\n",
      "[5000]\ttraining's l1: 0.0341936\tvalid_1's l1: 0.162688\n",
      "[5100]\ttraining's l1: 0.0334628\tvalid_1's l1: 0.162543\n",
      "[5200]\ttraining's l1: 0.0327572\tvalid_1's l1: 0.162415\n",
      "[5300]\ttraining's l1: 0.0320799\tvalid_1's l1: 0.162284\n",
      "[5400]\ttraining's l1: 0.0314013\tvalid_1's l1: 0.162144\n",
      "[5500]\ttraining's l1: 0.0307709\tvalid_1's l1: 0.162018\n",
      "[5600]\ttraining's l1: 0.0301645\tvalid_1's l1: 0.16192\n",
      "[5700]\ttraining's l1: 0.0295598\tvalid_1's l1: 0.161822\n",
      "[5800]\ttraining's l1: 0.0289915\tvalid_1's l1: 0.161724\n",
      "[5900]\ttraining's l1: 0.0284217\tvalid_1's l1: 0.161611\n",
      "[6000]\ttraining's l1: 0.0278877\tvalid_1's l1: 0.161512\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0278877\tvalid_1's l1: 0.161512\n",
      "2JHH Fold 1, logMAE: -1.8231759168553103\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.256379\tvalid_1's l1: 0.277247\n",
      "[200]\ttraining's l1: 0.209376\tvalid_1's l1: 0.241512\n",
      "[300]\ttraining's l1: 0.184499\tvalid_1's l1: 0.224924\n",
      "[400]\ttraining's l1: 0.166904\tvalid_1's l1: 0.214305\n",
      "[500]\ttraining's l1: 0.153564\tvalid_1's l1: 0.207197\n",
      "[600]\ttraining's l1: 0.143267\tvalid_1's l1: 0.201969\n",
      "[700]\ttraining's l1: 0.134854\tvalid_1's l1: 0.197865\n",
      "[800]\ttraining's l1: 0.127153\tvalid_1's l1: 0.19423\n",
      "[900]\ttraining's l1: 0.120121\tvalid_1's l1: 0.19114\n",
      "[1000]\ttraining's l1: 0.114295\tvalid_1's l1: 0.188488\n",
      "[1100]\ttraining's l1: 0.109052\tvalid_1's l1: 0.186473\n",
      "[1200]\ttraining's l1: 0.104133\tvalid_1's l1: 0.184529\n",
      "[1300]\ttraining's l1: 0.0998985\tvalid_1's l1: 0.182862\n",
      "[1400]\ttraining's l1: 0.0956312\tvalid_1's l1: 0.181283\n",
      "[1500]\ttraining's l1: 0.0918611\tvalid_1's l1: 0.179935\n",
      "[1600]\ttraining's l1: 0.0883671\tvalid_1's l1: 0.178711\n",
      "[1700]\ttraining's l1: 0.0850192\tvalid_1's l1: 0.177539\n",
      "[1800]\ttraining's l1: 0.0817979\tvalid_1's l1: 0.176522\n",
      "[1900]\ttraining's l1: 0.0789646\tvalid_1's l1: 0.175589\n",
      "[2000]\ttraining's l1: 0.0763192\tvalid_1's l1: 0.174736\n",
      "[2100]\ttraining's l1: 0.0736096\tvalid_1's l1: 0.173914\n",
      "[2200]\ttraining's l1: 0.0711385\tvalid_1's l1: 0.173172\n",
      "[2300]\ttraining's l1: 0.0688407\tvalid_1's l1: 0.172556\n",
      "[2400]\ttraining's l1: 0.0667134\tvalid_1's l1: 0.171947\n",
      "[2500]\ttraining's l1: 0.0646891\tvalid_1's l1: 0.171383\n",
      "[2600]\ttraining's l1: 0.062741\tvalid_1's l1: 0.170767\n",
      "[2700]\ttraining's l1: 0.0608836\tvalid_1's l1: 0.170272\n",
      "[2800]\ttraining's l1: 0.0591503\tvalid_1's l1: 0.169774\n",
      "[2900]\ttraining's l1: 0.0574445\tvalid_1's l1: 0.169314\n",
      "[3000]\ttraining's l1: 0.0558357\tvalid_1's l1: 0.168878\n",
      "[3100]\ttraining's l1: 0.0542675\tvalid_1's l1: 0.168466\n",
      "[3200]\ttraining's l1: 0.0528182\tvalid_1's l1: 0.168113\n",
      "[3300]\ttraining's l1: 0.051406\tvalid_1's l1: 0.167756\n",
      "[3400]\ttraining's l1: 0.050101\tvalid_1's l1: 0.167366\n",
      "[3500]\ttraining's l1: 0.0488395\tvalid_1's l1: 0.167095\n",
      "[3600]\ttraining's l1: 0.0476009\tvalid_1's l1: 0.166821\n",
      "[3700]\ttraining's l1: 0.0463968\tvalid_1's l1: 0.166542\n",
      "[3800]\ttraining's l1: 0.0452409\tvalid_1's l1: 0.166257\n",
      "[3900]\ttraining's l1: 0.0441195\tvalid_1's l1: 0.16603\n",
      "[4000]\ttraining's l1: 0.0430484\tvalid_1's l1: 0.1658\n",
      "[4100]\ttraining's l1: 0.0420042\tvalid_1's l1: 0.165592\n",
      "[4200]\ttraining's l1: 0.0410119\tvalid_1's l1: 0.16538\n",
      "[4300]\ttraining's l1: 0.0400562\tvalid_1's l1: 0.165178\n",
      "[4400]\ttraining's l1: 0.0391192\tvalid_1's l1: 0.16496\n",
      "[4500]\ttraining's l1: 0.0382511\tvalid_1's l1: 0.164779\n",
      "[4600]\ttraining's l1: 0.0374089\tvalid_1's l1: 0.164608\n",
      "[4700]\ttraining's l1: 0.0365868\tvalid_1's l1: 0.164421\n",
      "[4800]\ttraining's l1: 0.0357948\tvalid_1's l1: 0.164256\n",
      "[4900]\ttraining's l1: 0.0350286\tvalid_1's l1: 0.164116\n",
      "[5000]\ttraining's l1: 0.0342395\tvalid_1's l1: 0.16396\n",
      "[5100]\ttraining's l1: 0.0335027\tvalid_1's l1: 0.163815\n",
      "[5200]\ttraining's l1: 0.0327958\tvalid_1's l1: 0.163683\n",
      "[5300]\ttraining's l1: 0.0321014\tvalid_1's l1: 0.163546\n",
      "[5400]\ttraining's l1: 0.0314478\tvalid_1's l1: 0.163427\n",
      "[5500]\ttraining's l1: 0.0308035\tvalid_1's l1: 0.163291\n",
      "[5600]\ttraining's l1: 0.0301728\tvalid_1's l1: 0.163164\n",
      "[5700]\ttraining's l1: 0.02954\tvalid_1's l1: 0.163053\n",
      "[5800]\ttraining's l1: 0.0289388\tvalid_1's l1: 0.162953\n",
      "[5900]\ttraining's l1: 0.0284045\tvalid_1's l1: 0.162878\n",
      "[6000]\ttraining's l1: 0.0278496\tvalid_1's l1: 0.162787\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0278496\tvalid_1's l1: 0.162787\n",
      "2JHH Fold 2, logMAE: -1.8153101132847207\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.257846\tvalid_1's l1: 0.280217\n",
      "[200]\ttraining's l1: 0.209872\tvalid_1's l1: 0.243118\n",
      "[300]\ttraining's l1: 0.184927\tvalid_1's l1: 0.22634\n",
      "[400]\ttraining's l1: 0.167773\tvalid_1's l1: 0.216388\n",
      "[500]\ttraining's l1: 0.155248\tvalid_1's l1: 0.20952\n",
      "[600]\ttraining's l1: 0.144665\tvalid_1's l1: 0.203955\n",
      "[700]\ttraining's l1: 0.13527\tvalid_1's l1: 0.199084\n",
      "[800]\ttraining's l1: 0.127976\tvalid_1's l1: 0.195624\n",
      "[900]\ttraining's l1: 0.121333\tvalid_1's l1: 0.192624\n",
      "[1000]\ttraining's l1: 0.115216\tvalid_1's l1: 0.190134\n",
      "[1100]\ttraining's l1: 0.109715\tvalid_1's l1: 0.187897\n",
      "[1200]\ttraining's l1: 0.104774\tvalid_1's l1: 0.185828\n",
      "[1300]\ttraining's l1: 0.100109\tvalid_1's l1: 0.183956\n",
      "[1400]\ttraining's l1: 0.0961484\tvalid_1's l1: 0.182443\n",
      "[1500]\ttraining's l1: 0.0923622\tvalid_1's l1: 0.181197\n",
      "[1600]\ttraining's l1: 0.0887961\tvalid_1's l1: 0.179982\n",
      "[1700]\ttraining's l1: 0.0854978\tvalid_1's l1: 0.178893\n",
      "[1800]\ttraining's l1: 0.0823453\tvalid_1's l1: 0.177892\n",
      "[1900]\ttraining's l1: 0.0795457\tvalid_1's l1: 0.176927\n",
      "[2000]\ttraining's l1: 0.0767272\tvalid_1's l1: 0.176011\n",
      "[2100]\ttraining's l1: 0.0742042\tvalid_1's l1: 0.175276\n",
      "[2200]\ttraining's l1: 0.0718623\tvalid_1's l1: 0.174552\n",
      "[2300]\ttraining's l1: 0.06954\tvalid_1's l1: 0.173882\n",
      "[2400]\ttraining's l1: 0.0673378\tvalid_1's l1: 0.173283\n",
      "[2500]\ttraining's l1: 0.0652603\tvalid_1's l1: 0.172709\n",
      "[2600]\ttraining's l1: 0.063298\tvalid_1's l1: 0.172163\n",
      "[2700]\ttraining's l1: 0.0613907\tvalid_1's l1: 0.171689\n",
      "[2800]\ttraining's l1: 0.059623\tvalid_1's l1: 0.171233\n",
      "[2900]\ttraining's l1: 0.0579134\tvalid_1's l1: 0.170742\n",
      "[3000]\ttraining's l1: 0.0562628\tvalid_1's l1: 0.170332\n",
      "[3100]\ttraining's l1: 0.0546948\tvalid_1's l1: 0.169919\n",
      "[3200]\ttraining's l1: 0.0532526\tvalid_1's l1: 0.169563\n",
      "[3300]\ttraining's l1: 0.0518627\tvalid_1's l1: 0.169249\n",
      "[3400]\ttraining's l1: 0.0504894\tvalid_1's l1: 0.168904\n",
      "[3500]\ttraining's l1: 0.0491841\tvalid_1's l1: 0.168566\n",
      "[3600]\ttraining's l1: 0.0479589\tvalid_1's l1: 0.1683\n",
      "[3700]\ttraining's l1: 0.0467723\tvalid_1's l1: 0.167993\n",
      "[3800]\ttraining's l1: 0.0456153\tvalid_1's l1: 0.167751\n",
      "[3900]\ttraining's l1: 0.0444923\tvalid_1's l1: 0.167494\n",
      "[4000]\ttraining's l1: 0.0433893\tvalid_1's l1: 0.167265\n",
      "[4100]\ttraining's l1: 0.0423742\tvalid_1's l1: 0.167034\n",
      "[4200]\ttraining's l1: 0.0413915\tvalid_1's l1: 0.166827\n",
      "[4300]\ttraining's l1: 0.0404118\tvalid_1's l1: 0.166632\n",
      "[4400]\ttraining's l1: 0.0394575\tvalid_1's l1: 0.166406\n",
      "[4500]\ttraining's l1: 0.0385114\tvalid_1's l1: 0.166199\n",
      "[4600]\ttraining's l1: 0.0376383\tvalid_1's l1: 0.16604\n",
      "[4700]\ttraining's l1: 0.0368095\tvalid_1's l1: 0.16589\n",
      "[4800]\ttraining's l1: 0.0359902\tvalid_1's l1: 0.165719\n",
      "[4900]\ttraining's l1: 0.0351928\tvalid_1's l1: 0.16556\n",
      "[5000]\ttraining's l1: 0.0344681\tvalid_1's l1: 0.165429\n",
      "[5100]\ttraining's l1: 0.0337175\tvalid_1's l1: 0.16529\n",
      "[5200]\ttraining's l1: 0.0329833\tvalid_1's l1: 0.165135\n",
      "[5300]\ttraining's l1: 0.0322988\tvalid_1's l1: 0.165003\n",
      "[5400]\ttraining's l1: 0.0316291\tvalid_1's l1: 0.164882\n",
      "[5500]\ttraining's l1: 0.0310062\tvalid_1's l1: 0.164768\n",
      "[5600]\ttraining's l1: 0.0304013\tvalid_1's l1: 0.164644\n",
      "[5700]\ttraining's l1: 0.0297867\tvalid_1's l1: 0.164547\n",
      "[5800]\ttraining's l1: 0.0292083\tvalid_1's l1: 0.164466\n",
      "[5900]\ttraining's l1: 0.0286531\tvalid_1's l1: 0.164372\n",
      "[6000]\ttraining's l1: 0.0281035\tvalid_1's l1: 0.164295\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0281035\tvalid_1's l1: 0.164295\n",
      "2JHH Fold 3, logMAE: -1.8060908162157763\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.25636\tvalid_1's l1: 0.278595\n",
      "[200]\ttraining's l1: 0.208119\tvalid_1's l1: 0.241675\n",
      "[300]\ttraining's l1: 0.183135\tvalid_1's l1: 0.224685\n",
      "[400]\ttraining's l1: 0.167053\tvalid_1's l1: 0.214785\n",
      "[500]\ttraining's l1: 0.154047\tvalid_1's l1: 0.207277\n",
      "[600]\ttraining's l1: 0.143566\tvalid_1's l1: 0.201556\n",
      "[700]\ttraining's l1: 0.134558\tvalid_1's l1: 0.196866\n",
      "[800]\ttraining's l1: 0.127043\tvalid_1's l1: 0.193286\n",
      "[900]\ttraining's l1: 0.120451\tvalid_1's l1: 0.190234\n",
      "[1000]\ttraining's l1: 0.114702\tvalid_1's l1: 0.187797\n",
      "[1100]\ttraining's l1: 0.109595\tvalid_1's l1: 0.185685\n",
      "[1200]\ttraining's l1: 0.104702\tvalid_1's l1: 0.183752\n",
      "[1300]\ttraining's l1: 0.100384\tvalid_1's l1: 0.182045\n",
      "[1400]\ttraining's l1: 0.0962314\tvalid_1's l1: 0.180557\n",
      "[1500]\ttraining's l1: 0.0923288\tvalid_1's l1: 0.179136\n",
      "[1600]\ttraining's l1: 0.0887852\tvalid_1's l1: 0.177951\n",
      "[1700]\ttraining's l1: 0.0854981\tvalid_1's l1: 0.176787\n",
      "[1800]\ttraining's l1: 0.0824714\tvalid_1's l1: 0.175801\n",
      "[1900]\ttraining's l1: 0.0794783\tvalid_1's l1: 0.174783\n",
      "[2000]\ttraining's l1: 0.0768935\tvalid_1's l1: 0.173963\n",
      "[2100]\ttraining's l1: 0.0742738\tvalid_1's l1: 0.173224\n",
      "[2200]\ttraining's l1: 0.0719249\tvalid_1's l1: 0.172513\n",
      "[2300]\ttraining's l1: 0.0695407\tvalid_1's l1: 0.171828\n",
      "[2400]\ttraining's l1: 0.0673636\tvalid_1's l1: 0.171283\n",
      "[2500]\ttraining's l1: 0.0652174\tvalid_1's l1: 0.170757\n",
      "[2600]\ttraining's l1: 0.0632855\tvalid_1's l1: 0.170189\n",
      "[2700]\ttraining's l1: 0.0614949\tvalid_1's l1: 0.169727\n",
      "[2800]\ttraining's l1: 0.059676\tvalid_1's l1: 0.169252\n",
      "[2900]\ttraining's l1: 0.0579758\tvalid_1's l1: 0.168806\n",
      "[3000]\ttraining's l1: 0.0563371\tvalid_1's l1: 0.168427\n",
      "[3100]\ttraining's l1: 0.0548165\tvalid_1's l1: 0.168046\n",
      "[3200]\ttraining's l1: 0.0533084\tvalid_1's l1: 0.167678\n",
      "[3300]\ttraining's l1: 0.0518429\tvalid_1's l1: 0.167321\n",
      "[3400]\ttraining's l1: 0.0504355\tvalid_1's l1: 0.167006\n",
      "[3500]\ttraining's l1: 0.0491639\tvalid_1's l1: 0.166709\n",
      "[3600]\ttraining's l1: 0.0478629\tvalid_1's l1: 0.166406\n",
      "[3700]\ttraining's l1: 0.046667\tvalid_1's l1: 0.166174\n",
      "[3800]\ttraining's l1: 0.0455118\tvalid_1's l1: 0.165909\n",
      "[3900]\ttraining's l1: 0.0443758\tvalid_1's l1: 0.165647\n",
      "[4000]\ttraining's l1: 0.0432747\tvalid_1's l1: 0.165382\n",
      "[4100]\ttraining's l1: 0.0422482\tvalid_1's l1: 0.165137\n",
      "[4200]\ttraining's l1: 0.0412363\tvalid_1's l1: 0.16491\n",
      "[4300]\ttraining's l1: 0.040284\tvalid_1's l1: 0.164689\n",
      "[4400]\ttraining's l1: 0.0393774\tvalid_1's l1: 0.164497\n",
      "[4500]\ttraining's l1: 0.0384792\tvalid_1's l1: 0.164322\n",
      "[4600]\ttraining's l1: 0.0376258\tvalid_1's l1: 0.16416\n",
      "[4700]\ttraining's l1: 0.0368117\tvalid_1's l1: 0.164012\n",
      "[4800]\ttraining's l1: 0.0360305\tvalid_1's l1: 0.163873\n",
      "[4900]\ttraining's l1: 0.0352235\tvalid_1's l1: 0.163709\n",
      "[5000]\ttraining's l1: 0.0344519\tvalid_1's l1: 0.163566\n",
      "[5100]\ttraining's l1: 0.0337514\tvalid_1's l1: 0.163412\n",
      "[5200]\ttraining's l1: 0.0330279\tvalid_1's l1: 0.16328\n",
      "[5300]\ttraining's l1: 0.0323446\tvalid_1's l1: 0.163167\n",
      "[5400]\ttraining's l1: 0.0316663\tvalid_1's l1: 0.163057\n",
      "[5500]\ttraining's l1: 0.0310313\tvalid_1's l1: 0.162954\n",
      "[5600]\ttraining's l1: 0.0304161\tvalid_1's l1: 0.162844\n",
      "[5700]\ttraining's l1: 0.0298166\tvalid_1's l1: 0.162726\n",
      "[5800]\ttraining's l1: 0.0292248\tvalid_1's l1: 0.162628\n",
      "[5900]\ttraining's l1: 0.0286656\tvalid_1's l1: 0.162532\n",
      "[6000]\ttraining's l1: 0.0280943\tvalid_1's l1: 0.162436\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0280943\tvalid_1's l1: 0.162436\n",
      "2JHH Fold 4, logMAE: -1.8174693884284325\n",
      "*** Training Model for 2JHN ***\n",
      "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8',\n",
      "       'd_1_0', 'd_2_0', 'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0', 'd_4_1',\n",
      "       'd_4_2', 'd_4_3', 'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0', 'd_6_1',\n",
      "       'd_6_2', 'd_6_3', 'd_7_0', 'd_7_1', 'd_7_2', 'd_7_3', 'd_8_0', 'd_8_1',\n",
      "       'd_8_2', 'd_8_3', 'scalar_coupling_constant'],\n",
      "      dtype='object')\n",
      "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8',\n",
      "       'd_1_0', 'd_2_0', 'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0', 'd_4_1',\n",
      "       'd_4_2', 'd_4_3', 'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0', 'd_6_1',\n",
      "       'd_6_2', 'd_6_3', 'd_7_0', 'd_7_1', 'd_7_2', 'd_7_3', 'd_8_0', 'd_8_1',\n",
      "       'd_8_2', 'd_8_3'],\n",
      "      dtype='object')\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.185305\tvalid_1's l1: 0.218522\n",
      "[200]\ttraining's l1: 0.149339\tvalid_1's l1: 0.193811\n",
      "[300]\ttraining's l1: 0.128883\tvalid_1's l1: 0.181127\n",
      "[400]\ttraining's l1: 0.115574\tvalid_1's l1: 0.17368\n",
      "[500]\ttraining's l1: 0.104749\tvalid_1's l1: 0.168167\n",
      "[600]\ttraining's l1: 0.0959389\tvalid_1's l1: 0.164021\n",
      "[700]\ttraining's l1: 0.0886439\tvalid_1's l1: 0.160809\n",
      "[800]\ttraining's l1: 0.0821489\tvalid_1's l1: 0.157838\n",
      "[900]\ttraining's l1: 0.0766242\tvalid_1's l1: 0.155308\n",
      "[1000]\ttraining's l1: 0.0717065\tvalid_1's l1: 0.153556\n",
      "[1100]\ttraining's l1: 0.067389\tvalid_1's l1: 0.152021\n",
      "[1200]\ttraining's l1: 0.0634243\tvalid_1's l1: 0.150703\n",
      "[1300]\ttraining's l1: 0.0599006\tvalid_1's l1: 0.149468\n",
      "[1400]\ttraining's l1: 0.0565502\tvalid_1's l1: 0.14842\n",
      "[1500]\ttraining's l1: 0.0536731\tvalid_1's l1: 0.147564\n",
      "[1600]\ttraining's l1: 0.0509061\tvalid_1's l1: 0.146578\n",
      "[1700]\ttraining's l1: 0.0483432\tvalid_1's l1: 0.145792\n",
      "[1800]\ttraining's l1: 0.0459614\tvalid_1's l1: 0.145102\n",
      "[1900]\ttraining's l1: 0.0438379\tvalid_1's l1: 0.144429\n",
      "[2000]\ttraining's l1: 0.0418818\tvalid_1's l1: 0.143851\n",
      "[2100]\ttraining's l1: 0.0400604\tvalid_1's l1: 0.143425\n",
      "[2200]\ttraining's l1: 0.0383976\tvalid_1's l1: 0.142988\n",
      "[2300]\ttraining's l1: 0.0368274\tvalid_1's l1: 0.142637\n",
      "[2400]\ttraining's l1: 0.0353378\tvalid_1's l1: 0.142199\n",
      "[2500]\ttraining's l1: 0.0338911\tvalid_1's l1: 0.141901\n",
      "[2600]\ttraining's l1: 0.0325465\tvalid_1's l1: 0.141596\n",
      "[2700]\ttraining's l1: 0.0312681\tvalid_1's l1: 0.141252\n",
      "[2800]\ttraining's l1: 0.0300837\tvalid_1's l1: 0.140973\n",
      "[2900]\ttraining's l1: 0.0288921\tvalid_1's l1: 0.140686\n",
      "[3000]\ttraining's l1: 0.0278158\tvalid_1's l1: 0.140479\n",
      "[3100]\ttraining's l1: 0.0267899\tvalid_1's l1: 0.140246\n",
      "[3200]\ttraining's l1: 0.0258116\tvalid_1's l1: 0.14002\n",
      "[3300]\ttraining's l1: 0.0249099\tvalid_1's l1: 0.139822\n",
      "[3400]\ttraining's l1: 0.0240518\tvalid_1's l1: 0.139642\n",
      "[3500]\ttraining's l1: 0.0232113\tvalid_1's l1: 0.139457\n",
      "[3600]\ttraining's l1: 0.0224276\tvalid_1's l1: 0.139325\n",
      "[3700]\ttraining's l1: 0.0216807\tvalid_1's l1: 0.139174\n",
      "[3800]\ttraining's l1: 0.0209754\tvalid_1's l1: 0.139006\n",
      "[3900]\ttraining's l1: 0.0202643\tvalid_1's l1: 0.138866\n",
      "[4000]\ttraining's l1: 0.0196272\tvalid_1's l1: 0.138736\n",
      "[4100]\ttraining's l1: 0.019017\tvalid_1's l1: 0.1386\n",
      "[4200]\ttraining's l1: 0.0184307\tvalid_1's l1: 0.138492\n",
      "[4300]\ttraining's l1: 0.0178693\tvalid_1's l1: 0.138372\n",
      "[4400]\ttraining's l1: 0.017315\tvalid_1's l1: 0.138253\n",
      "[4500]\ttraining's l1: 0.0168159\tvalid_1's l1: 0.138172\n",
      "[4600]\ttraining's l1: 0.0163301\tvalid_1's l1: 0.138079\n",
      "[4700]\ttraining's l1: 0.0158762\tvalid_1's l1: 0.138012\n",
      "[4800]\ttraining's l1: 0.0154237\tvalid_1's l1: 0.137926\n",
      "[4900]\ttraining's l1: 0.0149817\tvalid_1's l1: 0.137879\n",
      "[5000]\ttraining's l1: 0.0145694\tvalid_1's l1: 0.137798\n",
      "[5100]\ttraining's l1: 0.0141845\tvalid_1's l1: 0.137719\n",
      "[5200]\ttraining's l1: 0.0138182\tvalid_1's l1: 0.137654\n",
      "[5300]\ttraining's l1: 0.0134732\tvalid_1's l1: 0.137596\n",
      "[5400]\ttraining's l1: 0.0131489\tvalid_1's l1: 0.13755\n",
      "[5500]\ttraining's l1: 0.0128332\tvalid_1's l1: 0.137499\n",
      "[5600]\ttraining's l1: 0.0125305\tvalid_1's l1: 0.137452\n",
      "[5700]\ttraining's l1: 0.0122419\tvalid_1's l1: 0.137399\n",
      "[5800]\ttraining's l1: 0.0119604\tvalid_1's l1: 0.137355\n",
      "[5900]\ttraining's l1: 0.0116839\tvalid_1's l1: 0.137297\n",
      "[6000]\ttraining's l1: 0.0114265\tvalid_1's l1: 0.13727\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0114265\tvalid_1's l1: 0.13727\n",
      "2JHN Fold 0, logMAE: -1.9858019344142148\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.187662\tvalid_1's l1: 0.218777\n",
      "[200]\ttraining's l1: 0.150456\tvalid_1's l1: 0.192193\n",
      "[300]\ttraining's l1: 0.129789\tvalid_1's l1: 0.179148\n",
      "[400]\ttraining's l1: 0.115514\tvalid_1's l1: 0.171603\n",
      "[500]\ttraining's l1: 0.104927\tvalid_1's l1: 0.166111\n",
      "[600]\ttraining's l1: 0.0961584\tvalid_1's l1: 0.161521\n",
      "[700]\ttraining's l1: 0.0888011\tvalid_1's l1: 0.15849\n",
      "[800]\ttraining's l1: 0.0824592\tvalid_1's l1: 0.155671\n",
      "[900]\ttraining's l1: 0.0769605\tvalid_1's l1: 0.15336\n",
      "[1000]\ttraining's l1: 0.0721228\tvalid_1's l1: 0.151473\n",
      "[1100]\ttraining's l1: 0.0677516\tvalid_1's l1: 0.150013\n",
      "[1200]\ttraining's l1: 0.0637898\tvalid_1's l1: 0.148613\n",
      "[1300]\ttraining's l1: 0.0603027\tvalid_1's l1: 0.147268\n",
      "[1400]\ttraining's l1: 0.0570157\tvalid_1's l1: 0.146177\n",
      "[1500]\ttraining's l1: 0.0540505\tvalid_1's l1: 0.145384\n",
      "[1600]\ttraining's l1: 0.0513013\tvalid_1's l1: 0.144626\n",
      "[1700]\ttraining's l1: 0.0486641\tvalid_1's l1: 0.143672\n",
      "[1800]\ttraining's l1: 0.046291\tvalid_1's l1: 0.143043\n",
      "[1900]\ttraining's l1: 0.0441018\tvalid_1's l1: 0.142462\n",
      "[2000]\ttraining's l1: 0.0421121\tvalid_1's l1: 0.141821\n",
      "[2100]\ttraining's l1: 0.040239\tvalid_1's l1: 0.141273\n",
      "[2200]\ttraining's l1: 0.0384707\tvalid_1's l1: 0.140852\n",
      "[2300]\ttraining's l1: 0.0367757\tvalid_1's l1: 0.140403\n",
      "[2400]\ttraining's l1: 0.035176\tvalid_1's l1: 0.139954\n",
      "[2500]\ttraining's l1: 0.033811\tvalid_1's l1: 0.139648\n",
      "[2600]\ttraining's l1: 0.0324508\tvalid_1's l1: 0.139352\n",
      "[2700]\ttraining's l1: 0.0312034\tvalid_1's l1: 0.139035\n",
      "[2800]\ttraining's l1: 0.030025\tvalid_1's l1: 0.1388\n",
      "[2900]\ttraining's l1: 0.0288808\tvalid_1's l1: 0.138549\n",
      "[3000]\ttraining's l1: 0.0277939\tvalid_1's l1: 0.138266\n",
      "[3100]\ttraining's l1: 0.0267544\tvalid_1's l1: 0.138054\n",
      "[3200]\ttraining's l1: 0.0257602\tvalid_1's l1: 0.137855\n",
      "[3300]\ttraining's l1: 0.0248435\tvalid_1's l1: 0.13764\n",
      "[3400]\ttraining's l1: 0.0239767\tvalid_1's l1: 0.13746\n",
      "[3500]\ttraining's l1: 0.0231414\tvalid_1's l1: 0.137287\n",
      "[3600]\ttraining's l1: 0.0223278\tvalid_1's l1: 0.137127\n",
      "[3700]\ttraining's l1: 0.0215761\tvalid_1's l1: 0.136968\n",
      "[3800]\ttraining's l1: 0.0208332\tvalid_1's l1: 0.136817\n",
      "[3900]\ttraining's l1: 0.0201514\tvalid_1's l1: 0.136701\n",
      "[4000]\ttraining's l1: 0.0195268\tvalid_1's l1: 0.136598\n",
      "[4100]\ttraining's l1: 0.0189164\tvalid_1's l1: 0.13648\n",
      "[4200]\ttraining's l1: 0.01834\tvalid_1's l1: 0.136385\n",
      "[4300]\ttraining's l1: 0.0177705\tvalid_1's l1: 0.136304\n",
      "[4400]\ttraining's l1: 0.0172755\tvalid_1's l1: 0.1362\n",
      "[4500]\ttraining's l1: 0.0167677\tvalid_1's l1: 0.136095\n",
      "[4600]\ttraining's l1: 0.0162668\tvalid_1's l1: 0.136006\n",
      "[4700]\ttraining's l1: 0.0158037\tvalid_1's l1: 0.135925\n",
      "[4800]\ttraining's l1: 0.0153665\tvalid_1's l1: 0.135863\n",
      "[4900]\ttraining's l1: 0.0149401\tvalid_1's l1: 0.135792\n",
      "[5000]\ttraining's l1: 0.014537\tvalid_1's l1: 0.135712\n",
      "[5100]\ttraining's l1: 0.0141664\tvalid_1's l1: 0.135653\n",
      "[5200]\ttraining's l1: 0.0138032\tvalid_1's l1: 0.135579\n",
      "[5300]\ttraining's l1: 0.0134595\tvalid_1's l1: 0.135529\n",
      "[5400]\ttraining's l1: 0.0131414\tvalid_1's l1: 0.135472\n",
      "[5500]\ttraining's l1: 0.012821\tvalid_1's l1: 0.135417\n",
      "[5600]\ttraining's l1: 0.0125236\tvalid_1's l1: 0.135375\n",
      "[5700]\ttraining's l1: 0.0122233\tvalid_1's l1: 0.135329\n",
      "[5800]\ttraining's l1: 0.0119462\tvalid_1's l1: 0.13528\n",
      "[5900]\ttraining's l1: 0.0116678\tvalid_1's l1: 0.135239\n",
      "[6000]\ttraining's l1: 0.0114118\tvalid_1's l1: 0.135189\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0114118\tvalid_1's l1: 0.135189\n",
      "2JHN Fold 1, logMAE: -2.001079952505093\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.186204\tvalid_1's l1: 0.214531\n",
      "[200]\ttraining's l1: 0.1508\tvalid_1's l1: 0.189666\n",
      "[300]\ttraining's l1: 0.131311\tvalid_1's l1: 0.177845\n",
      "[400]\ttraining's l1: 0.117412\tvalid_1's l1: 0.170141\n",
      "[500]\ttraining's l1: 0.106382\tvalid_1's l1: 0.164794\n",
      "[600]\ttraining's l1: 0.0972885\tvalid_1's l1: 0.160509\n",
      "[700]\ttraining's l1: 0.0900095\tvalid_1's l1: 0.157413\n",
      "[800]\ttraining's l1: 0.083415\tvalid_1's l1: 0.154578\n",
      "[900]\ttraining's l1: 0.0777552\tvalid_1's l1: 0.15244\n",
      "[1000]\ttraining's l1: 0.0728171\tvalid_1's l1: 0.150646\n",
      "[1100]\ttraining's l1: 0.0686169\tvalid_1's l1: 0.1492\n",
      "[1200]\ttraining's l1: 0.0644183\tvalid_1's l1: 0.147747\n",
      "[1300]\ttraining's l1: 0.0607592\tvalid_1's l1: 0.146544\n",
      "[1400]\ttraining's l1: 0.057446\tvalid_1's l1: 0.145499\n",
      "[1500]\ttraining's l1: 0.0544211\tvalid_1's l1: 0.144576\n",
      "[1600]\ttraining's l1: 0.0516709\tvalid_1's l1: 0.143824\n",
      "[1700]\ttraining's l1: 0.0492089\tvalid_1's l1: 0.143069\n",
      "[1800]\ttraining's l1: 0.046782\tvalid_1's l1: 0.142381\n",
      "[1900]\ttraining's l1: 0.0446306\tvalid_1's l1: 0.141781\n",
      "[2000]\ttraining's l1: 0.0425062\tvalid_1's l1: 0.141171\n",
      "[2100]\ttraining's l1: 0.0406162\tvalid_1's l1: 0.140677\n",
      "[2200]\ttraining's l1: 0.0388569\tvalid_1's l1: 0.140303\n",
      "[2300]\ttraining's l1: 0.0372325\tvalid_1's l1: 0.13989\n",
      "[2400]\ttraining's l1: 0.035617\tvalid_1's l1: 0.139506\n",
      "[2500]\ttraining's l1: 0.0341739\tvalid_1's l1: 0.139146\n",
      "[2600]\ttraining's l1: 0.0328312\tvalid_1's l1: 0.138861\n",
      "[2700]\ttraining's l1: 0.0315061\tvalid_1's l1: 0.138514\n",
      "[2800]\ttraining's l1: 0.0302942\tvalid_1's l1: 0.138259\n",
      "[2900]\ttraining's l1: 0.0291047\tvalid_1's l1: 0.138053\n",
      "[3000]\ttraining's l1: 0.027994\tvalid_1's l1: 0.137796\n",
      "[3100]\ttraining's l1: 0.0269474\tvalid_1's l1: 0.137572\n",
      "[3200]\ttraining's l1: 0.0259374\tvalid_1's l1: 0.137343\n",
      "[3300]\ttraining's l1: 0.0250536\tvalid_1's l1: 0.137153\n",
      "[3400]\ttraining's l1: 0.0241449\tvalid_1's l1: 0.136978\n",
      "[3500]\ttraining's l1: 0.0233225\tvalid_1's l1: 0.136815\n",
      "[3600]\ttraining's l1: 0.0225662\tvalid_1's l1: 0.136639\n",
      "[3700]\ttraining's l1: 0.0218109\tvalid_1's l1: 0.136509\n",
      "[3800]\ttraining's l1: 0.0210958\tvalid_1's l1: 0.136364\n",
      "[3900]\ttraining's l1: 0.0204235\tvalid_1's l1: 0.136237\n",
      "[4000]\ttraining's l1: 0.0197857\tvalid_1's l1: 0.136129\n",
      "[4100]\ttraining's l1: 0.0191733\tvalid_1's l1: 0.135998\n",
      "[4200]\ttraining's l1: 0.0185902\tvalid_1's l1: 0.135907\n",
      "[4300]\ttraining's l1: 0.0180288\tvalid_1's l1: 0.13578\n",
      "[4400]\ttraining's l1: 0.0175003\tvalid_1's l1: 0.135698\n",
      "[4500]\ttraining's l1: 0.0169857\tvalid_1's l1: 0.135606\n",
      "[4600]\ttraining's l1: 0.0164889\tvalid_1's l1: 0.135499\n",
      "[4700]\ttraining's l1: 0.016017\tvalid_1's l1: 0.135402\n",
      "[4800]\ttraining's l1: 0.015569\tvalid_1's l1: 0.135336\n",
      "[4900]\ttraining's l1: 0.015151\tvalid_1's l1: 0.135275\n",
      "[5000]\ttraining's l1: 0.014752\tvalid_1's l1: 0.135203\n",
      "[5100]\ttraining's l1: 0.014353\tvalid_1's l1: 0.135129\n",
      "[5200]\ttraining's l1: 0.0139894\tvalid_1's l1: 0.135064\n",
      "[5300]\ttraining's l1: 0.0136359\tvalid_1's l1: 0.135001\n",
      "[5400]\ttraining's l1: 0.0132913\tvalid_1's l1: 0.134937\n",
      "[5500]\ttraining's l1: 0.0129706\tvalid_1's l1: 0.134884\n",
      "[5600]\ttraining's l1: 0.012662\tvalid_1's l1: 0.134838\n",
      "[5700]\ttraining's l1: 0.0123687\tvalid_1's l1: 0.134781\n",
      "[5800]\ttraining's l1: 0.0120816\tvalid_1's l1: 0.134731\n",
      "[5900]\ttraining's l1: 0.0118058\tvalid_1's l1: 0.134686\n",
      "[6000]\ttraining's l1: 0.0115493\tvalid_1's l1: 0.134653\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0115493\tvalid_1's l1: 0.134653\n",
      "2JHN Fold 2, logMAE: -2.0050543968326613\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.18702\tvalid_1's l1: 0.21641\n",
      "[200]\ttraining's l1: 0.150089\tvalid_1's l1: 0.190172\n",
      "[300]\ttraining's l1: 0.130023\tvalid_1's l1: 0.177589\n",
      "[400]\ttraining's l1: 0.115824\tvalid_1's l1: 0.169517\n",
      "[500]\ttraining's l1: 0.104842\tvalid_1's l1: 0.163994\n",
      "[600]\ttraining's l1: 0.0965384\tvalid_1's l1: 0.159822\n",
      "[700]\ttraining's l1: 0.0893566\tvalid_1's l1: 0.156809\n",
      "[800]\ttraining's l1: 0.0828818\tvalid_1's l1: 0.154017\n",
      "[900]\ttraining's l1: 0.0775934\tvalid_1's l1: 0.152007\n",
      "[1000]\ttraining's l1: 0.0723714\tvalid_1's l1: 0.15002\n",
      "[1100]\ttraining's l1: 0.0680594\tvalid_1's l1: 0.14841\n",
      "[1200]\ttraining's l1: 0.0640745\tvalid_1's l1: 0.146988\n",
      "[1300]\ttraining's l1: 0.0604963\tvalid_1's l1: 0.14562\n",
      "[1400]\ttraining's l1: 0.0572396\tvalid_1's l1: 0.14462\n",
      "[1500]\ttraining's l1: 0.0542822\tvalid_1's l1: 0.143744\n",
      "[1600]\ttraining's l1: 0.051629\tvalid_1's l1: 0.143014\n",
      "[1700]\ttraining's l1: 0.0490798\tvalid_1's l1: 0.142268\n",
      "[1800]\ttraining's l1: 0.0466917\tvalid_1's l1: 0.14158\n",
      "[1900]\ttraining's l1: 0.0445851\tvalid_1's l1: 0.140983\n",
      "[2000]\ttraining's l1: 0.0425014\tvalid_1's l1: 0.14044\n",
      "[2100]\ttraining's l1: 0.0406173\tvalid_1's l1: 0.139896\n",
      "[2200]\ttraining's l1: 0.0388897\tvalid_1's l1: 0.139457\n",
      "[2300]\ttraining's l1: 0.0372398\tvalid_1's l1: 0.139128\n",
      "[2400]\ttraining's l1: 0.0357236\tvalid_1's l1: 0.138751\n",
      "[2500]\ttraining's l1: 0.0342761\tvalid_1's l1: 0.138431\n",
      "[2600]\ttraining's l1: 0.0329019\tvalid_1's l1: 0.138134\n",
      "[2700]\ttraining's l1: 0.031593\tvalid_1's l1: 0.137803\n",
      "[2800]\ttraining's l1: 0.030321\tvalid_1's l1: 0.137536\n",
      "[2900]\ttraining's l1: 0.0291468\tvalid_1's l1: 0.137308\n",
      "[3000]\ttraining's l1: 0.0280351\tvalid_1's l1: 0.137062\n",
      "[3100]\ttraining's l1: 0.0269986\tvalid_1's l1: 0.13684\n",
      "[3200]\ttraining's l1: 0.0260473\tvalid_1's l1: 0.136636\n",
      "[3300]\ttraining's l1: 0.0251167\tvalid_1's l1: 0.136452\n",
      "[3400]\ttraining's l1: 0.0242296\tvalid_1's l1: 0.136265\n",
      "[3500]\ttraining's l1: 0.0233701\tvalid_1's l1: 0.136077\n",
      "[3600]\ttraining's l1: 0.0225644\tvalid_1's l1: 0.13592\n",
      "[3700]\ttraining's l1: 0.0218069\tvalid_1's l1: 0.135814\n",
      "[3800]\ttraining's l1: 0.0210674\tvalid_1's l1: 0.13564\n",
      "[3900]\ttraining's l1: 0.0203754\tvalid_1's l1: 0.135534\n",
      "[4000]\ttraining's l1: 0.0197235\tvalid_1's l1: 0.135435\n",
      "[4100]\ttraining's l1: 0.0191327\tvalid_1's l1: 0.135318\n",
      "[4200]\ttraining's l1: 0.0185376\tvalid_1's l1: 0.135198\n",
      "[4300]\ttraining's l1: 0.0179825\tvalid_1's l1: 0.13512\n",
      "[4400]\ttraining's l1: 0.017463\tvalid_1's l1: 0.135029\n",
      "[4500]\ttraining's l1: 0.0169636\tvalid_1's l1: 0.13494\n",
      "[4600]\ttraining's l1: 0.016479\tvalid_1's l1: 0.134864\n",
      "[4700]\ttraining's l1: 0.0160038\tvalid_1's l1: 0.134779\n",
      "[4800]\ttraining's l1: 0.0155407\tvalid_1's l1: 0.134703\n",
      "[4900]\ttraining's l1: 0.0151128\tvalid_1's l1: 0.134619\n",
      "[5000]\ttraining's l1: 0.0147085\tvalid_1's l1: 0.134573\n",
      "[5100]\ttraining's l1: 0.0143217\tvalid_1's l1: 0.134519\n",
      "[5200]\ttraining's l1: 0.0139505\tvalid_1's l1: 0.134463\n",
      "[5300]\ttraining's l1: 0.0136007\tvalid_1's l1: 0.134411\n",
      "[5400]\ttraining's l1: 0.0132707\tvalid_1's l1: 0.134355\n",
      "[5500]\ttraining's l1: 0.0129451\tvalid_1's l1: 0.134296\n",
      "[5600]\ttraining's l1: 0.012637\tvalid_1's l1: 0.134236\n",
      "[5700]\ttraining's l1: 0.0123412\tvalid_1's l1: 0.134196\n",
      "[5800]\ttraining's l1: 0.0120576\tvalid_1's l1: 0.134156\n",
      "[5900]\ttraining's l1: 0.0117904\tvalid_1's l1: 0.134114\n",
      "[6000]\ttraining's l1: 0.0115282\tvalid_1's l1: 0.134072\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0115282\tvalid_1's l1: 0.134072\n",
      "2JHN Fold 3, logMAE: -2.0093786910196494\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.184763\tvalid_1's l1: 0.214382\n",
      "[200]\ttraining's l1: 0.149335\tvalid_1's l1: 0.188745\n",
      "[300]\ttraining's l1: 0.129373\tvalid_1's l1: 0.176887\n",
      "[400]\ttraining's l1: 0.115374\tvalid_1's l1: 0.168987\n",
      "[500]\ttraining's l1: 0.104763\tvalid_1's l1: 0.16365\n",
      "[600]\ttraining's l1: 0.0960526\tvalid_1's l1: 0.159375\n",
      "[700]\ttraining's l1: 0.088762\tvalid_1's l1: 0.156241\n",
      "[800]\ttraining's l1: 0.0822655\tvalid_1's l1: 0.153516\n",
      "[900]\ttraining's l1: 0.0769367\tvalid_1's l1: 0.151587\n",
      "[1000]\ttraining's l1: 0.0722185\tvalid_1's l1: 0.14991\n",
      "[1100]\ttraining's l1: 0.0678883\tvalid_1's l1: 0.148269\n",
      "[1200]\ttraining's l1: 0.0637322\tvalid_1's l1: 0.146929\n",
      "[1300]\ttraining's l1: 0.0603243\tvalid_1's l1: 0.14574\n",
      "[1400]\ttraining's l1: 0.0572414\tvalid_1's l1: 0.144636\n",
      "[1500]\ttraining's l1: 0.054255\tvalid_1's l1: 0.143783\n",
      "[1600]\ttraining's l1: 0.0514906\tvalid_1's l1: 0.142953\n",
      "[1700]\ttraining's l1: 0.048981\tvalid_1's l1: 0.142204\n",
      "[1800]\ttraining's l1: 0.0466086\tvalid_1's l1: 0.14156\n",
      "[1900]\ttraining's l1: 0.0443728\tvalid_1's l1: 0.140864\n",
      "[2000]\ttraining's l1: 0.0423822\tvalid_1's l1: 0.14029\n",
      "[2100]\ttraining's l1: 0.0404295\tvalid_1's l1: 0.13973\n",
      "[2200]\ttraining's l1: 0.0387605\tvalid_1's l1: 0.139252\n",
      "[2300]\ttraining's l1: 0.0371339\tvalid_1's l1: 0.138798\n",
      "[2400]\ttraining's l1: 0.0355667\tvalid_1's l1: 0.138393\n",
      "[2500]\ttraining's l1: 0.0341458\tvalid_1's l1: 0.13804\n",
      "[2600]\ttraining's l1: 0.0327341\tvalid_1's l1: 0.137737\n",
      "[2700]\ttraining's l1: 0.0314166\tvalid_1's l1: 0.137379\n",
      "[2800]\ttraining's l1: 0.0301747\tvalid_1's l1: 0.137099\n",
      "[2900]\ttraining's l1: 0.0290063\tvalid_1's l1: 0.136813\n",
      "[3000]\ttraining's l1: 0.027945\tvalid_1's l1: 0.136547\n",
      "[3100]\ttraining's l1: 0.026918\tvalid_1's l1: 0.136312\n",
      "[3200]\ttraining's l1: 0.0259754\tvalid_1's l1: 0.136112\n",
      "[3300]\ttraining's l1: 0.0250228\tvalid_1's l1: 0.135923\n",
      "[3400]\ttraining's l1: 0.0241105\tvalid_1's l1: 0.13572\n",
      "[3500]\ttraining's l1: 0.0232935\tvalid_1's l1: 0.135571\n",
      "[3600]\ttraining's l1: 0.022446\tvalid_1's l1: 0.135383\n",
      "[3700]\ttraining's l1: 0.0216881\tvalid_1's l1: 0.135214\n",
      "[3800]\ttraining's l1: 0.0209564\tvalid_1's l1: 0.13508\n",
      "[3900]\ttraining's l1: 0.0203082\tvalid_1's l1: 0.134952\n",
      "[4000]\ttraining's l1: 0.0196828\tvalid_1's l1: 0.134821\n",
      "[4100]\ttraining's l1: 0.0190443\tvalid_1's l1: 0.134723\n",
      "[4200]\ttraining's l1: 0.0184636\tvalid_1's l1: 0.134626\n",
      "[4300]\ttraining's l1: 0.0178983\tvalid_1's l1: 0.134514\n",
      "[4400]\ttraining's l1: 0.017366\tvalid_1's l1: 0.134421\n",
      "[4500]\ttraining's l1: 0.0168555\tvalid_1's l1: 0.134344\n",
      "[4600]\ttraining's l1: 0.016381\tvalid_1's l1: 0.134287\n",
      "[4700]\ttraining's l1: 0.0159061\tvalid_1's l1: 0.134198\n",
      "[4800]\ttraining's l1: 0.0154665\tvalid_1's l1: 0.134112\n",
      "[4900]\ttraining's l1: 0.0150478\tvalid_1's l1: 0.134036\n",
      "[5000]\ttraining's l1: 0.0146566\tvalid_1's l1: 0.133973\n",
      "[5100]\ttraining's l1: 0.0142738\tvalid_1's l1: 0.133908\n",
      "[5200]\ttraining's l1: 0.0139177\tvalid_1's l1: 0.133854\n",
      "[5300]\ttraining's l1: 0.0135566\tvalid_1's l1: 0.133778\n",
      "[5400]\ttraining's l1: 0.0132088\tvalid_1's l1: 0.133722\n",
      "[5500]\ttraining's l1: 0.0128845\tvalid_1's l1: 0.133668\n",
      "[5600]\ttraining's l1: 0.0125752\tvalid_1's l1: 0.133633\n",
      "[5700]\ttraining's l1: 0.0122858\tvalid_1's l1: 0.133583\n",
      "[5800]\ttraining's l1: 0.0119967\tvalid_1's l1: 0.13353\n",
      "[5900]\ttraining's l1: 0.0117326\tvalid_1's l1: 0.133485\n",
      "[6000]\ttraining's l1: 0.0114714\tvalid_1's l1: 0.133446\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0114714\tvalid_1's l1: 0.133446\n",
      "2JHN Fold 4, logMAE: -2.0140553439858353\n",
      "*** Training Model for 2JHC ***\n",
      "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8',\n",
      "       'd_1_0', 'd_2_0', 'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0', 'd_4_1',\n",
      "       'd_4_2', 'd_4_3', 'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0', 'd_6_1',\n",
      "       'd_6_2', 'd_6_3', 'd_7_0', 'd_7_1', 'd_7_2', 'd_7_3', 'd_8_0', 'd_8_1',\n",
      "       'd_8_2', 'd_8_3', 'scalar_coupling_constant'],\n",
      "      dtype='object')\n",
      "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8',\n",
      "       'd_1_0', 'd_2_0', 'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0', 'd_4_1',\n",
      "       'd_4_2', 'd_4_3', 'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0', 'd_6_1',\n",
      "       'd_6_2', 'd_6_3', 'd_7_0', 'd_7_1', 'd_7_2', 'd_7_3', 'd_8_0', 'd_8_1',\n",
      "       'd_8_2', 'd_8_3'],\n",
      "      dtype='object')\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.499492\tvalid_1's l1: 0.514592\n",
      "[200]\ttraining's l1: 0.422753\tvalid_1's l1: 0.445078\n",
      "[300]\ttraining's l1: 0.382761\tvalid_1's l1: 0.41156\n",
      "[400]\ttraining's l1: 0.355125\tvalid_1's l1: 0.389621\n",
      "[500]\ttraining's l1: 0.333646\tvalid_1's l1: 0.373477\n",
      "[600]\ttraining's l1: 0.316385\tvalid_1's l1: 0.360918\n",
      "[700]\ttraining's l1: 0.30199\tvalid_1's l1: 0.350883\n",
      "[800]\ttraining's l1: 0.289795\tvalid_1's l1: 0.342671\n",
      "[900]\ttraining's l1: 0.27902\tvalid_1's l1: 0.335695\n",
      "[1000]\ttraining's l1: 0.269509\tvalid_1's l1: 0.329598\n",
      "[1100]\ttraining's l1: 0.26102\tvalid_1's l1: 0.324366\n",
      "[1200]\ttraining's l1: 0.253099\tvalid_1's l1: 0.319596\n",
      "[1300]\ttraining's l1: 0.245894\tvalid_1's l1: 0.315231\n",
      "[1400]\ttraining's l1: 0.239445\tvalid_1's l1: 0.31168\n",
      "[1500]\ttraining's l1: 0.233367\tvalid_1's l1: 0.30835\n",
      "[1600]\ttraining's l1: 0.22759\tvalid_1's l1: 0.305035\n",
      "[1700]\ttraining's l1: 0.222239\tvalid_1's l1: 0.302175\n",
      "[1800]\ttraining's l1: 0.217278\tvalid_1's l1: 0.299504\n",
      "[1900]\ttraining's l1: 0.212411\tvalid_1's l1: 0.296968\n",
      "[2000]\ttraining's l1: 0.207807\tvalid_1's l1: 0.294681\n",
      "[2100]\ttraining's l1: 0.203564\tvalid_1's l1: 0.29256\n",
      "[2200]\ttraining's l1: 0.199406\tvalid_1's l1: 0.290523\n",
      "[2300]\ttraining's l1: 0.195531\tvalid_1's l1: 0.288574\n",
      "[2400]\ttraining's l1: 0.191816\tvalid_1's l1: 0.286843\n",
      "[2500]\ttraining's l1: 0.188303\tvalid_1's l1: 0.285135\n",
      "[2600]\ttraining's l1: 0.184951\tvalid_1's l1: 0.283616\n",
      "[2700]\ttraining's l1: 0.18169\tvalid_1's l1: 0.282152\n",
      "[2800]\ttraining's l1: 0.178605\tvalid_1's l1: 0.28073\n",
      "[2900]\ttraining's l1: 0.175562\tvalid_1's l1: 0.279364\n",
      "[3000]\ttraining's l1: 0.1726\tvalid_1's l1: 0.27807\n",
      "[3100]\ttraining's l1: 0.169829\tvalid_1's l1: 0.276964\n",
      "[3200]\ttraining's l1: 0.167215\tvalid_1's l1: 0.275857\n",
      "[3300]\ttraining's l1: 0.164603\tvalid_1's l1: 0.274703\n",
      "[3400]\ttraining's l1: 0.162116\tvalid_1's l1: 0.273608\n",
      "[3500]\ttraining's l1: 0.159578\tvalid_1's l1: 0.272585\n",
      "[3600]\ttraining's l1: 0.157179\tvalid_1's l1: 0.271593\n",
      "[3700]\ttraining's l1: 0.154803\tvalid_1's l1: 0.270636\n",
      "[3800]\ttraining's l1: 0.152577\tvalid_1's l1: 0.269729\n",
      "[3900]\ttraining's l1: 0.150356\tvalid_1's l1: 0.268854\n",
      "[4000]\ttraining's l1: 0.148242\tvalid_1's l1: 0.268004\n",
      "[4100]\ttraining's l1: 0.14618\tvalid_1's l1: 0.267162\n",
      "[4200]\ttraining's l1: 0.144108\tvalid_1's l1: 0.26641\n",
      "[4300]\ttraining's l1: 0.142126\tvalid_1's l1: 0.265666\n",
      "[4400]\ttraining's l1: 0.140192\tvalid_1's l1: 0.264903\n",
      "[4500]\ttraining's l1: 0.138335\tvalid_1's l1: 0.264221\n",
      "[4600]\ttraining's l1: 0.136572\tvalid_1's l1: 0.263566\n",
      "[4700]\ttraining's l1: 0.134869\tvalid_1's l1: 0.262913\n",
      "[4800]\ttraining's l1: 0.133198\tvalid_1's l1: 0.262316\n",
      "[4900]\ttraining's l1: 0.131519\tvalid_1's l1: 0.261678\n",
      "[5000]\ttraining's l1: 0.12991\tvalid_1's l1: 0.261084\n",
      "[5100]\ttraining's l1: 0.128361\tvalid_1's l1: 0.260594\n",
      "[5200]\ttraining's l1: 0.126798\tvalid_1's l1: 0.260079\n",
      "[5300]\ttraining's l1: 0.125273\tvalid_1's l1: 0.259502\n",
      "[5400]\ttraining's l1: 0.123755\tvalid_1's l1: 0.25895\n",
      "[5500]\ttraining's l1: 0.122321\tvalid_1's l1: 0.258416\n",
      "[5600]\ttraining's l1: 0.120928\tvalid_1's l1: 0.25793\n",
      "[5700]\ttraining's l1: 0.119514\tvalid_1's l1: 0.257412\n",
      "[5800]\ttraining's l1: 0.118191\tvalid_1's l1: 0.257025\n",
      "[5900]\ttraining's l1: 0.116842\tvalid_1's l1: 0.256531\n",
      "[6000]\ttraining's l1: 0.115541\tvalid_1's l1: 0.256066\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.115541\tvalid_1's l1: 0.256066\n",
      "2JHC Fold 0, logMAE: -1.3623214880303163\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.498693\tvalid_1's l1: 0.514296\n",
      "[200]\ttraining's l1: 0.420817\tvalid_1's l1: 0.444003\n",
      "[300]\ttraining's l1: 0.380637\tvalid_1's l1: 0.410379\n",
      "[400]\ttraining's l1: 0.351814\tvalid_1's l1: 0.387416\n",
      "[500]\ttraining's l1: 0.331549\tvalid_1's l1: 0.372217\n",
      "[600]\ttraining's l1: 0.314488\tvalid_1's l1: 0.359842\n",
      "[700]\ttraining's l1: 0.300442\tvalid_1's l1: 0.350117\n",
      "[800]\ttraining's l1: 0.288207\tvalid_1's l1: 0.341659\n",
      "[900]\ttraining's l1: 0.277641\tvalid_1's l1: 0.334673\n",
      "[1000]\ttraining's l1: 0.268169\tvalid_1's l1: 0.328645\n",
      "[1100]\ttraining's l1: 0.259851\tvalid_1's l1: 0.323492\n",
      "[1200]\ttraining's l1: 0.251973\tvalid_1's l1: 0.318701\n",
      "[1300]\ttraining's l1: 0.244815\tvalid_1's l1: 0.314308\n",
      "[1400]\ttraining's l1: 0.238135\tvalid_1's l1: 0.310438\n",
      "[1500]\ttraining's l1: 0.232036\tvalid_1's l1: 0.306994\n",
      "[1600]\ttraining's l1: 0.226515\tvalid_1's l1: 0.303888\n",
      "[1700]\ttraining's l1: 0.221034\tvalid_1's l1: 0.300917\n",
      "[1800]\ttraining's l1: 0.216133\tvalid_1's l1: 0.298352\n",
      "[1900]\ttraining's l1: 0.211489\tvalid_1's l1: 0.296039\n",
      "[2000]\ttraining's l1: 0.207168\tvalid_1's l1: 0.293863\n",
      "[2100]\ttraining's l1: 0.202839\tvalid_1's l1: 0.291641\n",
      "[2200]\ttraining's l1: 0.198705\tvalid_1's l1: 0.289534\n",
      "[2300]\ttraining's l1: 0.194909\tvalid_1's l1: 0.287766\n",
      "[2400]\ttraining's l1: 0.191365\tvalid_1's l1: 0.286049\n",
      "[2500]\ttraining's l1: 0.187899\tvalid_1's l1: 0.284356\n",
      "[2600]\ttraining's l1: 0.184582\tvalid_1's l1: 0.28276\n",
      "[2700]\ttraining's l1: 0.181386\tvalid_1's l1: 0.28129\n",
      "[2800]\ttraining's l1: 0.178284\tvalid_1's l1: 0.279869\n",
      "[2900]\ttraining's l1: 0.17528\tvalid_1's l1: 0.278546\n",
      "[3000]\ttraining's l1: 0.172412\tvalid_1's l1: 0.277294\n",
      "[3100]\ttraining's l1: 0.169594\tvalid_1's l1: 0.27606\n",
      "[3200]\ttraining's l1: 0.166948\tvalid_1's l1: 0.274929\n",
      "[3300]\ttraining's l1: 0.164343\tvalid_1's l1: 0.273822\n",
      "[3400]\ttraining's l1: 0.161794\tvalid_1's l1: 0.272732\n",
      "[3500]\ttraining's l1: 0.159388\tvalid_1's l1: 0.271761\n",
      "[3600]\ttraining's l1: 0.157017\tvalid_1's l1: 0.270777\n",
      "[3700]\ttraining's l1: 0.15473\tvalid_1's l1: 0.269825\n",
      "[3800]\ttraining's l1: 0.15247\tvalid_1's l1: 0.268924\n",
      "[3900]\ttraining's l1: 0.150261\tvalid_1's l1: 0.268079\n",
      "[4000]\ttraining's l1: 0.148125\tvalid_1's l1: 0.267251\n",
      "[4100]\ttraining's l1: 0.146063\tvalid_1's l1: 0.266497\n",
      "[4200]\ttraining's l1: 0.144086\tvalid_1's l1: 0.265706\n",
      "[4300]\ttraining's l1: 0.142162\tvalid_1's l1: 0.26498\n",
      "[4400]\ttraining's l1: 0.140256\tvalid_1's l1: 0.264284\n",
      "[4500]\ttraining's l1: 0.138461\tvalid_1's l1: 0.263629\n",
      "[4600]\ttraining's l1: 0.136688\tvalid_1's l1: 0.262945\n",
      "[4700]\ttraining's l1: 0.134962\tvalid_1's l1: 0.262327\n",
      "[4800]\ttraining's l1: 0.133252\tvalid_1's l1: 0.261705\n",
      "[4900]\ttraining's l1: 0.131624\tvalid_1's l1: 0.26111\n",
      "[5000]\ttraining's l1: 0.130005\tvalid_1's l1: 0.260467\n",
      "[5100]\ttraining's l1: 0.128452\tvalid_1's l1: 0.259922\n",
      "[5200]\ttraining's l1: 0.126935\tvalid_1's l1: 0.259373\n",
      "[5300]\ttraining's l1: 0.125416\tvalid_1's l1: 0.258817\n",
      "[5400]\ttraining's l1: 0.123935\tvalid_1's l1: 0.258249\n",
      "[5500]\ttraining's l1: 0.122488\tvalid_1's l1: 0.25775\n",
      "[5600]\ttraining's l1: 0.121054\tvalid_1's l1: 0.257279\n",
      "[5700]\ttraining's l1: 0.119698\tvalid_1's l1: 0.256765\n",
      "[5800]\ttraining's l1: 0.118343\tvalid_1's l1: 0.256299\n",
      "[5900]\ttraining's l1: 0.11705\tvalid_1's l1: 0.255875\n",
      "[6000]\ttraining's l1: 0.115735\tvalid_1's l1: 0.255425\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.115735\tvalid_1's l1: 0.255425\n",
      "2JHC Fold 1, logMAE: -1.3648267545770987\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.4998\tvalid_1's l1: 0.513611\n",
      "[200]\ttraining's l1: 0.422086\tvalid_1's l1: 0.443974\n",
      "[300]\ttraining's l1: 0.381406\tvalid_1's l1: 0.410147\n",
      "[400]\ttraining's l1: 0.353128\tvalid_1's l1: 0.387794\n",
      "[500]\ttraining's l1: 0.331883\tvalid_1's l1: 0.371832\n",
      "[600]\ttraining's l1: 0.315249\tvalid_1's l1: 0.359674\n",
      "[700]\ttraining's l1: 0.301065\tvalid_1's l1: 0.349616\n",
      "[800]\ttraining's l1: 0.288797\tvalid_1's l1: 0.341249\n",
      "[900]\ttraining's l1: 0.278038\tvalid_1's l1: 0.334243\n",
      "[1000]\ttraining's l1: 0.268634\tvalid_1's l1: 0.328153\n",
      "[1100]\ttraining's l1: 0.260086\tvalid_1's l1: 0.322667\n",
      "[1200]\ttraining's l1: 0.252435\tvalid_1's l1: 0.318108\n",
      "[1300]\ttraining's l1: 0.245352\tvalid_1's l1: 0.313962\n",
      "[1400]\ttraining's l1: 0.23868\tvalid_1's l1: 0.310087\n",
      "[1500]\ttraining's l1: 0.232608\tvalid_1's l1: 0.30655\n",
      "[1600]\ttraining's l1: 0.22696\tvalid_1's l1: 0.303407\n",
      "[1700]\ttraining's l1: 0.221713\tvalid_1's l1: 0.300564\n",
      "[1800]\ttraining's l1: 0.216785\tvalid_1's l1: 0.29793\n",
      "[1900]\ttraining's l1: 0.211987\tvalid_1's l1: 0.295423\n",
      "[2000]\ttraining's l1: 0.207555\tvalid_1's l1: 0.293146\n",
      "[2100]\ttraining's l1: 0.203263\tvalid_1's l1: 0.290943\n",
      "[2200]\ttraining's l1: 0.199102\tvalid_1's l1: 0.288913\n",
      "[2300]\ttraining's l1: 0.195216\tvalid_1's l1: 0.286939\n",
      "[2400]\ttraining's l1: 0.191508\tvalid_1's l1: 0.285132\n",
      "[2500]\ttraining's l1: 0.188066\tvalid_1's l1: 0.283497\n",
      "[2600]\ttraining's l1: 0.184753\tvalid_1's l1: 0.282011\n",
      "[2700]\ttraining's l1: 0.181541\tvalid_1's l1: 0.280495\n",
      "[2800]\ttraining's l1: 0.178423\tvalid_1's l1: 0.279144\n",
      "[2900]\ttraining's l1: 0.175376\tvalid_1's l1: 0.277801\n",
      "[3000]\ttraining's l1: 0.172496\tvalid_1's l1: 0.276538\n",
      "[3100]\ttraining's l1: 0.169643\tvalid_1's l1: 0.27524\n",
      "[3200]\ttraining's l1: 0.166969\tvalid_1's l1: 0.274036\n",
      "[3300]\ttraining's l1: 0.164285\tvalid_1's l1: 0.272946\n",
      "[3400]\ttraining's l1: 0.161771\tvalid_1's l1: 0.271878\n",
      "[3500]\ttraining's l1: 0.159343\tvalid_1's l1: 0.270886\n",
      "[3600]\ttraining's l1: 0.156967\tvalid_1's l1: 0.269901\n",
      "[3700]\ttraining's l1: 0.154693\tvalid_1's l1: 0.268981\n",
      "[3800]\ttraining's l1: 0.152413\tvalid_1's l1: 0.26806\n",
      "[3900]\ttraining's l1: 0.150223\tvalid_1's l1: 0.267156\n",
      "[4000]\ttraining's l1: 0.148141\tvalid_1's l1: 0.266354\n",
      "[4100]\ttraining's l1: 0.146099\tvalid_1's l1: 0.26557\n",
      "[4200]\ttraining's l1: 0.144139\tvalid_1's l1: 0.264838\n",
      "[4300]\ttraining's l1: 0.142194\tvalid_1's l1: 0.264119\n",
      "[4400]\ttraining's l1: 0.140328\tvalid_1's l1: 0.263441\n",
      "[4500]\ttraining's l1: 0.138517\tvalid_1's l1: 0.26279\n",
      "[4600]\ttraining's l1: 0.136692\tvalid_1's l1: 0.262131\n",
      "[4700]\ttraining's l1: 0.134986\tvalid_1's l1: 0.261505\n",
      "[4800]\ttraining's l1: 0.133228\tvalid_1's l1: 0.260877\n",
      "[4900]\ttraining's l1: 0.13154\tvalid_1's l1: 0.260315\n",
      "[5000]\ttraining's l1: 0.129944\tvalid_1's l1: 0.259768\n",
      "[5100]\ttraining's l1: 0.128421\tvalid_1's l1: 0.259223\n",
      "[5200]\ttraining's l1: 0.12686\tvalid_1's l1: 0.258691\n",
      "[5300]\ttraining's l1: 0.125331\tvalid_1's l1: 0.25813\n",
      "[5400]\ttraining's l1: 0.123892\tvalid_1's l1: 0.25756\n",
      "[5500]\ttraining's l1: 0.122442\tvalid_1's l1: 0.257048\n",
      "[5600]\ttraining's l1: 0.121019\tvalid_1's l1: 0.256555\n",
      "[5700]\ttraining's l1: 0.119649\tvalid_1's l1: 0.256081\n",
      "[5800]\ttraining's l1: 0.118244\tvalid_1's l1: 0.255568\n",
      "[5900]\ttraining's l1: 0.116918\tvalid_1's l1: 0.25513\n",
      "[6000]\ttraining's l1: 0.11563\tvalid_1's l1: 0.254701\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.11563\tvalid_1's l1: 0.254701\n",
      "2JHC Fold 2, logMAE: -1.367666788891122\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.5018\tvalid_1's l1: 0.518726\n",
      "[200]\ttraining's l1: 0.424078\tvalid_1's l1: 0.449273\n",
      "[300]\ttraining's l1: 0.383057\tvalid_1's l1: 0.414277\n",
      "[400]\ttraining's l1: 0.354639\tvalid_1's l1: 0.391761\n",
      "[500]\ttraining's l1: 0.333149\tvalid_1's l1: 0.37508\n",
      "[600]\ttraining's l1: 0.315809\tvalid_1's l1: 0.362204\n",
      "[700]\ttraining's l1: 0.301899\tvalid_1's l1: 0.352292\n",
      "[800]\ttraining's l1: 0.289735\tvalid_1's l1: 0.344065\n",
      "[900]\ttraining's l1: 0.279227\tvalid_1's l1: 0.337071\n",
      "[1000]\ttraining's l1: 0.269872\tvalid_1's l1: 0.331134\n",
      "[1100]\ttraining's l1: 0.261324\tvalid_1's l1: 0.325899\n",
      "[1200]\ttraining's l1: 0.25346\tvalid_1's l1: 0.321021\n",
      "[1300]\ttraining's l1: 0.24599\tvalid_1's l1: 0.316506\n",
      "[1400]\ttraining's l1: 0.23939\tvalid_1's l1: 0.312735\n",
      "[1500]\ttraining's l1: 0.233259\tvalid_1's l1: 0.309272\n",
      "[1600]\ttraining's l1: 0.227668\tvalid_1's l1: 0.306179\n",
      "[1700]\ttraining's l1: 0.222112\tvalid_1's l1: 0.303109\n",
      "[1800]\ttraining's l1: 0.216988\tvalid_1's l1: 0.300419\n",
      "[1900]\ttraining's l1: 0.21222\tvalid_1's l1: 0.298056\n",
      "[2000]\ttraining's l1: 0.207818\tvalid_1's l1: 0.295755\n",
      "[2100]\ttraining's l1: 0.203683\tvalid_1's l1: 0.293691\n",
      "[2200]\ttraining's l1: 0.199574\tvalid_1's l1: 0.291652\n",
      "[2300]\ttraining's l1: 0.195755\tvalid_1's l1: 0.289852\n",
      "[2400]\ttraining's l1: 0.192134\tvalid_1's l1: 0.288184\n",
      "[2500]\ttraining's l1: 0.188671\tvalid_1's l1: 0.286665\n",
      "[2600]\ttraining's l1: 0.185341\tvalid_1's l1: 0.285143\n",
      "[2700]\ttraining's l1: 0.182083\tvalid_1's l1: 0.283686\n",
      "[2800]\ttraining's l1: 0.178903\tvalid_1's l1: 0.282291\n",
      "[2900]\ttraining's l1: 0.175854\tvalid_1's l1: 0.280958\n",
      "[3000]\ttraining's l1: 0.172946\tvalid_1's l1: 0.279736\n",
      "[3100]\ttraining's l1: 0.170167\tvalid_1's l1: 0.278499\n",
      "[3200]\ttraining's l1: 0.167504\tvalid_1's l1: 0.277315\n",
      "[3300]\ttraining's l1: 0.164831\tvalid_1's l1: 0.276158\n",
      "[3400]\ttraining's l1: 0.162397\tvalid_1's l1: 0.275172\n",
      "[3500]\ttraining's l1: 0.159898\tvalid_1's l1: 0.274201\n",
      "[3600]\ttraining's l1: 0.157536\tvalid_1's l1: 0.273242\n",
      "[3700]\ttraining's l1: 0.155334\tvalid_1's l1: 0.272323\n",
      "[3800]\ttraining's l1: 0.153089\tvalid_1's l1: 0.27141\n",
      "[3900]\ttraining's l1: 0.150812\tvalid_1's l1: 0.270488\n",
      "[4000]\ttraining's l1: 0.14871\tvalid_1's l1: 0.269706\n",
      "[4100]\ttraining's l1: 0.146652\tvalid_1's l1: 0.268901\n",
      "[4200]\ttraining's l1: 0.144646\tvalid_1's l1: 0.268162\n",
      "[4300]\ttraining's l1: 0.14267\tvalid_1's l1: 0.267451\n",
      "[4400]\ttraining's l1: 0.140777\tvalid_1's l1: 0.26675\n",
      "[4500]\ttraining's l1: 0.138894\tvalid_1's l1: 0.266062\n",
      "[4600]\ttraining's l1: 0.13706\tvalid_1's l1: 0.265373\n",
      "[4700]\ttraining's l1: 0.135298\tvalid_1's l1: 0.264742\n",
      "[4800]\ttraining's l1: 0.133605\tvalid_1's l1: 0.264088\n",
      "[4900]\ttraining's l1: 0.131953\tvalid_1's l1: 0.263476\n",
      "[5000]\ttraining's l1: 0.130365\tvalid_1's l1: 0.262908\n",
      "[5100]\ttraining's l1: 0.128765\tvalid_1's l1: 0.262401\n",
      "[5200]\ttraining's l1: 0.1272\tvalid_1's l1: 0.261856\n",
      "[5300]\ttraining's l1: 0.125686\tvalid_1's l1: 0.261294\n",
      "[5400]\ttraining's l1: 0.124181\tvalid_1's l1: 0.260762\n",
      "[5500]\ttraining's l1: 0.122739\tvalid_1's l1: 0.260269\n",
      "[5600]\ttraining's l1: 0.121308\tvalid_1's l1: 0.25976\n",
      "[5700]\ttraining's l1: 0.119951\tvalid_1's l1: 0.259331\n",
      "[5800]\ttraining's l1: 0.118626\tvalid_1's l1: 0.258889\n",
      "[5900]\ttraining's l1: 0.11726\tvalid_1's l1: 0.258455\n",
      "[6000]\ttraining's l1: 0.115979\tvalid_1's l1: 0.257995\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.115979\tvalid_1's l1: 0.257995\n",
      "2JHC Fold 3, logMAE: -1.3548139667227836\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.497164\tvalid_1's l1: 0.512673\n",
      "[200]\ttraining's l1: 0.422757\tvalid_1's l1: 0.446229\n",
      "[300]\ttraining's l1: 0.381309\tvalid_1's l1: 0.411327\n",
      "[400]\ttraining's l1: 0.352909\tvalid_1's l1: 0.388874\n",
      "[500]\ttraining's l1: 0.33185\tvalid_1's l1: 0.372577\n",
      "[600]\ttraining's l1: 0.314886\tvalid_1's l1: 0.360088\n",
      "[700]\ttraining's l1: 0.300912\tvalid_1's l1: 0.349948\n",
      "[800]\ttraining's l1: 0.288738\tvalid_1's l1: 0.341645\n",
      "[900]\ttraining's l1: 0.277902\tvalid_1's l1: 0.334394\n",
      "[1000]\ttraining's l1: 0.268532\tvalid_1's l1: 0.328519\n",
      "[1100]\ttraining's l1: 0.260074\tvalid_1's l1: 0.323169\n",
      "[1200]\ttraining's l1: 0.252248\tvalid_1's l1: 0.318293\n",
      "[1300]\ttraining's l1: 0.245194\tvalid_1's l1: 0.314091\n",
      "[1400]\ttraining's l1: 0.238819\tvalid_1's l1: 0.310438\n",
      "[1500]\ttraining's l1: 0.232692\tvalid_1's l1: 0.306922\n",
      "[1600]\ttraining's l1: 0.227112\tvalid_1's l1: 0.303814\n",
      "[1700]\ttraining's l1: 0.221849\tvalid_1's l1: 0.300964\n",
      "[1800]\ttraining's l1: 0.216767\tvalid_1's l1: 0.298279\n",
      "[1900]\ttraining's l1: 0.212058\tvalid_1's l1: 0.295801\n",
      "[2000]\ttraining's l1: 0.207652\tvalid_1's l1: 0.293447\n",
      "[2100]\ttraining's l1: 0.203554\tvalid_1's l1: 0.29139\n",
      "[2200]\ttraining's l1: 0.199493\tvalid_1's l1: 0.289436\n",
      "[2300]\ttraining's l1: 0.195555\tvalid_1's l1: 0.287483\n",
      "[2400]\ttraining's l1: 0.191817\tvalid_1's l1: 0.285625\n",
      "[2500]\ttraining's l1: 0.188232\tvalid_1's l1: 0.283938\n",
      "[2600]\ttraining's l1: 0.184842\tvalid_1's l1: 0.282303\n",
      "[2700]\ttraining's l1: 0.18163\tvalid_1's l1: 0.280874\n",
      "[2800]\ttraining's l1: 0.178515\tvalid_1's l1: 0.279474\n",
      "[2900]\ttraining's l1: 0.175586\tvalid_1's l1: 0.278179\n",
      "[3000]\ttraining's l1: 0.172687\tvalid_1's l1: 0.276867\n",
      "[3100]\ttraining's l1: 0.169841\tvalid_1's l1: 0.27564\n",
      "[3200]\ttraining's l1: 0.167056\tvalid_1's l1: 0.274464\n",
      "[3300]\ttraining's l1: 0.164474\tvalid_1's l1: 0.273394\n",
      "[3400]\ttraining's l1: 0.161892\tvalid_1's l1: 0.272262\n",
      "[3500]\ttraining's l1: 0.159475\tvalid_1's l1: 0.271304\n",
      "[3600]\ttraining's l1: 0.15716\tvalid_1's l1: 0.270363\n",
      "[3700]\ttraining's l1: 0.15479\tvalid_1's l1: 0.269431\n",
      "[3800]\ttraining's l1: 0.152546\tvalid_1's l1: 0.268539\n",
      "[3900]\ttraining's l1: 0.150391\tvalid_1's l1: 0.267726\n",
      "[4000]\ttraining's l1: 0.148261\tvalid_1's l1: 0.266898\n",
      "[4100]\ttraining's l1: 0.146255\tvalid_1's l1: 0.266113\n",
      "[4200]\ttraining's l1: 0.144349\tvalid_1's l1: 0.265408\n",
      "[4300]\ttraining's l1: 0.142393\tvalid_1's l1: 0.264678\n",
      "[4400]\ttraining's l1: 0.140541\tvalid_1's l1: 0.263934\n",
      "[4500]\ttraining's l1: 0.138765\tvalid_1's l1: 0.263236\n",
      "[4600]\ttraining's l1: 0.136976\tvalid_1's l1: 0.262587\n",
      "[4700]\ttraining's l1: 0.135196\tvalid_1's l1: 0.261892\n",
      "[4800]\ttraining's l1: 0.133478\tvalid_1's l1: 0.261245\n",
      "[4900]\ttraining's l1: 0.131816\tvalid_1's l1: 0.260645\n",
      "[5000]\ttraining's l1: 0.130158\tvalid_1's l1: 0.26004\n",
      "[5100]\ttraining's l1: 0.128591\tvalid_1's l1: 0.259501\n",
      "[5200]\ttraining's l1: 0.127044\tvalid_1's l1: 0.258958\n",
      "[5300]\ttraining's l1: 0.125479\tvalid_1's l1: 0.258392\n",
      "[5400]\ttraining's l1: 0.124008\tvalid_1's l1: 0.257862\n",
      "[5500]\ttraining's l1: 0.122583\tvalid_1's l1: 0.257343\n",
      "[5600]\ttraining's l1: 0.121155\tvalid_1's l1: 0.256859\n",
      "[5700]\ttraining's l1: 0.119762\tvalid_1's l1: 0.256394\n",
      "[5800]\ttraining's l1: 0.118415\tvalid_1's l1: 0.255946\n",
      "[5900]\ttraining's l1: 0.117056\tvalid_1's l1: 0.255456\n",
      "[6000]\ttraining's l1: 0.115758\tvalid_1's l1: 0.254992\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.115758\tvalid_1's l1: 0.254992\n",
      "2JHC Fold 4, logMAE: -1.3665212186547788\n",
      "*** Training Model for 3JHH ***\n",
      "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8',\n",
      "       'd_1_0', 'd_2_0', 'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0', 'd_4_1',\n",
      "       'd_4_2', 'd_4_3', 'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0', 'd_6_1',\n",
      "       'd_6_2', 'd_6_3', 'd_7_0', 'd_7_1', 'd_7_2', 'd_7_3', 'd_8_0', 'd_8_1',\n",
      "       'd_8_2', 'd_8_3', 'scalar_coupling_constant'],\n",
      "      dtype='object')\n",
      "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8',\n",
      "       'd_1_0', 'd_2_0', 'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0', 'd_4_1',\n",
      "       'd_4_2', 'd_4_3', 'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0', 'd_6_1',\n",
      "       'd_6_2', 'd_6_3', 'd_7_0', 'd_7_1', 'd_7_2', 'd_7_3', 'd_8_0', 'd_8_1',\n",
      "       'd_8_2', 'd_8_3'],\n",
      "      dtype='object')\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.283901\tvalid_1's l1: 0.295042\n",
      "[200]\ttraining's l1: 0.232777\tvalid_1's l1: 0.252536\n",
      "[300]\ttraining's l1: 0.20653\tvalid_1's l1: 0.232247\n",
      "[400]\ttraining's l1: 0.188817\tvalid_1's l1: 0.219424\n",
      "[500]\ttraining's l1: 0.175847\tvalid_1's l1: 0.210843\n",
      "[600]\ttraining's l1: 0.164882\tvalid_1's l1: 0.203862\n",
      "[700]\ttraining's l1: 0.156112\tvalid_1's l1: 0.198556\n",
      "[800]\ttraining's l1: 0.148499\tvalid_1's l1: 0.194112\n",
      "[900]\ttraining's l1: 0.14157\tvalid_1's l1: 0.190347\n",
      "[1000]\ttraining's l1: 0.135546\tvalid_1's l1: 0.187072\n",
      "[1100]\ttraining's l1: 0.130176\tvalid_1's l1: 0.184371\n",
      "[1200]\ttraining's l1: 0.125403\tvalid_1's l1: 0.182002\n",
      "[1300]\ttraining's l1: 0.120896\tvalid_1's l1: 0.179756\n",
      "[1400]\ttraining's l1: 0.116859\tvalid_1's l1: 0.177894\n",
      "[1500]\ttraining's l1: 0.113044\tvalid_1's l1: 0.176242\n",
      "[1600]\ttraining's l1: 0.10936\tvalid_1's l1: 0.174432\n",
      "[1700]\ttraining's l1: 0.106026\tvalid_1's l1: 0.172985\n",
      "[1800]\ttraining's l1: 0.103042\tvalid_1's l1: 0.171762\n",
      "[1900]\ttraining's l1: 0.100079\tvalid_1's l1: 0.170529\n",
      "[2000]\ttraining's l1: 0.0972933\tvalid_1's l1: 0.169438\n",
      "[2100]\ttraining's l1: 0.0946695\tvalid_1's l1: 0.168412\n",
      "[2200]\ttraining's l1: 0.0921791\tvalid_1's l1: 0.167432\n",
      "[2300]\ttraining's l1: 0.0898493\tvalid_1's l1: 0.166618\n",
      "[2400]\ttraining's l1: 0.0875817\tvalid_1's l1: 0.165812\n",
      "[2500]\ttraining's l1: 0.0854431\tvalid_1's l1: 0.165006\n",
      "[2600]\ttraining's l1: 0.0834269\tvalid_1's l1: 0.164295\n",
      "[2700]\ttraining's l1: 0.0814704\tvalid_1's l1: 0.163642\n",
      "[2800]\ttraining's l1: 0.0795883\tvalid_1's l1: 0.162974\n",
      "[2900]\ttraining's l1: 0.0777613\tvalid_1's l1: 0.162291\n",
      "[3000]\ttraining's l1: 0.0760579\tvalid_1's l1: 0.161763\n",
      "[3100]\ttraining's l1: 0.0744128\tvalid_1's l1: 0.161192\n",
      "[3200]\ttraining's l1: 0.0728262\tvalid_1's l1: 0.160673\n",
      "[3300]\ttraining's l1: 0.0712864\tvalid_1's l1: 0.160201\n",
      "[3400]\ttraining's l1: 0.0698265\tvalid_1's l1: 0.159736\n",
      "[3500]\ttraining's l1: 0.0684218\tvalid_1's l1: 0.15928\n",
      "[3600]\ttraining's l1: 0.0670423\tvalid_1's l1: 0.15885\n",
      "[3700]\ttraining's l1: 0.0656992\tvalid_1's l1: 0.158394\n",
      "[3800]\ttraining's l1: 0.0644368\tvalid_1's l1: 0.158005\n",
      "[3900]\ttraining's l1: 0.063236\tvalid_1's l1: 0.15762\n",
      "[4000]\ttraining's l1: 0.062045\tvalid_1's l1: 0.157272\n",
      "[4100]\ttraining's l1: 0.0608955\tvalid_1's l1: 0.156952\n",
      "[4200]\ttraining's l1: 0.059787\tvalid_1's l1: 0.156612\n",
      "[4300]\ttraining's l1: 0.0586922\tvalid_1's l1: 0.156323\n",
      "[4400]\ttraining's l1: 0.0576253\tvalid_1's l1: 0.156031\n",
      "[4500]\ttraining's l1: 0.0565876\tvalid_1's l1: 0.155734\n",
      "[4600]\ttraining's l1: 0.0555931\tvalid_1's l1: 0.155451\n",
      "[4700]\ttraining's l1: 0.0546498\tvalid_1's l1: 0.155189\n",
      "[4800]\ttraining's l1: 0.0537006\tvalid_1's l1: 0.154929\n",
      "[4900]\ttraining's l1: 0.052798\tvalid_1's l1: 0.154704\n",
      "[5000]\ttraining's l1: 0.0519133\tvalid_1's l1: 0.154488\n",
      "[5100]\ttraining's l1: 0.0510634\tvalid_1's l1: 0.154263\n",
      "[5200]\ttraining's l1: 0.0502169\tvalid_1's l1: 0.154063\n",
      "[5300]\ttraining's l1: 0.0493976\tvalid_1's l1: 0.153849\n",
      "[5400]\ttraining's l1: 0.0486018\tvalid_1's l1: 0.153668\n",
      "[5500]\ttraining's l1: 0.0478203\tvalid_1's l1: 0.153487\n",
      "[5600]\ttraining's l1: 0.0470676\tvalid_1's l1: 0.153284\n",
      "[5700]\ttraining's l1: 0.0463316\tvalid_1's l1: 0.153084\n",
      "[5800]\ttraining's l1: 0.0456019\tvalid_1's l1: 0.152926\n",
      "[5900]\ttraining's l1: 0.0448987\tvalid_1's l1: 0.152765\n",
      "[6000]\ttraining's l1: 0.0442076\tvalid_1's l1: 0.152603\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0442076\tvalid_1's l1: 0.152603\n",
      "3JHH Fold 0, logMAE: -1.8799128526398603\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.282663\tvalid_1's l1: 0.29594\n",
      "[200]\ttraining's l1: 0.232634\tvalid_1's l1: 0.253586\n",
      "[300]\ttraining's l1: 0.20634\tvalid_1's l1: 0.233286\n",
      "[400]\ttraining's l1: 0.18874\tvalid_1's l1: 0.220598\n",
      "[500]\ttraining's l1: 0.175204\tvalid_1's l1: 0.211459\n",
      "[600]\ttraining's l1: 0.165008\tvalid_1's l1: 0.20502\n",
      "[700]\ttraining's l1: 0.155881\tvalid_1's l1: 0.199537\n",
      "[800]\ttraining's l1: 0.148267\tvalid_1's l1: 0.194957\n",
      "[900]\ttraining's l1: 0.141686\tvalid_1's l1: 0.191248\n",
      "[1000]\ttraining's l1: 0.136031\tvalid_1's l1: 0.188319\n",
      "[1100]\ttraining's l1: 0.130544\tvalid_1's l1: 0.185352\n",
      "[1200]\ttraining's l1: 0.125635\tvalid_1's l1: 0.182972\n",
      "[1300]\ttraining's l1: 0.121007\tvalid_1's l1: 0.180814\n",
      "[1400]\ttraining's l1: 0.11685\tvalid_1's l1: 0.178754\n",
      "[1500]\ttraining's l1: 0.112967\tvalid_1's l1: 0.176915\n",
      "[1600]\ttraining's l1: 0.109436\tvalid_1's l1: 0.175335\n",
      "[1700]\ttraining's l1: 0.10606\tvalid_1's l1: 0.173832\n",
      "[1800]\ttraining's l1: 0.10295\tvalid_1's l1: 0.172426\n",
      "[1900]\ttraining's l1: 0.100007\tvalid_1's l1: 0.171241\n",
      "[2000]\ttraining's l1: 0.0972396\tvalid_1's l1: 0.170095\n",
      "[2100]\ttraining's l1: 0.0946623\tvalid_1's l1: 0.169105\n",
      "[2200]\ttraining's l1: 0.0922343\tvalid_1's l1: 0.168122\n",
      "[2300]\ttraining's l1: 0.0898731\tvalid_1's l1: 0.167191\n",
      "[2400]\ttraining's l1: 0.0876762\tvalid_1's l1: 0.166363\n",
      "[2500]\ttraining's l1: 0.0855563\tvalid_1's l1: 0.165552\n",
      "[2600]\ttraining's l1: 0.0834776\tvalid_1's l1: 0.164788\n",
      "[2700]\ttraining's l1: 0.0814974\tvalid_1's l1: 0.164046\n",
      "[2800]\ttraining's l1: 0.079634\tvalid_1's l1: 0.163406\n",
      "[2900]\ttraining's l1: 0.0778522\tvalid_1's l1: 0.162792\n",
      "[3000]\ttraining's l1: 0.0761303\tvalid_1's l1: 0.162185\n",
      "[3100]\ttraining's l1: 0.0744955\tvalid_1's l1: 0.161614\n",
      "[3200]\ttraining's l1: 0.0729089\tvalid_1's l1: 0.161091\n",
      "[3300]\ttraining's l1: 0.0714288\tvalid_1's l1: 0.160606\n",
      "[3400]\ttraining's l1: 0.069928\tvalid_1's l1: 0.160169\n",
      "[3500]\ttraining's l1: 0.06853\tvalid_1's l1: 0.159724\n",
      "[3600]\ttraining's l1: 0.0671408\tvalid_1's l1: 0.159288\n",
      "[3700]\ttraining's l1: 0.0658019\tvalid_1's l1: 0.158865\n",
      "[3800]\ttraining's l1: 0.0645122\tvalid_1's l1: 0.158459\n",
      "[3900]\ttraining's l1: 0.0632705\tvalid_1's l1: 0.158042\n",
      "[4000]\ttraining's l1: 0.0620425\tvalid_1's l1: 0.157703\n",
      "[4100]\ttraining's l1: 0.0608655\tvalid_1's l1: 0.157325\n",
      "[4200]\ttraining's l1: 0.0597391\tvalid_1's l1: 0.157026\n",
      "[4300]\ttraining's l1: 0.0586351\tvalid_1's l1: 0.15671\n",
      "[4400]\ttraining's l1: 0.0575664\tvalid_1's l1: 0.156426\n",
      "[4500]\ttraining's l1: 0.0565293\tvalid_1's l1: 0.156143\n",
      "[4600]\ttraining's l1: 0.0555225\tvalid_1's l1: 0.155882\n",
      "[4700]\ttraining's l1: 0.0545848\tvalid_1's l1: 0.155631\n",
      "[4800]\ttraining's l1: 0.0536629\tvalid_1's l1: 0.155397\n",
      "[4900]\ttraining's l1: 0.0527724\tvalid_1's l1: 0.155154\n",
      "[5000]\ttraining's l1: 0.0518788\tvalid_1's l1: 0.154886\n",
      "[5100]\ttraining's l1: 0.0510183\tvalid_1's l1: 0.154656\n",
      "[5200]\ttraining's l1: 0.0501651\tvalid_1's l1: 0.154438\n",
      "[5300]\ttraining's l1: 0.0493441\tvalid_1's l1: 0.154216\n",
      "[5400]\ttraining's l1: 0.0485579\tvalid_1's l1: 0.154024\n",
      "[5500]\ttraining's l1: 0.0477571\tvalid_1's l1: 0.153803\n",
      "[5600]\ttraining's l1: 0.0470121\tvalid_1's l1: 0.153589\n",
      "[5700]\ttraining's l1: 0.0462895\tvalid_1's l1: 0.153423\n",
      "[5800]\ttraining's l1: 0.0455789\tvalid_1's l1: 0.153232\n",
      "[5900]\ttraining's l1: 0.0448998\tvalid_1's l1: 0.153066\n",
      "[6000]\ttraining's l1: 0.0441994\tvalid_1's l1: 0.152882\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0441994\tvalid_1's l1: 0.152882\n",
      "3JHH Fold 1, logMAE: -1.8780913980144618\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.282408\tvalid_1's l1: 0.296177\n",
      "[200]\ttraining's l1: 0.232554\tvalid_1's l1: 0.253541\n",
      "[300]\ttraining's l1: 0.206253\tvalid_1's l1: 0.232841\n",
      "[400]\ttraining's l1: 0.188948\tvalid_1's l1: 0.220318\n",
      "[500]\ttraining's l1: 0.175323\tvalid_1's l1: 0.211175\n",
      "[600]\ttraining's l1: 0.164578\tvalid_1's l1: 0.204304\n",
      "[700]\ttraining's l1: 0.155353\tvalid_1's l1: 0.198699\n",
      "[800]\ttraining's l1: 0.147569\tvalid_1's l1: 0.194112\n",
      "[900]\ttraining's l1: 0.140831\tvalid_1's l1: 0.190262\n",
      "[1000]\ttraining's l1: 0.134793\tvalid_1's l1: 0.187008\n",
      "[1100]\ttraining's l1: 0.12949\tvalid_1's l1: 0.184152\n",
      "[1200]\ttraining's l1: 0.124781\tvalid_1's l1: 0.181817\n",
      "[1300]\ttraining's l1: 0.120238\tvalid_1's l1: 0.179594\n",
      "[1400]\ttraining's l1: 0.116041\tvalid_1's l1: 0.177612\n",
      "[1500]\ttraining's l1: 0.112346\tvalid_1's l1: 0.175949\n",
      "[1600]\ttraining's l1: 0.108842\tvalid_1's l1: 0.174498\n",
      "[1700]\ttraining's l1: 0.105504\tvalid_1's l1: 0.173064\n",
      "[1800]\ttraining's l1: 0.102391\tvalid_1's l1: 0.171699\n",
      "[1900]\ttraining's l1: 0.0995151\tvalid_1's l1: 0.170485\n",
      "[2000]\ttraining's l1: 0.0967803\tvalid_1's l1: 0.169352\n",
      "[2100]\ttraining's l1: 0.0942145\tvalid_1's l1: 0.168455\n",
      "[2200]\ttraining's l1: 0.0917469\tvalid_1's l1: 0.167608\n",
      "[2300]\ttraining's l1: 0.0894033\tvalid_1's l1: 0.166713\n",
      "[2400]\ttraining's l1: 0.0871332\tvalid_1's l1: 0.165852\n",
      "[2500]\ttraining's l1: 0.0850561\tvalid_1's l1: 0.165077\n",
      "[2600]\ttraining's l1: 0.0830855\tvalid_1's l1: 0.164396\n",
      "[2700]\ttraining's l1: 0.0811353\tvalid_1's l1: 0.1637\n",
      "[2800]\ttraining's l1: 0.0792652\tvalid_1's l1: 0.163073\n",
      "[2900]\ttraining's l1: 0.077467\tvalid_1's l1: 0.162433\n",
      "[3000]\ttraining's l1: 0.0757948\tvalid_1's l1: 0.161927\n",
      "[3100]\ttraining's l1: 0.074131\tvalid_1's l1: 0.161353\n",
      "[3200]\ttraining's l1: 0.0725271\tvalid_1's l1: 0.160854\n",
      "[3300]\ttraining's l1: 0.0709715\tvalid_1's l1: 0.160356\n",
      "[3400]\ttraining's l1: 0.0695123\tvalid_1's l1: 0.159918\n",
      "[3500]\ttraining's l1: 0.0680934\tvalid_1's l1: 0.159472\n",
      "[3600]\ttraining's l1: 0.0667312\tvalid_1's l1: 0.159063\n",
      "[3700]\ttraining's l1: 0.0653938\tvalid_1's l1: 0.158669\n",
      "[3800]\ttraining's l1: 0.0640797\tvalid_1's l1: 0.15826\n",
      "[3900]\ttraining's l1: 0.0628513\tvalid_1's l1: 0.157937\n",
      "[4000]\ttraining's l1: 0.06165\tvalid_1's l1: 0.157564\n",
      "[4100]\ttraining's l1: 0.0604952\tvalid_1's l1: 0.157271\n",
      "[4200]\ttraining's l1: 0.0593753\tvalid_1's l1: 0.156955\n",
      "[4300]\ttraining's l1: 0.0583035\tvalid_1's l1: 0.156654\n",
      "[4400]\ttraining's l1: 0.0572383\tvalid_1's l1: 0.156379\n",
      "[4500]\ttraining's l1: 0.0562266\tvalid_1's l1: 0.156107\n",
      "[4600]\ttraining's l1: 0.0552459\tvalid_1's l1: 0.15582\n",
      "[4700]\ttraining's l1: 0.0542756\tvalid_1's l1: 0.155556\n",
      "[4800]\ttraining's l1: 0.0533279\tvalid_1's l1: 0.155296\n",
      "[4900]\ttraining's l1: 0.0524447\tvalid_1's l1: 0.155071\n",
      "[5000]\ttraining's l1: 0.0515612\tvalid_1's l1: 0.154842\n",
      "[5100]\ttraining's l1: 0.0507145\tvalid_1's l1: 0.154603\n",
      "[5200]\ttraining's l1: 0.0498742\tvalid_1's l1: 0.154367\n",
      "[5300]\ttraining's l1: 0.0490467\tvalid_1's l1: 0.154154\n",
      "[5400]\ttraining's l1: 0.0482793\tvalid_1's l1: 0.15394\n",
      "[5500]\ttraining's l1: 0.0475197\tvalid_1's l1: 0.153744\n",
      "[5600]\ttraining's l1: 0.0467694\tvalid_1's l1: 0.153532\n",
      "[5700]\ttraining's l1: 0.0460656\tvalid_1's l1: 0.153357\n",
      "[5800]\ttraining's l1: 0.0453339\tvalid_1's l1: 0.153173\n",
      "[5900]\ttraining's l1: 0.0446401\tvalid_1's l1: 0.152999\n",
      "[6000]\ttraining's l1: 0.0439865\tvalid_1's l1: 0.152836\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0439865\tvalid_1's l1: 0.152836\n",
      "3JHH Fold 2, logMAE: -1.8783913000859944\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.281291\tvalid_1's l1: 0.296024\n",
      "[200]\ttraining's l1: 0.231509\tvalid_1's l1: 0.253458\n",
      "[300]\ttraining's l1: 0.206583\tvalid_1's l1: 0.234093\n",
      "[400]\ttraining's l1: 0.188975\tvalid_1's l1: 0.221358\n",
      "[500]\ttraining's l1: 0.175363\tvalid_1's l1: 0.212188\n",
      "[600]\ttraining's l1: 0.16461\tvalid_1's l1: 0.205288\n",
      "[700]\ttraining's l1: 0.155697\tvalid_1's l1: 0.199698\n",
      "[800]\ttraining's l1: 0.147976\tvalid_1's l1: 0.19516\n",
      "[900]\ttraining's l1: 0.141154\tvalid_1's l1: 0.191137\n",
      "[1000]\ttraining's l1: 0.135213\tvalid_1's l1: 0.187929\n",
      "[1100]\ttraining's l1: 0.130055\tvalid_1's l1: 0.18524\n",
      "[1200]\ttraining's l1: 0.125097\tvalid_1's l1: 0.182774\n",
      "[1300]\ttraining's l1: 0.120644\tvalid_1's l1: 0.180499\n",
      "[1400]\ttraining's l1: 0.116558\tvalid_1's l1: 0.178648\n",
      "[1500]\ttraining's l1: 0.112672\tvalid_1's l1: 0.176828\n",
      "[1600]\ttraining's l1: 0.109142\tvalid_1's l1: 0.175252\n",
      "[1700]\ttraining's l1: 0.105885\tvalid_1's l1: 0.173907\n",
      "[1800]\ttraining's l1: 0.102738\tvalid_1's l1: 0.172575\n",
      "[1900]\ttraining's l1: 0.0997289\tvalid_1's l1: 0.171301\n",
      "[2000]\ttraining's l1: 0.0970046\tvalid_1's l1: 0.170198\n",
      "[2100]\ttraining's l1: 0.0943626\tvalid_1's l1: 0.169183\n",
      "[2200]\ttraining's l1: 0.0919121\tvalid_1's l1: 0.168268\n",
      "[2300]\ttraining's l1: 0.089608\tvalid_1's l1: 0.167384\n",
      "[2400]\ttraining's l1: 0.0874171\tvalid_1's l1: 0.166559\n",
      "[2500]\ttraining's l1: 0.0852884\tvalid_1's l1: 0.165759\n",
      "[2600]\ttraining's l1: 0.0832428\tvalid_1's l1: 0.164982\n",
      "[2700]\ttraining's l1: 0.0812878\tvalid_1's l1: 0.164333\n",
      "[2800]\ttraining's l1: 0.0794034\tvalid_1's l1: 0.163685\n",
      "[2900]\ttraining's l1: 0.0776162\tvalid_1's l1: 0.163087\n",
      "[3000]\ttraining's l1: 0.0758864\tvalid_1's l1: 0.162574\n",
      "[3100]\ttraining's l1: 0.0742484\tvalid_1's l1: 0.162004\n",
      "[3200]\ttraining's l1: 0.0726455\tvalid_1's l1: 0.161467\n",
      "[3300]\ttraining's l1: 0.0711503\tvalid_1's l1: 0.161006\n",
      "[3400]\ttraining's l1: 0.0696983\tvalid_1's l1: 0.160534\n",
      "[3500]\ttraining's l1: 0.0682572\tvalid_1's l1: 0.160097\n",
      "[3600]\ttraining's l1: 0.0668827\tvalid_1's l1: 0.159711\n",
      "[3700]\ttraining's l1: 0.0655731\tvalid_1's l1: 0.15933\n",
      "[3800]\ttraining's l1: 0.0642734\tvalid_1's l1: 0.158953\n",
      "[3900]\ttraining's l1: 0.0630011\tvalid_1's l1: 0.1586\n",
      "[4000]\ttraining's l1: 0.0618217\tvalid_1's l1: 0.158243\n",
      "[4100]\ttraining's l1: 0.0606402\tvalid_1's l1: 0.157941\n",
      "[4200]\ttraining's l1: 0.0595222\tvalid_1's l1: 0.157628\n",
      "[4300]\ttraining's l1: 0.0584272\tvalid_1's l1: 0.157344\n",
      "[4400]\ttraining's l1: 0.0573912\tvalid_1's l1: 0.157067\n",
      "[4500]\ttraining's l1: 0.056359\tvalid_1's l1: 0.156767\n",
      "[4600]\ttraining's l1: 0.0553557\tvalid_1's l1: 0.156466\n",
      "[4700]\ttraining's l1: 0.0543896\tvalid_1's l1: 0.156197\n",
      "[4800]\ttraining's l1: 0.0534544\tvalid_1's l1: 0.155972\n",
      "[4900]\ttraining's l1: 0.0525593\tvalid_1's l1: 0.155748\n",
      "[5000]\ttraining's l1: 0.0516413\tvalid_1's l1: 0.1555\n",
      "[5100]\ttraining's l1: 0.0507823\tvalid_1's l1: 0.155269\n",
      "[5200]\ttraining's l1: 0.0499313\tvalid_1's l1: 0.155058\n",
      "[5300]\ttraining's l1: 0.049139\tvalid_1's l1: 0.154839\n",
      "[5400]\ttraining's l1: 0.0483354\tvalid_1's l1: 0.154638\n",
      "[5500]\ttraining's l1: 0.047547\tvalid_1's l1: 0.154444\n",
      "[5600]\ttraining's l1: 0.0467976\tvalid_1's l1: 0.154249\n",
      "[5700]\ttraining's l1: 0.0460497\tvalid_1's l1: 0.154043\n",
      "[5800]\ttraining's l1: 0.0453403\tvalid_1's l1: 0.153857\n",
      "[5900]\ttraining's l1: 0.044669\tvalid_1's l1: 0.153704\n",
      "[6000]\ttraining's l1: 0.0440131\tvalid_1's l1: 0.153548\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0440131\tvalid_1's l1: 0.153548\n",
      "3JHH Fold 3, logMAE: -1.8737422646007047\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.282893\tvalid_1's l1: 0.296519\n",
      "[200]\ttraining's l1: 0.231249\tvalid_1's l1: 0.251788\n",
      "[300]\ttraining's l1: 0.206071\tvalid_1's l1: 0.23239\n",
      "[400]\ttraining's l1: 0.188184\tvalid_1's l1: 0.219487\n",
      "[500]\ttraining's l1: 0.174648\tvalid_1's l1: 0.210402\n",
      "[600]\ttraining's l1: 0.164017\tvalid_1's l1: 0.203675\n",
      "[700]\ttraining's l1: 0.155008\tvalid_1's l1: 0.198249\n",
      "[800]\ttraining's l1: 0.14743\tvalid_1's l1: 0.193844\n",
      "[900]\ttraining's l1: 0.140913\tvalid_1's l1: 0.190153\n",
      "[1000]\ttraining's l1: 0.135005\tvalid_1's l1: 0.186989\n",
      "[1100]\ttraining's l1: 0.129634\tvalid_1's l1: 0.18442\n",
      "[1200]\ttraining's l1: 0.124674\tvalid_1's l1: 0.182061\n",
      "[1300]\ttraining's l1: 0.120116\tvalid_1's l1: 0.179713\n",
      "[1400]\ttraining's l1: 0.115955\tvalid_1's l1: 0.177818\n",
      "[1500]\ttraining's l1: 0.112236\tvalid_1's l1: 0.176207\n",
      "[1600]\ttraining's l1: 0.10878\tvalid_1's l1: 0.174682\n",
      "[1700]\ttraining's l1: 0.105603\tvalid_1's l1: 0.173335\n",
      "[1800]\ttraining's l1: 0.102567\tvalid_1's l1: 0.172072\n",
      "[1900]\ttraining's l1: 0.0997258\tvalid_1's l1: 0.170931\n",
      "[2000]\ttraining's l1: 0.0969453\tvalid_1's l1: 0.16984\n",
      "[2100]\ttraining's l1: 0.0943791\tvalid_1's l1: 0.16878\n",
      "[2200]\ttraining's l1: 0.0919216\tvalid_1's l1: 0.167799\n",
      "[2300]\ttraining's l1: 0.0895742\tvalid_1's l1: 0.166876\n",
      "[2400]\ttraining's l1: 0.0874073\tvalid_1's l1: 0.166076\n",
      "[2500]\ttraining's l1: 0.0852849\tvalid_1's l1: 0.165306\n",
      "[2600]\ttraining's l1: 0.0831788\tvalid_1's l1: 0.16454\n",
      "[2700]\ttraining's l1: 0.0812542\tvalid_1's l1: 0.163787\n",
      "[2800]\ttraining's l1: 0.079379\tvalid_1's l1: 0.163136\n",
      "[2900]\ttraining's l1: 0.0775974\tvalid_1's l1: 0.162555\n",
      "[3000]\ttraining's l1: 0.0758947\tvalid_1's l1: 0.161931\n",
      "[3100]\ttraining's l1: 0.074263\tvalid_1's l1: 0.161346\n",
      "[3200]\ttraining's l1: 0.0726751\tvalid_1's l1: 0.160846\n",
      "[3300]\ttraining's l1: 0.07112\tvalid_1's l1: 0.160354\n",
      "[3400]\ttraining's l1: 0.0696508\tvalid_1's l1: 0.159902\n",
      "[3500]\ttraining's l1: 0.0682505\tvalid_1's l1: 0.159463\n",
      "[3600]\ttraining's l1: 0.0668998\tvalid_1's l1: 0.159068\n",
      "[3700]\ttraining's l1: 0.0656099\tvalid_1's l1: 0.158684\n",
      "[3800]\ttraining's l1: 0.0643344\tvalid_1's l1: 0.158329\n",
      "[3900]\ttraining's l1: 0.0630735\tvalid_1's l1: 0.157914\n",
      "[4000]\ttraining's l1: 0.061879\tvalid_1's l1: 0.157599\n",
      "[4100]\ttraining's l1: 0.0607287\tvalid_1's l1: 0.157319\n",
      "[4200]\ttraining's l1: 0.059606\tvalid_1's l1: 0.157003\n",
      "[4300]\ttraining's l1: 0.0585276\tvalid_1's l1: 0.156706\n",
      "[4400]\ttraining's l1: 0.0574949\tvalid_1's l1: 0.156421\n",
      "[4500]\ttraining's l1: 0.0564721\tvalid_1's l1: 0.156164\n",
      "[4600]\ttraining's l1: 0.0554673\tvalid_1's l1: 0.155863\n",
      "[4700]\ttraining's l1: 0.0545204\tvalid_1's l1: 0.1556\n",
      "[4800]\ttraining's l1: 0.0535813\tvalid_1's l1: 0.155318\n",
      "[4900]\ttraining's l1: 0.0526618\tvalid_1's l1: 0.155055\n",
      "[5000]\ttraining's l1: 0.0517585\tvalid_1's l1: 0.15482\n",
      "[5100]\ttraining's l1: 0.0509253\tvalid_1's l1: 0.154612\n",
      "[5200]\ttraining's l1: 0.0500826\tvalid_1's l1: 0.154387\n",
      "[5300]\ttraining's l1: 0.0492714\tvalid_1's l1: 0.154171\n",
      "[5400]\ttraining's l1: 0.048487\tvalid_1's l1: 0.153957\n",
      "[5500]\ttraining's l1: 0.0476979\tvalid_1's l1: 0.153744\n",
      "[5600]\ttraining's l1: 0.0469576\tvalid_1's l1: 0.153556\n",
      "[5700]\ttraining's l1: 0.0462223\tvalid_1's l1: 0.153376\n",
      "[5800]\ttraining's l1: 0.0455145\tvalid_1's l1: 0.153211\n",
      "[5900]\ttraining's l1: 0.0448169\tvalid_1's l1: 0.153024\n",
      "[6000]\ttraining's l1: 0.0441405\tvalid_1's l1: 0.152853\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0441405\tvalid_1's l1: 0.152853\n",
      "3JHH Fold 4, logMAE: -1.8782792309075644\n",
      "*** Training Model for 3JHC ***\n",
      "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8',\n",
      "       'atom_9', 'd_1_0', 'd_2_0', 'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0',\n",
      "       'd_4_1', 'd_4_2', 'd_4_3', 'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0',\n",
      "       'd_6_1', 'd_6_2', 'd_6_3', 'd_7_0', 'd_7_1', 'd_7_2', 'd_7_3', 'd_8_0',\n",
      "       'd_8_1', 'd_8_2', 'd_8_3', 'd_9_0', 'd_9_1', 'd_9_2', 'd_9_3',\n",
      "       'scalar_coupling_constant'],\n",
      "      dtype='object')\n",
      "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8',\n",
      "       'atom_9', 'd_1_0', 'd_2_0', 'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0',\n",
      "       'd_4_1', 'd_4_2', 'd_4_3', 'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0',\n",
      "       'd_6_1', 'd_6_2', 'd_6_3', 'd_7_0', 'd_7_1', 'd_7_2', 'd_7_3', 'd_8_0',\n",
      "       'd_8_1', 'd_8_2', 'd_8_3', 'd_9_0', 'd_9_1', 'd_9_2', 'd_9_3'],\n",
      "      dtype='object')\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.553537\tvalid_1's l1: 0.562906\n",
      "[200]\ttraining's l1: 0.474571\tvalid_1's l1: 0.491884\n",
      "[300]\ttraining's l1: 0.429906\tvalid_1's l1: 0.453374\n",
      "[400]\ttraining's l1: 0.39991\tvalid_1's l1: 0.428397\n",
      "[500]\ttraining's l1: 0.37761\tvalid_1's l1: 0.410929\n",
      "[600]\ttraining's l1: 0.359893\tvalid_1's l1: 0.397206\n",
      "[700]\ttraining's l1: 0.344936\tvalid_1's l1: 0.386136\n",
      "[800]\ttraining's l1: 0.332216\tvalid_1's l1: 0.377255\n",
      "[900]\ttraining's l1: 0.321291\tvalid_1's l1: 0.369834\n",
      "[1000]\ttraining's l1: 0.311351\tvalid_1's l1: 0.363099\n",
      "[1100]\ttraining's l1: 0.302624\tvalid_1's l1: 0.357461\n",
      "[1200]\ttraining's l1: 0.294599\tvalid_1's l1: 0.352306\n",
      "[1300]\ttraining's l1: 0.287094\tvalid_1's l1: 0.347598\n",
      "[1400]\ttraining's l1: 0.28004\tvalid_1's l1: 0.343363\n",
      "[1500]\ttraining's l1: 0.273608\tvalid_1's l1: 0.339557\n",
      "[1600]\ttraining's l1: 0.267678\tvalid_1's l1: 0.336087\n",
      "[1700]\ttraining's l1: 0.262113\tvalid_1's l1: 0.33288\n",
      "[1800]\ttraining's l1: 0.256762\tvalid_1's l1: 0.329927\n",
      "[1900]\ttraining's l1: 0.251741\tvalid_1's l1: 0.327084\n",
      "[2000]\ttraining's l1: 0.246989\tvalid_1's l1: 0.324559\n",
      "[2100]\ttraining's l1: 0.242567\tvalid_1's l1: 0.322325\n",
      "[2200]\ttraining's l1: 0.238226\tvalid_1's l1: 0.320097\n",
      "[2300]\ttraining's l1: 0.234045\tvalid_1's l1: 0.317961\n",
      "[2400]\ttraining's l1: 0.230155\tvalid_1's l1: 0.31604\n",
      "[2500]\ttraining's l1: 0.226385\tvalid_1's l1: 0.314289\n",
      "[2600]\ttraining's l1: 0.222758\tvalid_1's l1: 0.312601\n",
      "[2700]\ttraining's l1: 0.219194\tvalid_1's l1: 0.310947\n",
      "[2800]\ttraining's l1: 0.215815\tvalid_1's l1: 0.309287\n",
      "[2900]\ttraining's l1: 0.212526\tvalid_1's l1: 0.307777\n",
      "[3000]\ttraining's l1: 0.209441\tvalid_1's l1: 0.306364\n",
      "[3100]\ttraining's l1: 0.206432\tvalid_1's l1: 0.305037\n",
      "[3200]\ttraining's l1: 0.20349\tvalid_1's l1: 0.303735\n",
      "[3300]\ttraining's l1: 0.200651\tvalid_1's l1: 0.302502\n",
      "[3400]\ttraining's l1: 0.197913\tvalid_1's l1: 0.301377\n",
      "[3500]\ttraining's l1: 0.195206\tvalid_1's l1: 0.3002\n",
      "[3600]\ttraining's l1: 0.192645\tvalid_1's l1: 0.299076\n",
      "[3700]\ttraining's l1: 0.190127\tvalid_1's l1: 0.298029\n",
      "[3800]\ttraining's l1: 0.187689\tvalid_1's l1: 0.297051\n",
      "[3900]\ttraining's l1: 0.185329\tvalid_1's l1: 0.296095\n",
      "[4000]\ttraining's l1: 0.182995\tvalid_1's l1: 0.295138\n",
      "[4100]\ttraining's l1: 0.180708\tvalid_1's l1: 0.294141\n",
      "[4200]\ttraining's l1: 0.17851\tvalid_1's l1: 0.293229\n",
      "[4300]\ttraining's l1: 0.176341\tvalid_1's l1: 0.292339\n",
      "[4400]\ttraining's l1: 0.174281\tvalid_1's l1: 0.291537\n",
      "[4500]\ttraining's l1: 0.172229\tvalid_1's l1: 0.290735\n",
      "[4600]\ttraining's l1: 0.170269\tvalid_1's l1: 0.289965\n",
      "[4700]\ttraining's l1: 0.16831\tvalid_1's l1: 0.289207\n",
      "[4800]\ttraining's l1: 0.166381\tvalid_1's l1: 0.288457\n",
      "[4900]\ttraining's l1: 0.164481\tvalid_1's l1: 0.287697\n",
      "[5000]\ttraining's l1: 0.162643\tvalid_1's l1: 0.286997\n",
      "[5100]\ttraining's l1: 0.160858\tvalid_1's l1: 0.286343\n",
      "[5200]\ttraining's l1: 0.15907\tvalid_1's l1: 0.285706\n",
      "[5300]\ttraining's l1: 0.157329\tvalid_1's l1: 0.285037\n",
      "[5400]\ttraining's l1: 0.155599\tvalid_1's l1: 0.284419\n",
      "[5500]\ttraining's l1: 0.153967\tvalid_1's l1: 0.283857\n",
      "[5600]\ttraining's l1: 0.152335\tvalid_1's l1: 0.283251\n",
      "[5700]\ttraining's l1: 0.150761\tvalid_1's l1: 0.282697\n",
      "[5800]\ttraining's l1: 0.149217\tvalid_1's l1: 0.282182\n",
      "[5900]\ttraining's l1: 0.147686\tvalid_1's l1: 0.281643\n",
      "[6000]\ttraining's l1: 0.146181\tvalid_1's l1: 0.281093\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.146181\tvalid_1's l1: 0.281093\n",
      "3JHC Fold 0, logMAE: -1.2690711171875588\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.552565\tvalid_1's l1: 0.564276\n",
      "[200]\ttraining's l1: 0.472611\tvalid_1's l1: 0.491181\n",
      "[300]\ttraining's l1: 0.42846\tvalid_1's l1: 0.452532\n",
      "[400]\ttraining's l1: 0.399262\tvalid_1's l1: 0.428137\n",
      "[500]\ttraining's l1: 0.377794\tvalid_1's l1: 0.411086\n",
      "[600]\ttraining's l1: 0.359981\tvalid_1's l1: 0.397512\n",
      "[700]\ttraining's l1: 0.345161\tvalid_1's l1: 0.386697\n",
      "[800]\ttraining's l1: 0.332514\tvalid_1's l1: 0.377646\n",
      "[900]\ttraining's l1: 0.321592\tvalid_1's l1: 0.370104\n",
      "[1000]\ttraining's l1: 0.311618\tvalid_1's l1: 0.363314\n",
      "[1100]\ttraining's l1: 0.302667\tvalid_1's l1: 0.357366\n",
      "[1200]\ttraining's l1: 0.2946\tvalid_1's l1: 0.352209\n",
      "[1300]\ttraining's l1: 0.287167\tvalid_1's l1: 0.347612\n",
      "[1400]\ttraining's l1: 0.280225\tvalid_1's l1: 0.343243\n",
      "[1500]\ttraining's l1: 0.273813\tvalid_1's l1: 0.339342\n",
      "[1600]\ttraining's l1: 0.267893\tvalid_1's l1: 0.335912\n",
      "[1700]\ttraining's l1: 0.262199\tvalid_1's l1: 0.332526\n",
      "[1800]\ttraining's l1: 0.256869\tvalid_1's l1: 0.329422\n",
      "[1900]\ttraining's l1: 0.251879\tvalid_1's l1: 0.326671\n",
      "[2000]\ttraining's l1: 0.247085\tvalid_1's l1: 0.324215\n",
      "[2100]\ttraining's l1: 0.242482\tvalid_1's l1: 0.321674\n",
      "[2200]\ttraining's l1: 0.238183\tvalid_1's l1: 0.319433\n",
      "[2300]\ttraining's l1: 0.233895\tvalid_1's l1: 0.31721\n",
      "[2400]\ttraining's l1: 0.229942\tvalid_1's l1: 0.31525\n",
      "[2500]\ttraining's l1: 0.226151\tvalid_1's l1: 0.313376\n",
      "[2600]\ttraining's l1: 0.222529\tvalid_1's l1: 0.31161\n",
      "[2700]\ttraining's l1: 0.219009\tvalid_1's l1: 0.309931\n",
      "[2800]\ttraining's l1: 0.215606\tvalid_1's l1: 0.308256\n",
      "[2900]\ttraining's l1: 0.212253\tvalid_1's l1: 0.306751\n",
      "[3000]\ttraining's l1: 0.209087\tvalid_1's l1: 0.30531\n",
      "[3100]\ttraining's l1: 0.206004\tvalid_1's l1: 0.303951\n",
      "[3200]\ttraining's l1: 0.203062\tvalid_1's l1: 0.30256\n",
      "[3300]\ttraining's l1: 0.200168\tvalid_1's l1: 0.301297\n",
      "[3400]\ttraining's l1: 0.197412\tvalid_1's l1: 0.300092\n",
      "[3500]\ttraining's l1: 0.194738\tvalid_1's l1: 0.298937\n",
      "[3600]\ttraining's l1: 0.192049\tvalid_1's l1: 0.297751\n",
      "[3700]\ttraining's l1: 0.189458\tvalid_1's l1: 0.296683\n",
      "[3800]\ttraining's l1: 0.186999\tvalid_1's l1: 0.295626\n",
      "[3900]\ttraining's l1: 0.184627\tvalid_1's l1: 0.294673\n",
      "[4000]\ttraining's l1: 0.182291\tvalid_1's l1: 0.293717\n",
      "[4100]\ttraining's l1: 0.180035\tvalid_1's l1: 0.292771\n",
      "[4200]\ttraining's l1: 0.177858\tvalid_1's l1: 0.29193\n",
      "[4300]\ttraining's l1: 0.175686\tvalid_1's l1: 0.291074\n",
      "[4400]\ttraining's l1: 0.173572\tvalid_1's l1: 0.290204\n",
      "[4500]\ttraining's l1: 0.171535\tvalid_1's l1: 0.289398\n",
      "[4600]\ttraining's l1: 0.169526\tvalid_1's l1: 0.288616\n",
      "[4700]\ttraining's l1: 0.167606\tvalid_1's l1: 0.287889\n",
      "[4800]\ttraining's l1: 0.165698\tvalid_1's l1: 0.287124\n",
      "[4900]\ttraining's l1: 0.16382\tvalid_1's l1: 0.28641\n",
      "[5000]\ttraining's l1: 0.162008\tvalid_1's l1: 0.285737\n",
      "[5100]\ttraining's l1: 0.160263\tvalid_1's l1: 0.285066\n",
      "[5200]\ttraining's l1: 0.158489\tvalid_1's l1: 0.284474\n",
      "[5300]\ttraining's l1: 0.156796\tvalid_1's l1: 0.28385\n",
      "[5400]\ttraining's l1: 0.155105\tvalid_1's l1: 0.28325\n",
      "[5500]\ttraining's l1: 0.153484\tvalid_1's l1: 0.282656\n",
      "[5600]\ttraining's l1: 0.151882\tvalid_1's l1: 0.282091\n",
      "[5700]\ttraining's l1: 0.150297\tvalid_1's l1: 0.281509\n",
      "[5800]\ttraining's l1: 0.148753\tvalid_1's l1: 0.28095\n",
      "[5900]\ttraining's l1: 0.147228\tvalid_1's l1: 0.28044\n",
      "[6000]\ttraining's l1: 0.145734\tvalid_1's l1: 0.279906\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.145734\tvalid_1's l1: 0.279906\n",
      "3JHC Fold 1, logMAE: -1.2733001583568073\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.553631\tvalid_1's l1: 0.563626\n",
      "[200]\ttraining's l1: 0.472044\tvalid_1's l1: 0.489228\n",
      "[300]\ttraining's l1: 0.429238\tvalid_1's l1: 0.452267\n",
      "[400]\ttraining's l1: 0.400225\tvalid_1's l1: 0.428314\n",
      "[500]\ttraining's l1: 0.378674\tvalid_1's l1: 0.411417\n",
      "[600]\ttraining's l1: 0.360899\tvalid_1's l1: 0.397906\n",
      "[700]\ttraining's l1: 0.346126\tvalid_1's l1: 0.386915\n",
      "[800]\ttraining's l1: 0.333221\tvalid_1's l1: 0.377483\n",
      "[900]\ttraining's l1: 0.321894\tvalid_1's l1: 0.369625\n",
      "[1000]\ttraining's l1: 0.312074\tvalid_1's l1: 0.362788\n",
      "[1100]\ttraining's l1: 0.302924\tvalid_1's l1: 0.356643\n",
      "[1200]\ttraining's l1: 0.29486\tvalid_1's l1: 0.351539\n",
      "[1300]\ttraining's l1: 0.287477\tvalid_1's l1: 0.347069\n",
      "[1400]\ttraining's l1: 0.280458\tvalid_1's l1: 0.342746\n",
      "[1500]\ttraining's l1: 0.274001\tvalid_1's l1: 0.338794\n",
      "[1600]\ttraining's l1: 0.268033\tvalid_1's l1: 0.335341\n",
      "[1700]\ttraining's l1: 0.26241\tvalid_1's l1: 0.332191\n",
      "[1800]\ttraining's l1: 0.257058\tvalid_1's l1: 0.329207\n",
      "[1900]\ttraining's l1: 0.252049\tvalid_1's l1: 0.3264\n",
      "[2000]\ttraining's l1: 0.24726\tvalid_1's l1: 0.323808\n",
      "[2100]\ttraining's l1: 0.242657\tvalid_1's l1: 0.321334\n",
      "[2200]\ttraining's l1: 0.238258\tvalid_1's l1: 0.319007\n",
      "[2300]\ttraining's l1: 0.234121\tvalid_1's l1: 0.316831\n",
      "[2400]\ttraining's l1: 0.23011\tvalid_1's l1: 0.314774\n",
      "[2500]\ttraining's l1: 0.2263\tvalid_1's l1: 0.312934\n",
      "[2600]\ttraining's l1: 0.222664\tvalid_1's l1: 0.311113\n",
      "[2700]\ttraining's l1: 0.219192\tvalid_1's l1: 0.30938\n",
      "[2800]\ttraining's l1: 0.215885\tvalid_1's l1: 0.307783\n",
      "[2900]\ttraining's l1: 0.212578\tvalid_1's l1: 0.306314\n",
      "[3000]\ttraining's l1: 0.209456\tvalid_1's l1: 0.304888\n",
      "[3100]\ttraining's l1: 0.20638\tvalid_1's l1: 0.303535\n",
      "[3200]\ttraining's l1: 0.203497\tvalid_1's l1: 0.302225\n",
      "[3300]\ttraining's l1: 0.200639\tvalid_1's l1: 0.300935\n",
      "[3400]\ttraining's l1: 0.197861\tvalid_1's l1: 0.299657\n",
      "[3500]\ttraining's l1: 0.195184\tvalid_1's l1: 0.298527\n",
      "[3600]\ttraining's l1: 0.192543\tvalid_1's l1: 0.297401\n",
      "[3700]\ttraining's l1: 0.189997\tvalid_1's l1: 0.296309\n",
      "[3800]\ttraining's l1: 0.187517\tvalid_1's l1: 0.295252\n",
      "[3900]\ttraining's l1: 0.185152\tvalid_1's l1: 0.29431\n",
      "[4000]\ttraining's l1: 0.182802\tvalid_1's l1: 0.293355\n",
      "[4100]\ttraining's l1: 0.180542\tvalid_1's l1: 0.292438\n",
      "[4200]\ttraining's l1: 0.17831\tvalid_1's l1: 0.291585\n",
      "[4300]\ttraining's l1: 0.176167\tvalid_1's l1: 0.29074\n",
      "[4400]\ttraining's l1: 0.174061\tvalid_1's l1: 0.289955\n",
      "[4500]\ttraining's l1: 0.171999\tvalid_1's l1: 0.289172\n",
      "[4600]\ttraining's l1: 0.169998\tvalid_1's l1: 0.288374\n",
      "[4700]\ttraining's l1: 0.168014\tvalid_1's l1: 0.287584\n",
      "[4800]\ttraining's l1: 0.166096\tvalid_1's l1: 0.28686\n",
      "[4900]\ttraining's l1: 0.164226\tvalid_1's l1: 0.286173\n",
      "[5000]\ttraining's l1: 0.162432\tvalid_1's l1: 0.285482\n",
      "[5100]\ttraining's l1: 0.160664\tvalid_1's l1: 0.284858\n",
      "[5200]\ttraining's l1: 0.158884\tvalid_1's l1: 0.284229\n",
      "[5300]\ttraining's l1: 0.157147\tvalid_1's l1: 0.283659\n",
      "[5400]\ttraining's l1: 0.155433\tvalid_1's l1: 0.283052\n",
      "[5500]\ttraining's l1: 0.153809\tvalid_1's l1: 0.282428\n",
      "[5600]\ttraining's l1: 0.152208\tvalid_1's l1: 0.281873\n",
      "[5700]\ttraining's l1: 0.150643\tvalid_1's l1: 0.2813\n",
      "[5800]\ttraining's l1: 0.149091\tvalid_1's l1: 0.280742\n",
      "[5900]\ttraining's l1: 0.147598\tvalid_1's l1: 0.280247\n",
      "[6000]\ttraining's l1: 0.146077\tvalid_1's l1: 0.279732\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.146077\tvalid_1's l1: 0.279732\n",
      "3JHC Fold 2, logMAE: -1.2739242254391487\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.55593\tvalid_1's l1: 0.566107\n",
      "[200]\ttraining's l1: 0.472498\tvalid_1's l1: 0.489648\n",
      "[300]\ttraining's l1: 0.429697\tvalid_1's l1: 0.452706\n",
      "[400]\ttraining's l1: 0.400555\tvalid_1's l1: 0.428761\n",
      "[500]\ttraining's l1: 0.378976\tvalid_1's l1: 0.411918\n",
      "[600]\ttraining's l1: 0.36118\tvalid_1's l1: 0.398334\n",
      "[700]\ttraining's l1: 0.346297\tvalid_1's l1: 0.38757\n",
      "[800]\ttraining's l1: 0.333344\tvalid_1's l1: 0.378402\n",
      "[900]\ttraining's l1: 0.322241\tvalid_1's l1: 0.370781\n",
      "[1000]\ttraining's l1: 0.312444\tvalid_1's l1: 0.364304\n",
      "[1100]\ttraining's l1: 0.303461\tvalid_1's l1: 0.358322\n",
      "[1200]\ttraining's l1: 0.295343\tvalid_1's l1: 0.353158\n",
      "[1300]\ttraining's l1: 0.288057\tvalid_1's l1: 0.34867\n",
      "[1400]\ttraining's l1: 0.281103\tvalid_1's l1: 0.344411\n",
      "[1500]\ttraining's l1: 0.274565\tvalid_1's l1: 0.340475\n",
      "[1600]\ttraining's l1: 0.268527\tvalid_1's l1: 0.336981\n",
      "[1700]\ttraining's l1: 0.26275\tvalid_1's l1: 0.333589\n",
      "[1800]\ttraining's l1: 0.257324\tvalid_1's l1: 0.330539\n",
      "[1900]\ttraining's l1: 0.25224\tvalid_1's l1: 0.327655\n",
      "[2000]\ttraining's l1: 0.24751\tvalid_1's l1: 0.325276\n",
      "[2100]\ttraining's l1: 0.242932\tvalid_1's l1: 0.322833\n",
      "[2200]\ttraining's l1: 0.238543\tvalid_1's l1: 0.320492\n",
      "[2300]\ttraining's l1: 0.234278\tvalid_1's l1: 0.318204\n",
      "[2400]\ttraining's l1: 0.230347\tvalid_1's l1: 0.316235\n",
      "[2500]\ttraining's l1: 0.226594\tvalid_1's l1: 0.314349\n",
      "[2600]\ttraining's l1: 0.222962\tvalid_1's l1: 0.312516\n",
      "[2700]\ttraining's l1: 0.219458\tvalid_1's l1: 0.310786\n",
      "[2800]\ttraining's l1: 0.216077\tvalid_1's l1: 0.309185\n",
      "[2900]\ttraining's l1: 0.212794\tvalid_1's l1: 0.307645\n",
      "[3000]\ttraining's l1: 0.20965\tvalid_1's l1: 0.306178\n",
      "[3100]\ttraining's l1: 0.206608\tvalid_1's l1: 0.304812\n",
      "[3200]\ttraining's l1: 0.203667\tvalid_1's l1: 0.303474\n",
      "[3300]\ttraining's l1: 0.200749\tvalid_1's l1: 0.302213\n",
      "[3400]\ttraining's l1: 0.197981\tvalid_1's l1: 0.300997\n",
      "[3500]\ttraining's l1: 0.195264\tvalid_1's l1: 0.299825\n",
      "[3600]\ttraining's l1: 0.19264\tvalid_1's l1: 0.298676\n",
      "[3700]\ttraining's l1: 0.190115\tvalid_1's l1: 0.297567\n",
      "[3800]\ttraining's l1: 0.187627\tvalid_1's l1: 0.296501\n",
      "[3900]\ttraining's l1: 0.185259\tvalid_1's l1: 0.295593\n",
      "[4000]\ttraining's l1: 0.182958\tvalid_1's l1: 0.294604\n",
      "[4100]\ttraining's l1: 0.180747\tvalid_1's l1: 0.293737\n",
      "[4200]\ttraining's l1: 0.178481\tvalid_1's l1: 0.292799\n",
      "[4300]\ttraining's l1: 0.17632\tvalid_1's l1: 0.292001\n",
      "[4400]\ttraining's l1: 0.174179\tvalid_1's l1: 0.29121\n",
      "[4500]\ttraining's l1: 0.172156\tvalid_1's l1: 0.290429\n",
      "[4600]\ttraining's l1: 0.170166\tvalid_1's l1: 0.289677\n",
      "[4700]\ttraining's l1: 0.168214\tvalid_1's l1: 0.288883\n",
      "[4800]\ttraining's l1: 0.166274\tvalid_1's l1: 0.288193\n",
      "[4900]\ttraining's l1: 0.164413\tvalid_1's l1: 0.287437\n",
      "[5000]\ttraining's l1: 0.162594\tvalid_1's l1: 0.286762\n",
      "[5100]\ttraining's l1: 0.160783\tvalid_1's l1: 0.286179\n",
      "[5200]\ttraining's l1: 0.159014\tvalid_1's l1: 0.285504\n",
      "[5300]\ttraining's l1: 0.157293\tvalid_1's l1: 0.284896\n",
      "[5400]\ttraining's l1: 0.155588\tvalid_1's l1: 0.284327\n",
      "[5500]\ttraining's l1: 0.15391\tvalid_1's l1: 0.283778\n",
      "[5600]\ttraining's l1: 0.152265\tvalid_1's l1: 0.283145\n",
      "[5700]\ttraining's l1: 0.150675\tvalid_1's l1: 0.282562\n",
      "[5800]\ttraining's l1: 0.14914\tvalid_1's l1: 0.282002\n",
      "[5900]\ttraining's l1: 0.147595\tvalid_1's l1: 0.281458\n",
      "[6000]\ttraining's l1: 0.146094\tvalid_1's l1: 0.280992\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.146094\tvalid_1's l1: 0.280992\n",
      "3JHC Fold 3, logMAE: -1.2694282183857748\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.553599\tvalid_1's l1: 0.566304\n",
      "[200]\ttraining's l1: 0.470997\tvalid_1's l1: 0.489733\n",
      "[300]\ttraining's l1: 0.428936\tvalid_1's l1: 0.453475\n",
      "[400]\ttraining's l1: 0.399495\tvalid_1's l1: 0.428703\n",
      "[500]\ttraining's l1: 0.377407\tvalid_1's l1: 0.41107\n",
      "[600]\ttraining's l1: 0.359757\tvalid_1's l1: 0.397585\n",
      "[700]\ttraining's l1: 0.345294\tvalid_1's l1: 0.387083\n",
      "[800]\ttraining's l1: 0.332307\tvalid_1's l1: 0.37755\n",
      "[900]\ttraining's l1: 0.321315\tvalid_1's l1: 0.369955\n",
      "[1000]\ttraining's l1: 0.311353\tvalid_1's l1: 0.363155\n",
      "[1100]\ttraining's l1: 0.302391\tvalid_1's l1: 0.357183\n",
      "[1200]\ttraining's l1: 0.294197\tvalid_1's l1: 0.351881\n",
      "[1300]\ttraining's l1: 0.286792\tvalid_1's l1: 0.347255\n",
      "[1400]\ttraining's l1: 0.279938\tvalid_1's l1: 0.343116\n",
      "[1500]\ttraining's l1: 0.273508\tvalid_1's l1: 0.33929\n",
      "[1600]\ttraining's l1: 0.267517\tvalid_1's l1: 0.335657\n",
      "[1700]\ttraining's l1: 0.26187\tvalid_1's l1: 0.332436\n",
      "[1800]\ttraining's l1: 0.25655\tvalid_1's l1: 0.329381\n",
      "[1900]\ttraining's l1: 0.251657\tvalid_1's l1: 0.326729\n",
      "[2000]\ttraining's l1: 0.246793\tvalid_1's l1: 0.324092\n",
      "[2100]\ttraining's l1: 0.242266\tvalid_1's l1: 0.321776\n",
      "[2200]\ttraining's l1: 0.237872\tvalid_1's l1: 0.319506\n",
      "[2300]\ttraining's l1: 0.233744\tvalid_1's l1: 0.317416\n",
      "[2400]\ttraining's l1: 0.229897\tvalid_1's l1: 0.315449\n",
      "[2500]\ttraining's l1: 0.225975\tvalid_1's l1: 0.313491\n",
      "[2600]\ttraining's l1: 0.222313\tvalid_1's l1: 0.311673\n",
      "[2700]\ttraining's l1: 0.218853\tvalid_1's l1: 0.309944\n",
      "[2800]\ttraining's l1: 0.215471\tvalid_1's l1: 0.308458\n",
      "[2900]\ttraining's l1: 0.212215\tvalid_1's l1: 0.306931\n",
      "[3000]\ttraining's l1: 0.209046\tvalid_1's l1: 0.305414\n",
      "[3100]\ttraining's l1: 0.205985\tvalid_1's l1: 0.304034\n",
      "[3200]\ttraining's l1: 0.203098\tvalid_1's l1: 0.302754\n",
      "[3300]\ttraining's l1: 0.200232\tvalid_1's l1: 0.301479\n",
      "[3400]\ttraining's l1: 0.197451\tvalid_1's l1: 0.300237\n",
      "[3500]\ttraining's l1: 0.194713\tvalid_1's l1: 0.29903\n",
      "[3600]\ttraining's l1: 0.192104\tvalid_1's l1: 0.297937\n",
      "[3700]\ttraining's l1: 0.189603\tvalid_1's l1: 0.296882\n",
      "[3800]\ttraining's l1: 0.187218\tvalid_1's l1: 0.29582\n",
      "[3900]\ttraining's l1: 0.184822\tvalid_1's l1: 0.294837\n",
      "[4000]\ttraining's l1: 0.182456\tvalid_1's l1: 0.293828\n",
      "[4100]\ttraining's l1: 0.18021\tvalid_1's l1: 0.292942\n",
      "[4200]\ttraining's l1: 0.177991\tvalid_1's l1: 0.292055\n",
      "[4300]\ttraining's l1: 0.175847\tvalid_1's l1: 0.291197\n",
      "[4400]\ttraining's l1: 0.173744\tvalid_1's l1: 0.290384\n",
      "[4500]\ttraining's l1: 0.171755\tvalid_1's l1: 0.289668\n",
      "[4600]\ttraining's l1: 0.169735\tvalid_1's l1: 0.288902\n",
      "[4700]\ttraining's l1: 0.167801\tvalid_1's l1: 0.288126\n",
      "[4800]\ttraining's l1: 0.165849\tvalid_1's l1: 0.287369\n",
      "[4900]\ttraining's l1: 0.163953\tvalid_1's l1: 0.286642\n",
      "[5000]\ttraining's l1: 0.16212\tvalid_1's l1: 0.285921\n",
      "[5100]\ttraining's l1: 0.160346\tvalid_1's l1: 0.285275\n",
      "[5200]\ttraining's l1: 0.158579\tvalid_1's l1: 0.284628\n",
      "[5300]\ttraining's l1: 0.156824\tvalid_1's l1: 0.283974\n",
      "[5400]\ttraining's l1: 0.155174\tvalid_1's l1: 0.283403\n",
      "[5500]\ttraining's l1: 0.153524\tvalid_1's l1: 0.282819\n",
      "[5600]\ttraining's l1: 0.151908\tvalid_1's l1: 0.28224\n",
      "[5700]\ttraining's l1: 0.150334\tvalid_1's l1: 0.28165\n",
      "[5800]\ttraining's l1: 0.148777\tvalid_1's l1: 0.281136\n",
      "[5900]\ttraining's l1: 0.147282\tvalid_1's l1: 0.280624\n",
      "[6000]\ttraining's l1: 0.145797\tvalid_1's l1: 0.280125\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.145797\tvalid_1's l1: 0.280125\n",
      "3JHC Fold 4, logMAE: -1.272517683070479\n",
      "*** Training Model for 3JHN ***\n",
      "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8',\n",
      "       'atom_9', 'd_1_0', 'd_2_0', 'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0',\n",
      "       'd_4_1', 'd_4_2', 'd_4_3', 'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0',\n",
      "       'd_6_1', 'd_6_2', 'd_6_3', 'd_7_0', 'd_7_1', 'd_7_2', 'd_7_3', 'd_8_0',\n",
      "       'd_8_1', 'd_8_2', 'd_8_3', 'd_9_0', 'd_9_1', 'd_9_2', 'd_9_3',\n",
      "       'scalar_coupling_constant'],\n",
      "      dtype='object')\n",
      "Index(['atom_2', 'atom_3', 'atom_4', 'atom_5', 'atom_6', 'atom_7', 'atom_8',\n",
      "       'atom_9', 'd_1_0', 'd_2_0', 'd_2_1', 'd_3_0', 'd_3_1', 'd_3_2', 'd_4_0',\n",
      "       'd_4_1', 'd_4_2', 'd_4_3', 'd_5_0', 'd_5_1', 'd_5_2', 'd_5_3', 'd_6_0',\n",
      "       'd_6_1', 'd_6_2', 'd_6_3', 'd_7_0', 'd_7_1', 'd_7_2', 'd_7_3', 'd_8_0',\n",
      "       'd_8_1', 'd_8_2', 'd_8_3', 'd_9_0', 'd_9_1', 'd_9_2', 'd_9_3'],\n",
      "      dtype='object')\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.151116\tvalid_1's l1: 0.173211\n",
      "[200]\ttraining's l1: 0.120753\tvalid_1's l1: 0.151488\n",
      "[300]\ttraining's l1: 0.104059\tvalid_1's l1: 0.141625\n",
      "[400]\ttraining's l1: 0.0921055\tvalid_1's l1: 0.134959\n",
      "[500]\ttraining's l1: 0.0832582\tvalid_1's l1: 0.130317\n",
      "[600]\ttraining's l1: 0.0758058\tvalid_1's l1: 0.126952\n",
      "[700]\ttraining's l1: 0.0700027\tvalid_1's l1: 0.124497\n",
      "[800]\ttraining's l1: 0.0649309\tvalid_1's l1: 0.122366\n",
      "[900]\ttraining's l1: 0.0604481\tvalid_1's l1: 0.120615\n",
      "[1000]\ttraining's l1: 0.0565892\tvalid_1's l1: 0.119172\n",
      "[1100]\ttraining's l1: 0.0529782\tvalid_1's l1: 0.117803\n",
      "[1200]\ttraining's l1: 0.0498208\tvalid_1's l1: 0.116706\n",
      "[1300]\ttraining's l1: 0.0469292\tvalid_1's l1: 0.115738\n",
      "[1400]\ttraining's l1: 0.0443495\tvalid_1's l1: 0.114839\n",
      "[1500]\ttraining's l1: 0.041939\tvalid_1's l1: 0.114097\n",
      "[1600]\ttraining's l1: 0.0397708\tvalid_1's l1: 0.113331\n",
      "[1700]\ttraining's l1: 0.0377941\tvalid_1's l1: 0.1127\n",
      "[1800]\ttraining's l1: 0.0360206\tvalid_1's l1: 0.112222\n",
      "[1900]\ttraining's l1: 0.0343595\tvalid_1's l1: 0.111819\n",
      "[2000]\ttraining's l1: 0.0328369\tvalid_1's l1: 0.111381\n",
      "[2100]\ttraining's l1: 0.0313652\tvalid_1's l1: 0.11093\n",
      "[2200]\ttraining's l1: 0.0300035\tvalid_1's l1: 0.110564\n",
      "[2300]\ttraining's l1: 0.0288099\tvalid_1's l1: 0.110243\n",
      "[2400]\ttraining's l1: 0.0276433\tvalid_1's l1: 0.109939\n",
      "[2500]\ttraining's l1: 0.0265361\tvalid_1's l1: 0.109652\n",
      "[2600]\ttraining's l1: 0.0254808\tvalid_1's l1: 0.109428\n",
      "[2700]\ttraining's l1: 0.024481\tvalid_1's l1: 0.109228\n",
      "[2800]\ttraining's l1: 0.0235568\tvalid_1's l1: 0.109017\n",
      "[2900]\ttraining's l1: 0.0226848\tvalid_1's l1: 0.108835\n",
      "[3000]\ttraining's l1: 0.0218762\tvalid_1's l1: 0.108642\n",
      "[3100]\ttraining's l1: 0.0210801\tvalid_1's l1: 0.108462\n",
      "[3200]\ttraining's l1: 0.0203523\tvalid_1's l1: 0.108311\n",
      "[3300]\ttraining's l1: 0.0196668\tvalid_1's l1: 0.108156\n",
      "[3400]\ttraining's l1: 0.0190128\tvalid_1's l1: 0.108054\n",
      "[3500]\ttraining's l1: 0.0183997\tvalid_1's l1: 0.107912\n",
      "[3600]\ttraining's l1: 0.0178197\tvalid_1's l1: 0.107786\n",
      "[3700]\ttraining's l1: 0.0172659\tvalid_1's l1: 0.107673\n",
      "[3800]\ttraining's l1: 0.0167324\tvalid_1's l1: 0.107565\n",
      "[3900]\ttraining's l1: 0.0162219\tvalid_1's l1: 0.107457\n",
      "[4000]\ttraining's l1: 0.0157456\tvalid_1's l1: 0.107345\n",
      "[4100]\ttraining's l1: 0.0152924\tvalid_1's l1: 0.107247\n",
      "[4200]\ttraining's l1: 0.0148485\tvalid_1's l1: 0.107161\n",
      "[4300]\ttraining's l1: 0.0144449\tvalid_1's l1: 0.107082\n",
      "[4400]\ttraining's l1: 0.0140519\tvalid_1's l1: 0.106998\n",
      "[4500]\ttraining's l1: 0.0136679\tvalid_1's l1: 0.106922\n",
      "[4600]\ttraining's l1: 0.0133161\tvalid_1's l1: 0.106852\n",
      "[4700]\ttraining's l1: 0.0129725\tvalid_1's l1: 0.106788\n",
      "[4800]\ttraining's l1: 0.0126431\tvalid_1's l1: 0.106735\n",
      "[4900]\ttraining's l1: 0.012331\tvalid_1's l1: 0.106671\n",
      "[5000]\ttraining's l1: 0.012033\tvalid_1's l1: 0.106617\n",
      "[5100]\ttraining's l1: 0.0117478\tvalid_1's l1: 0.106559\n",
      "[5200]\ttraining's l1: 0.0114827\tvalid_1's l1: 0.106514\n",
      "[5300]\ttraining's l1: 0.0112232\tvalid_1's l1: 0.106474\n",
      "[5400]\ttraining's l1: 0.010981\tvalid_1's l1: 0.106424\n",
      "[5500]\ttraining's l1: 0.0107407\tvalid_1's l1: 0.106383\n",
      "[5600]\ttraining's l1: 0.0105149\tvalid_1's l1: 0.106363\n",
      "[5700]\ttraining's l1: 0.0102955\tvalid_1's l1: 0.106322\n",
      "[5800]\ttraining's l1: 0.0100802\tvalid_1's l1: 0.10628\n",
      "[5900]\ttraining's l1: 0.0098756\tvalid_1's l1: 0.106242\n",
      "[6000]\ttraining's l1: 0.00967557\tvalid_1's l1: 0.106206\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.00967557\tvalid_1's l1: 0.106206\n",
      "3JHN Fold 0, logMAE: -2.242375288818585\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.150887\tvalid_1's l1: 0.171135\n",
      "[200]\ttraining's l1: 0.120221\tvalid_1's l1: 0.149747\n",
      "[300]\ttraining's l1: 0.104225\tvalid_1's l1: 0.13974\n",
      "[400]\ttraining's l1: 0.0922644\tvalid_1's l1: 0.133263\n",
      "[500]\ttraining's l1: 0.0830221\tvalid_1's l1: 0.128826\n",
      "[600]\ttraining's l1: 0.0758649\tvalid_1's l1: 0.125561\n",
      "[700]\ttraining's l1: 0.0697194\tvalid_1's l1: 0.122926\n",
      "[800]\ttraining's l1: 0.0646651\tvalid_1's l1: 0.120901\n",
      "[900]\ttraining's l1: 0.0601011\tvalid_1's l1: 0.119213\n",
      "[1000]\ttraining's l1: 0.0561262\tvalid_1's l1: 0.117887\n",
      "[1100]\ttraining's l1: 0.0526248\tvalid_1's l1: 0.116672\n",
      "[1200]\ttraining's l1: 0.0494758\tvalid_1's l1: 0.115572\n",
      "[1300]\ttraining's l1: 0.0467601\tvalid_1's l1: 0.114726\n",
      "[1400]\ttraining's l1: 0.0442654\tvalid_1's l1: 0.113962\n",
      "[1500]\ttraining's l1: 0.0419905\tvalid_1's l1: 0.113223\n",
      "[1600]\ttraining's l1: 0.0398739\tvalid_1's l1: 0.112557\n",
      "[1700]\ttraining's l1: 0.0378487\tvalid_1's l1: 0.111928\n",
      "[1800]\ttraining's l1: 0.0360415\tvalid_1's l1: 0.111389\n",
      "[1900]\ttraining's l1: 0.03446\tvalid_1's l1: 0.110963\n",
      "[2000]\ttraining's l1: 0.0329125\tvalid_1's l1: 0.110487\n",
      "[2100]\ttraining's l1: 0.0315033\tvalid_1's l1: 0.110111\n",
      "[2200]\ttraining's l1: 0.030145\tvalid_1's l1: 0.10969\n",
      "[2300]\ttraining's l1: 0.0288397\tvalid_1's l1: 0.109326\n",
      "[2400]\ttraining's l1: 0.0276376\tvalid_1's l1: 0.109049\n",
      "[2500]\ttraining's l1: 0.0265416\tvalid_1's l1: 0.108809\n",
      "[2600]\ttraining's l1: 0.025497\tvalid_1's l1: 0.108573\n",
      "[2700]\ttraining's l1: 0.0245357\tvalid_1's l1: 0.108357\n",
      "[2800]\ttraining's l1: 0.0236156\tvalid_1's l1: 0.108124\n",
      "[2900]\ttraining's l1: 0.0227523\tvalid_1's l1: 0.107955\n",
      "[3000]\ttraining's l1: 0.0219151\tvalid_1's l1: 0.107757\n",
      "[3100]\ttraining's l1: 0.0211367\tvalid_1's l1: 0.107584\n",
      "[3200]\ttraining's l1: 0.0204075\tvalid_1's l1: 0.107432\n",
      "[3300]\ttraining's l1: 0.0197217\tvalid_1's l1: 0.107271\n",
      "[3400]\ttraining's l1: 0.0190557\tvalid_1's l1: 0.107131\n",
      "[3500]\ttraining's l1: 0.0184527\tvalid_1's l1: 0.107016\n",
      "[3600]\ttraining's l1: 0.0178695\tvalid_1's l1: 0.106886\n",
      "[3700]\ttraining's l1: 0.0173374\tvalid_1's l1: 0.106747\n",
      "[3800]\ttraining's l1: 0.0168002\tvalid_1's l1: 0.10664\n",
      "[3900]\ttraining's l1: 0.0163018\tvalid_1's l1: 0.106541\n",
      "[4000]\ttraining's l1: 0.0158412\tvalid_1's l1: 0.106456\n",
      "[4100]\ttraining's l1: 0.0153952\tvalid_1's l1: 0.106368\n",
      "[4200]\ttraining's l1: 0.0149895\tvalid_1's l1: 0.106268\n",
      "[4300]\ttraining's l1: 0.0145782\tvalid_1's l1: 0.106185\n",
      "[4400]\ttraining's l1: 0.0142053\tvalid_1's l1: 0.106108\n",
      "[4500]\ttraining's l1: 0.0138265\tvalid_1's l1: 0.106023\n",
      "[4600]\ttraining's l1: 0.0134707\tvalid_1's l1: 0.105944\n",
      "[4700]\ttraining's l1: 0.0131261\tvalid_1's l1: 0.105876\n",
      "[4800]\ttraining's l1: 0.0127981\tvalid_1's l1: 0.105815\n",
      "[4900]\ttraining's l1: 0.0124786\tvalid_1's l1: 0.105753\n",
      "[5000]\ttraining's l1: 0.0121762\tvalid_1's l1: 0.105684\n",
      "[5100]\ttraining's l1: 0.0119025\tvalid_1's l1: 0.105621\n",
      "[5200]\ttraining's l1: 0.0116281\tvalid_1's l1: 0.105559\n",
      "[5300]\ttraining's l1: 0.0113649\tvalid_1's l1: 0.105499\n",
      "[5400]\ttraining's l1: 0.011113\tvalid_1's l1: 0.105446\n",
      "[5500]\ttraining's l1: 0.0108722\tvalid_1's l1: 0.105403\n",
      "[5600]\ttraining's l1: 0.0106409\tvalid_1's l1: 0.105368\n",
      "[5700]\ttraining's l1: 0.0104203\tvalid_1's l1: 0.105328\n",
      "[5800]\ttraining's l1: 0.0102138\tvalid_1's l1: 0.105289\n",
      "[5900]\ttraining's l1: 0.0100097\tvalid_1's l1: 0.105247\n",
      "[6000]\ttraining's l1: 0.00981132\tvalid_1's l1: 0.105215\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.00981132\tvalid_1's l1: 0.105215\n",
      "3JHN Fold 1, logMAE: -2.251745012116278\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.150624\tvalid_1's l1: 0.173696\n",
      "[200]\ttraining's l1: 0.11931\tvalid_1's l1: 0.15189\n",
      "[300]\ttraining's l1: 0.102509\tvalid_1's l1: 0.141464\n",
      "[400]\ttraining's l1: 0.0914349\tvalid_1's l1: 0.13522\n",
      "[500]\ttraining's l1: 0.0827783\tvalid_1's l1: 0.130804\n",
      "[600]\ttraining's l1: 0.0757317\tvalid_1's l1: 0.127666\n",
      "[700]\ttraining's l1: 0.069542\tvalid_1's l1: 0.12492\n",
      "[800]\ttraining's l1: 0.0645368\tvalid_1's l1: 0.122943\n",
      "[900]\ttraining's l1: 0.0600459\tvalid_1's l1: 0.121143\n",
      "[1000]\ttraining's l1: 0.0560206\tvalid_1's l1: 0.119685\n",
      "[1100]\ttraining's l1: 0.0524013\tvalid_1's l1: 0.118331\n",
      "[1200]\ttraining's l1: 0.0492847\tvalid_1's l1: 0.117199\n",
      "[1300]\ttraining's l1: 0.0466054\tvalid_1's l1: 0.11631\n",
      "[1400]\ttraining's l1: 0.0440974\tvalid_1's l1: 0.115525\n",
      "[1500]\ttraining's l1: 0.0416961\tvalid_1's l1: 0.11486\n",
      "[1600]\ttraining's l1: 0.0395405\tvalid_1's l1: 0.114167\n",
      "[1700]\ttraining's l1: 0.0376211\tvalid_1's l1: 0.113573\n",
      "[1800]\ttraining's l1: 0.0357652\tvalid_1's l1: 0.113089\n",
      "[1900]\ttraining's l1: 0.0340899\tvalid_1's l1: 0.112618\n",
      "[2000]\ttraining's l1: 0.0325398\tvalid_1's l1: 0.11223\n",
      "[2100]\ttraining's l1: 0.0310651\tvalid_1's l1: 0.111862\n",
      "[2200]\ttraining's l1: 0.0297943\tvalid_1's l1: 0.111523\n",
      "[2300]\ttraining's l1: 0.0285529\tvalid_1's l1: 0.111221\n",
      "[2400]\ttraining's l1: 0.0273903\tvalid_1's l1: 0.110928\n",
      "[2500]\ttraining's l1: 0.026278\tvalid_1's l1: 0.11063\n",
      "[2600]\ttraining's l1: 0.0252405\tvalid_1's l1: 0.110363\n",
      "[2700]\ttraining's l1: 0.0242716\tvalid_1's l1: 0.110157\n",
      "[2800]\ttraining's l1: 0.0233212\tvalid_1's l1: 0.10994\n",
      "[2900]\ttraining's l1: 0.022487\tvalid_1's l1: 0.10975\n",
      "[3000]\ttraining's l1: 0.0216995\tvalid_1's l1: 0.109554\n",
      "[3100]\ttraining's l1: 0.0209627\tvalid_1's l1: 0.10937\n",
      "[3200]\ttraining's l1: 0.0202259\tvalid_1's l1: 0.109225\n",
      "[3300]\ttraining's l1: 0.0195541\tvalid_1's l1: 0.109049\n",
      "[3400]\ttraining's l1: 0.0189129\tvalid_1's l1: 0.108893\n",
      "[3500]\ttraining's l1: 0.0182994\tvalid_1's l1: 0.108769\n",
      "[3600]\ttraining's l1: 0.0177139\tvalid_1's l1: 0.108638\n",
      "[3700]\ttraining's l1: 0.0171711\tvalid_1's l1: 0.10852\n",
      "[3800]\ttraining's l1: 0.0166385\tvalid_1's l1: 0.108416\n",
      "[3900]\ttraining's l1: 0.0161356\tvalid_1's l1: 0.108309\n",
      "[4000]\ttraining's l1: 0.0156595\tvalid_1's l1: 0.108205\n",
      "[4100]\ttraining's l1: 0.0152019\tvalid_1's l1: 0.108101\n",
      "[4200]\ttraining's l1: 0.0147756\tvalid_1's l1: 0.107998\n",
      "[4300]\ttraining's l1: 0.0143638\tvalid_1's l1: 0.107922\n",
      "[4400]\ttraining's l1: 0.0139777\tvalid_1's l1: 0.107867\n",
      "[4500]\ttraining's l1: 0.013605\tvalid_1's l1: 0.107802\n",
      "[4600]\ttraining's l1: 0.0132453\tvalid_1's l1: 0.107728\n",
      "[4700]\ttraining's l1: 0.0129144\tvalid_1's l1: 0.107657\n",
      "[4800]\ttraining's l1: 0.0125954\tvalid_1's l1: 0.107605\n",
      "[4900]\ttraining's l1: 0.0122867\tvalid_1's l1: 0.107532\n",
      "[5000]\ttraining's l1: 0.0119955\tvalid_1's l1: 0.107481\n",
      "[5100]\ttraining's l1: 0.0117082\tvalid_1's l1: 0.107425\n",
      "[5200]\ttraining's l1: 0.0114363\tvalid_1's l1: 0.10737\n",
      "[5300]\ttraining's l1: 0.0111773\tvalid_1's l1: 0.107324\n",
      "[5400]\ttraining's l1: 0.0109362\tvalid_1's l1: 0.107286\n",
      "[5500]\ttraining's l1: 0.0107016\tvalid_1's l1: 0.107248\n",
      "[5600]\ttraining's l1: 0.0104712\tvalid_1's l1: 0.107211\n",
      "[5700]\ttraining's l1: 0.0102513\tvalid_1's l1: 0.107168\n",
      "[5800]\ttraining's l1: 0.0100419\tvalid_1's l1: 0.107128\n",
      "[5900]\ttraining's l1: 0.00984118\tvalid_1's l1: 0.107094\n",
      "[6000]\ttraining's l1: 0.0096523\tvalid_1's l1: 0.107069\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0096523\tvalid_1's l1: 0.107069\n",
      "3JHN Fold 2, logMAE: -2.234277948774053\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.148824\tvalid_1's l1: 0.172616\n",
      "[200]\ttraining's l1: 0.119101\tvalid_1's l1: 0.152011\n",
      "[300]\ttraining's l1: 0.102624\tvalid_1's l1: 0.141973\n",
      "[400]\ttraining's l1: 0.0908936\tvalid_1's l1: 0.135735\n",
      "[500]\ttraining's l1: 0.0821794\tvalid_1's l1: 0.131657\n",
      "[600]\ttraining's l1: 0.075168\tvalid_1's l1: 0.128606\n",
      "[700]\ttraining's l1: 0.0691234\tvalid_1's l1: 0.126013\n",
      "[800]\ttraining's l1: 0.0640519\tvalid_1's l1: 0.123803\n",
      "[900]\ttraining's l1: 0.059743\tvalid_1's l1: 0.122226\n",
      "[1000]\ttraining's l1: 0.0559913\tvalid_1's l1: 0.120951\n",
      "[1100]\ttraining's l1: 0.0524799\tvalid_1's l1: 0.119705\n",
      "[1200]\ttraining's l1: 0.0494064\tvalid_1's l1: 0.118594\n",
      "[1300]\ttraining's l1: 0.0466317\tvalid_1's l1: 0.117781\n",
      "[1400]\ttraining's l1: 0.0441194\tvalid_1's l1: 0.117065\n",
      "[1500]\ttraining's l1: 0.0418571\tvalid_1's l1: 0.116334\n",
      "[1600]\ttraining's l1: 0.0397161\tvalid_1's l1: 0.115691\n",
      "[1700]\ttraining's l1: 0.0377481\tvalid_1's l1: 0.115094\n",
      "[1800]\ttraining's l1: 0.0359613\tvalid_1's l1: 0.114592\n",
      "[1900]\ttraining's l1: 0.0342544\tvalid_1's l1: 0.11415\n",
      "[2000]\ttraining's l1: 0.0327432\tvalid_1's l1: 0.113754\n",
      "[2100]\ttraining's l1: 0.0313467\tvalid_1's l1: 0.113401\n",
      "[2200]\ttraining's l1: 0.0300656\tvalid_1's l1: 0.11306\n",
      "[2300]\ttraining's l1: 0.0288468\tvalid_1's l1: 0.112743\n",
      "[2400]\ttraining's l1: 0.0276829\tvalid_1's l1: 0.11244\n",
      "[2500]\ttraining's l1: 0.0265634\tvalid_1's l1: 0.112173\n",
      "[2600]\ttraining's l1: 0.0255303\tvalid_1's l1: 0.111934\n",
      "[2700]\ttraining's l1: 0.0245406\tvalid_1's l1: 0.111684\n",
      "[2800]\ttraining's l1: 0.0236329\tvalid_1's l1: 0.111446\n",
      "[2900]\ttraining's l1: 0.0227657\tvalid_1's l1: 0.111253\n",
      "[3000]\ttraining's l1: 0.021955\tvalid_1's l1: 0.111053\n",
      "[3100]\ttraining's l1: 0.0211752\tvalid_1's l1: 0.110864\n",
      "[3200]\ttraining's l1: 0.0204324\tvalid_1's l1: 0.110685\n",
      "[3300]\ttraining's l1: 0.019736\tvalid_1's l1: 0.110517\n",
      "[3400]\ttraining's l1: 0.0190622\tvalid_1's l1: 0.110376\n",
      "[3500]\ttraining's l1: 0.0184444\tvalid_1's l1: 0.110231\n",
      "[3600]\ttraining's l1: 0.0178434\tvalid_1's l1: 0.110109\n",
      "[3700]\ttraining's l1: 0.0172751\tvalid_1's l1: 0.109976\n",
      "[3800]\ttraining's l1: 0.0167387\tvalid_1's l1: 0.109878\n",
      "[3900]\ttraining's l1: 0.0162365\tvalid_1's l1: 0.109758\n",
      "[4000]\ttraining's l1: 0.0157684\tvalid_1's l1: 0.109662\n",
      "[4100]\ttraining's l1: 0.0153238\tvalid_1's l1: 0.109571\n",
      "[4200]\ttraining's l1: 0.0148885\tvalid_1's l1: 0.109489\n",
      "[4300]\ttraining's l1: 0.0144817\tvalid_1's l1: 0.109413\n",
      "[4400]\ttraining's l1: 0.014084\tvalid_1's l1: 0.109323\n",
      "[4500]\ttraining's l1: 0.0137059\tvalid_1's l1: 0.10925\n",
      "[4600]\ttraining's l1: 0.0133463\tvalid_1's l1: 0.10918\n",
      "[4700]\ttraining's l1: 0.0130104\tvalid_1's l1: 0.109119\n",
      "[4800]\ttraining's l1: 0.0126834\tvalid_1's l1: 0.109049\n",
      "[4900]\ttraining's l1: 0.0123829\tvalid_1's l1: 0.109004\n",
      "[5000]\ttraining's l1: 0.0120839\tvalid_1's l1: 0.10895\n",
      "[5100]\ttraining's l1: 0.0117937\tvalid_1's l1: 0.108883\n",
      "[5200]\ttraining's l1: 0.0115236\tvalid_1's l1: 0.10884\n",
      "[5300]\ttraining's l1: 0.011262\tvalid_1's l1: 0.108792\n",
      "[5400]\ttraining's l1: 0.0110159\tvalid_1's l1: 0.108742\n",
      "[5500]\ttraining's l1: 0.0107797\tvalid_1's l1: 0.108688\n",
      "[5600]\ttraining's l1: 0.0105517\tvalid_1's l1: 0.108641\n",
      "[5700]\ttraining's l1: 0.0103321\tvalid_1's l1: 0.108595\n",
      "[5800]\ttraining's l1: 0.0101239\tvalid_1's l1: 0.108555\n",
      "[5900]\ttraining's l1: 0.00991504\tvalid_1's l1: 0.108514\n",
      "[6000]\ttraining's l1: 0.00972344\tvalid_1's l1: 0.108472\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.00972344\tvalid_1's l1: 0.108472\n",
      "3JHN Fold 3, logMAE: -2.2212628081057946\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[100]\ttraining's l1: 0.149191\tvalid_1's l1: 0.1711\n",
      "[200]\ttraining's l1: 0.119415\tvalid_1's l1: 0.150914\n",
      "[300]\ttraining's l1: 0.102669\tvalid_1's l1: 0.140601\n",
      "[400]\ttraining's l1: 0.0912782\tvalid_1's l1: 0.134353\n",
      "[500]\ttraining's l1: 0.0828959\tvalid_1's l1: 0.130312\n",
      "[600]\ttraining's l1: 0.0752976\tvalid_1's l1: 0.126631\n",
      "[700]\ttraining's l1: 0.0695898\tvalid_1's l1: 0.124244\n",
      "[800]\ttraining's l1: 0.0644235\tvalid_1's l1: 0.122215\n",
      "[900]\ttraining's l1: 0.0601994\tvalid_1's l1: 0.120563\n",
      "[1000]\ttraining's l1: 0.0564485\tvalid_1's l1: 0.119147\n",
      "[1100]\ttraining's l1: 0.052914\tvalid_1's l1: 0.117804\n",
      "[1200]\ttraining's l1: 0.0497405\tvalid_1's l1: 0.116681\n",
      "[1300]\ttraining's l1: 0.0468504\tvalid_1's l1: 0.115725\n",
      "[1400]\ttraining's l1: 0.0442287\tvalid_1's l1: 0.114909\n",
      "[1500]\ttraining's l1: 0.0419491\tvalid_1's l1: 0.114166\n",
      "[1600]\ttraining's l1: 0.0398241\tvalid_1's l1: 0.113535\n",
      "[1700]\ttraining's l1: 0.0378534\tvalid_1's l1: 0.112964\n",
      "[1800]\ttraining's l1: 0.0360467\tvalid_1's l1: 0.112452\n",
      "[1900]\ttraining's l1: 0.0343664\tvalid_1's l1: 0.112\n",
      "[2000]\ttraining's l1: 0.032762\tvalid_1's l1: 0.111582\n",
      "[2100]\ttraining's l1: 0.0314022\tvalid_1's l1: 0.111228\n",
      "[2200]\ttraining's l1: 0.0300713\tvalid_1's l1: 0.110884\n",
      "[2300]\ttraining's l1: 0.0287701\tvalid_1's l1: 0.110532\n",
      "[2400]\ttraining's l1: 0.0275675\tvalid_1's l1: 0.110222\n",
      "[2500]\ttraining's l1: 0.0264597\tvalid_1's l1: 0.109946\n",
      "[2600]\ttraining's l1: 0.0254254\tvalid_1's l1: 0.109731\n",
      "[2700]\ttraining's l1: 0.024422\tvalid_1's l1: 0.10949\n",
      "[2800]\ttraining's l1: 0.0235179\tvalid_1's l1: 0.109269\n",
      "[2900]\ttraining's l1: 0.0226498\tvalid_1's l1: 0.109053\n",
      "[3000]\ttraining's l1: 0.0218345\tvalid_1's l1: 0.108866\n",
      "[3100]\ttraining's l1: 0.0210828\tvalid_1's l1: 0.108697\n",
      "[3200]\ttraining's l1: 0.0203263\tvalid_1's l1: 0.108521\n",
      "[3300]\ttraining's l1: 0.0196148\tvalid_1's l1: 0.108384\n",
      "[3400]\ttraining's l1: 0.0189667\tvalid_1's l1: 0.108217\n",
      "[3500]\ttraining's l1: 0.0183481\tvalid_1's l1: 0.108085\n",
      "[3600]\ttraining's l1: 0.0177478\tvalid_1's l1: 0.107945\n",
      "[3700]\ttraining's l1: 0.0171786\tvalid_1's l1: 0.107828\n",
      "[3800]\ttraining's l1: 0.0166631\tvalid_1's l1: 0.107716\n",
      "[3900]\ttraining's l1: 0.0161738\tvalid_1's l1: 0.107617\n",
      "[4000]\ttraining's l1: 0.0157023\tvalid_1's l1: 0.107508\n",
      "[4100]\ttraining's l1: 0.0152526\tvalid_1's l1: 0.10741\n",
      "[4200]\ttraining's l1: 0.0148169\tvalid_1's l1: 0.107333\n",
      "[4300]\ttraining's l1: 0.0143971\tvalid_1's l1: 0.107255\n",
      "[4400]\ttraining's l1: 0.0140141\tvalid_1's l1: 0.107178\n",
      "[4500]\ttraining's l1: 0.0136506\tvalid_1's l1: 0.107106\n",
      "[4600]\ttraining's l1: 0.0132829\tvalid_1's l1: 0.107046\n",
      "[4700]\ttraining's l1: 0.0129373\tvalid_1's l1: 0.106969\n",
      "[4800]\ttraining's l1: 0.0126163\tvalid_1's l1: 0.106894\n",
      "[4900]\ttraining's l1: 0.0123107\tvalid_1's l1: 0.106836\n",
      "[5000]\ttraining's l1: 0.0120196\tvalid_1's l1: 0.106784\n",
      "[5100]\ttraining's l1: 0.0117471\tvalid_1's l1: 0.10674\n",
      "[5200]\ttraining's l1: 0.011472\tvalid_1's l1: 0.106709\n",
      "[5300]\ttraining's l1: 0.0112137\tvalid_1's l1: 0.106664\n",
      "[5400]\ttraining's l1: 0.0109636\tvalid_1's l1: 0.106619\n",
      "[5500]\ttraining's l1: 0.0107205\tvalid_1's l1: 0.106578\n",
      "[5600]\ttraining's l1: 0.0104938\tvalid_1's l1: 0.106537\n",
      "[5700]\ttraining's l1: 0.0102724\tvalid_1's l1: 0.106485\n",
      "[5800]\ttraining's l1: 0.0100625\tvalid_1's l1: 0.106463\n",
      "[5900]\ttraining's l1: 0.00986631\tvalid_1's l1: 0.106425\n",
      "[6000]\ttraining's l1: 0.00967011\tvalid_1's l1: 0.106387\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.00967011\tvalid_1's l1: 0.106387\n",
      "3JHN Fold 4, logMAE: -2.240673933822566\n"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    '1JHN': 7,\n",
    "    '1JHC': 10,\n",
    "    '2JHH': 9,\n",
    "    '2JHN': 9,\n",
    "    '2JHC': 9,\n",
    "    '3JHH': 9,\n",
    "    '3JHC': 10,\n",
    "    '3JHN': 10\n",
    "}\n",
    "N_FOLDS = 5\n",
    "submission = submission_csv.copy()\n",
    "\n",
    "cv_scores = {}\n",
    "for coupling_type in model_params.keys():\n",
    "    cv_score = train_and_predict_for_one_coupling_type(\n",
    "        coupling_type, submission, n_atoms=model_params[coupling_type], n_folds=N_FOLDS)\n",
    "    cv_scores[coupling_type] = cv_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking cross-validation scores for each type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>cv_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1JHN</td>\n",
       "      <td>-0.994319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1JHC</td>\n",
       "      <td>-0.385266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2JHH</td>\n",
       "      <td>-1.814293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2JHN</td>\n",
       "      <td>-2.003074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2JHC</td>\n",
       "      <td>-1.363230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3JHH</td>\n",
       "      <td>-1.877683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3JHC</td>\n",
       "      <td>-1.271648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3JHN</td>\n",
       "      <td>-2.238067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  cv_score\n",
       "0  1JHN -0.994319\n",
       "1  1JHC -0.385266\n",
       "2  2JHH -1.814293\n",
       "3  2JHN -2.003074\n",
       "4  2JHC -1.363230\n",
       "5  3JHH -1.877683\n",
       "6  3JHC -1.271648\n",
       "7  3JHN -2.238067"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'type': list(cv_scores.keys()), 'cv_score': list(cv_scores.values())})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And cv mean score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.493447655231976"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(list(cv_scores.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check for all cells to be filled with predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission[submission['scalar_coupling_constant'] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scalar_coupling_constant</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4658147</th>\n",
       "      <td>22.414471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658148</th>\n",
       "      <td>142.809891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658149</th>\n",
       "      <td>9.560091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658150</th>\n",
       "      <td>142.809891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658151</th>\n",
       "      <td>22.414471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658152</th>\n",
       "      <td>91.488388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658153</th>\n",
       "      <td>2.440442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658154</th>\n",
       "      <td>-7.526279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658155</th>\n",
       "      <td>-9.623013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658156</th>\n",
       "      <td>91.492561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         scalar_coupling_constant\n",
       "id                               \n",
       "4658147  22.414471               \n",
       "4658148  142.809891              \n",
       "4658149  9.560091                \n",
       "4658150  142.809891              \n",
       "4658151  22.414471               \n",
       "4658152  91.488388               \n",
       "4658153  2.440442                \n",
       "4658154 -7.526279                \n",
       "4658155 -9.623013                \n",
       "4658156  91.492561               "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(f'{SUBMISSIONS_PATH}/submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Room for improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many steps, how to improve the score for this kernel:\n",
    "* Tune LGB hyperparameters - I did nothing for this\n",
    "* Tune number of atoms for each type\n",
    "* Try to add other features\n",
    "* Play with categorical features for atom types (one-hot-encoding, CatBoost?)\n",
    "* Try other tree libraries\n",
    "\n",
    "Also, this representation fails badly on `*JHC` coupling types. The main reason for this is that 3rd and 4th atoms are usually located on the same distance and representation starts \"jittering\" randomly picking one of them. So, two similar configurations will have different representation due to usage of 3/4 of 4/3 distances.\n",
    "\n",
    "The biggest challenge would be to implement handcrafted KNN with some compiled language(Rust, C++, C).\n",
    "\n",
    "Would be cool to see this kernel forked and addressed some of the issues with higher LB score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
