{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "If you feel 16GB memory is too small for >200 features bond-wise, you are not alone. In this kernel, we updated Andrew's workflow by generating features for each type to save memory. No `reduce_mem_usage` by Andrew is used (except when importing QM9), so every computation is retained its `np.float64` default accuracy. \n",
    "\n",
    "Within the loop of the training for each type, first the features are generated for a minimal passed train/test dataframe, then [Giba's features](https://www.kaggle.com/scaomath/lgb-giba-features-qm9-custom-objective-in-python) are loaded using the format of a kernel I made earlier. Finally the [Yukawa potentials](https://www.kaggle.com/scaomath/parallelization-of-coulomb-yukawa-interaction) are added as well using `structures` dataframe by type to save a tons of memory.\n",
    "\n",
    "Just wrapping your feature generation into a function you are good to go.\n",
    "\n",
    "\n",
    "### References:\n",
    "- [Brute force feature engineering](https://www.kaggle.com/artgor/brute-force-feature-engineering)\n",
    "- [Keras Neural Net for CHAMPS](https://www.kaggle.com/todnewman/keras-neural-net-for-champs)\n",
    "- [Giba R + data.table + Simple Features](https://www.kaggle.com/titericz/giba-r-data-table-simple-features-1-17-lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "pd.options.display.precision = 15\n",
    "\n",
    "import lightgbm as lgb\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import linear_model\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_numbers = {\n",
    "    'H': 1,\n",
    "    'C': 6,\n",
    "    'N': 7,\n",
    "    'O': 8,\n",
    "    'F': 9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', 120)\n",
    "pd.set_option('display.max_columns', 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_kg_hide-output": false,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "## Andrew's utils\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    \n",
    "    # 初期状態のメモリ消費量(MB)確認 （=value.nbytes)\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    \n",
    "    # 各列について\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        # 列のタイプがint16~64 or float16~64なら\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            # int型なら\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                # 列の全値がint8型の範囲に収まるなら列の値をint8型に変換\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                # 列の全値がint16型の範囲に収まるなら...\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                # 列の全値がint32型の範囲に収まるなら...\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                # 列の全値がint64型の範囲に収まるなら...\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            # float型なら\n",
    "            else:\n",
    "                # 列の全値がfloat32型の範囲に収まるなら列の値をfloat32型に変換\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                # 列の値をfloat64型に\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    # 型変換の後のメモリ消費量を確認\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n",
    "# 評価指標\n",
    "def group_mean_log_mae(y_true, y_pred, types, floor=1e-9):\n",
    "    \"\"\"\n",
    "    Fast metric computation for this competition: https://www.kaggle.com/c/champs-scalar-coupling\n",
    "    Code is from this kernel: https://www.kaggle.com/uberkinder/efficient-metric\n",
    "    \"\"\"\n",
    "    maes = (y_true-y_pred).abs().groupby(types).mean()\n",
    "    return np.log(maes.map(lambda x: max(x, floor))).mean()\n",
    "    \n",
    "\n",
    "def train_model_regression(X, X_test, y, params, folds, model_type='lgb',\n",
    "                           eval_metric='mae', columns=None, plot_feature_importance=False,\n",
    "                           model=None, verbose=10000, early_stopping_rounds=200,\n",
    "                           n_estimators=50000):\n",
    "    \"\"\"\n",
    "    A function to train a variety of regression models.\n",
    "    Returns dictionary with oof predictions, test predictions, scores and, if necessary, feature importances.\n",
    "    \n",
    "    :params: X - training data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: X_test - test data, can be pd.DataFrame or np.ndarray (after normalizing)\n",
    "    :params: y - target\n",
    "    :params: folds - folds to split data\n",
    "    :params: model_type - type of model to use\n",
    "    :params: eval_metric - metric to use\n",
    "    :params: columns - columns to use. If None - use all columns\n",
    "    :params: plot_feature_importance - whether to plot feature importance of LGB\n",
    "    :params: model - sklearn model, works only for \"sklearn\" model type\n",
    "    \n",
    "    \"\"\"\n",
    "    columns = X.columns if columns is None else columns\n",
    "    X_test = X_test[columns]\n",
    "    \n",
    "    # to set up scoring parameters\n",
    "    metrics_dict = {'mae': {'lgb_metric_name': 'mae',\n",
    "                        'catboost_metric_name': 'MAE',\n",
    "                        'sklearn_scoring_function': metrics.mean_absolute_error},\n",
    "                    'group_mae': {'lgb_metric_name': 'mae',\n",
    "                        'catboost_metric_name': 'MAE',\n",
    "                        'scoring_function': group_mean_log_mae},\n",
    "                    'mse': {'lgb_metric_name': 'mse',\n",
    "                        'catboost_metric_name': 'MSE',\n",
    "                        'sklearn_scoring_function': metrics.mean_squared_error}\n",
    "                    }\n",
    "\n",
    "    \n",
    "    result_dict = {}\n",
    "    \n",
    "    # out-of-fold predictions on train data\n",
    "    oof = np.zeros(len(X))\n",
    "    \n",
    "    # averaged predictions on train（test?) data\n",
    "    prediction = np.zeros(len(X_test))\n",
    "    \n",
    "    # list of scores on folds\n",
    "    scores = []\n",
    "    feature_importance = pd.DataFrame()\n",
    "    \n",
    "    # split and train on folds\n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X)):\n",
    "        # 各Foldについて\n",
    "        # k-fold目の開始時間\n",
    "        print(f'Fold {fold_n + 1} started at {time.ctime()}')\n",
    "        if type(X) == np.ndarray:\n",
    "            X_train, X_valid = X[columns][train_index], X[columns][valid_index]\n",
    "            y_train, y_valid = y[train_index], y[valid_index]\n",
    "        else: # pd.DataFrameの場合\n",
    "            X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]\n",
    "            y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "            \n",
    "        # LightGBM\n",
    "        if model_type == 'lgb':\n",
    "            model = lgb.LGBMRegressor(**params, n_estimators = n_estimators, n_jobs = -1)\n",
    "            model.fit(X_train, y_train, \n",
    "                    eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=metrics_dict[eval_metric]['lgb_metric_name'],\n",
    "                    verbose=verbose, early_stopping_rounds=early_stopping_rounds)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "            \n",
    "        # XGBoost\n",
    "        if model_type == 'xgb':\n",
    "            train_data = xgb.DMatrix(data=X_train, label=y_train, feature_names=X.columns)\n",
    "            valid_data = xgb.DMatrix(data=X_valid, label=y_valid, feature_names=X.columns)\n",
    "\n",
    "            watchlist = [(train_data, 'train'), (valid_data, 'valid_data')]\n",
    "            model = xgb.train(dtrain=train_data, num_boost_round=20000, evals=watchlist, early_stopping_rounds=200, verbose_eval=verbose, params=params)\n",
    "            y_pred_valid = model.predict(xgb.DMatrix(X_valid, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test, feature_names=X.columns), ntree_limit=model.best_ntree_limit)\n",
    "        \n",
    "        # sklearnのモデル\n",
    "        if model_type == 'sklearn':\n",
    "            model = model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_valid = model.predict(X_valid).reshape(-1,)\n",
    "            score = metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid)\n",
    "            print(f'Fold {fold_n}. {eval_metric}: {score:.4f}.')\n",
    "            print('')\n",
    "            \n",
    "            y_pred = model.predict(X_test).reshape(-1,)\n",
    "        \n",
    "        # CatBoost\n",
    "        if model_type == 'cat':\n",
    "            model = CatBoostRegressor(iterations=20000,  eval_metric=metrics_dict[eval_metric]['catboost_metric_name'], **params,\n",
    "                                      loss_function=metrics_dict[eval_metric]['catboost_metric_name'])\n",
    "            model.fit(X_train, y_train, eval_set=(X_valid, y_valid), cat_features=[], use_best_model=True, verbose=False)\n",
    "\n",
    "            y_pred_valid = model.predict(X_valid)\n",
    "            y_pred = model.predict(X_test)\n",
    "        \n",
    "        # 現在のFoldにおけるvalidデータの予測値\n",
    "        oof[valid_index] = y_pred_valid.reshape(-1,)\n",
    "        # validデータに関するscore\n",
    "        if eval_metric != 'group_mae': # sklearnのscore関数を用いる\n",
    "            scores.append(metrics_dict[eval_metric]['sklearn_scoring_function'](y_valid, y_pred_valid))\n",
    "        else:\n",
    "            scores.append(metrics_dict[eval_metric]['scoring_function'](y_valid, y_pred_valid, X_valid['type']))\n",
    "\n",
    "        # testデータの予測値に足し合わせる×foldの数\n",
    "        prediction += y_pred    \n",
    "        \n",
    "        if model_type == 'lgb' and plot_feature_importance:\n",
    "            # feature importance\n",
    "            # 各foldについてfeature importance算出\n",
    "            fold_importance = pd.DataFrame()\n",
    "            fold_importance[\"feature\"] = columns\n",
    "            fold_importance[\"importance\"] = model.feature_importances_\n",
    "            fold_importance[\"fold\"] = fold_n + 1\n",
    "            # 縦に連結\n",
    "            feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n",
    "\n",
    "        # for文ここまで----\n",
    "        \n",
    "    # foldの数だけ足し合わせた予測値をfoldの数で割る→cv score\n",
    "    prediction /= folds.n_splits\n",
    "    \n",
    "    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n",
    "    \n",
    "    # 各foldでvalidになっている部分の予測値を埋めたので、全てのデータのoofでの予測値が入る\n",
    "    result_dict['oof'] = oof\n",
    "    # testデータの予測値\n",
    "    result_dict['prediction'] = prediction\n",
    "    # validデータのscore\n",
    "    result_dict['scores'] = scores\n",
    "    \n",
    "    if model_type == 'lgb':\n",
    "        if plot_feature_importance:\n",
    "            # 各特徴量について、feature importanceがfold数の分あるので、fold数で割る\n",
    "            feature_importance[\"importance\"] /= folds.n_splits\n",
    "            # feature_importanceのDataFrameの\"feature\", \"importance\"列について操作\n",
    "            # \"feature\"ごとの\"importance\"の平均値を取得、\n",
    "            # 降順にソートした上位50個の特徴量をcolとする\n",
    "            cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n",
    "                by=\"importance\", ascending=False)[:50].index\n",
    "\n",
    "            # 上位50個の特徴量についての行を抽出、best_featuresとする\n",
    "            best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n",
    "\n",
    "            # plotの描画\n",
    "            plt.figure(figsize=(16, 12));\n",
    "            # best_featuresについて降順にソートしてプロット\n",
    "            sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\n",
    "            plt.title('LGB Features (avg over folds)');\n",
    "            \n",
    "            result_dict['feature_importance'] = feature_importance\n",
    "        \n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dtypes = {\n",
    "    'molecule_name': 'category',\n",
    "    'atom_index_0': 'int8',\n",
    "    'atom_index_1': 'int8',\n",
    "    'type': 'category',\n",
    "    'scalar_coupling_constant': 'float32'\n",
    "}\n",
    "structure_dtypes = {\n",
    "    'molecule_name': 'category',\n",
    "    'atom_index': 'int8',\n",
    "    'atom': 'category',\n",
    "    'x': 'float32',\n",
    "    'y': 'float32',\n",
    "    'z': 'float32'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_folder = 'champs-scalar-coupling' \n",
    "# '../input/champs-scalar-coupling'  (kaggle)\n",
    "train = pd.read_csv(f'{file_folder}/train.csv', index_col='id', dtype=train_dtypes)\n",
    "train['molecule_index'] = train.molecule_name.str.replace('dsgdb9nsd_', ' ').astype('int32')\n",
    "train = train[['molecule_index', 'atom_index_0', 'atom_index_1', 'type', 'scalar_coupling_constant']]\n",
    "\n",
    "test = pd.read_csv(f'{file_folder}/test.csv', index_col='id', dtype=train_dtypes)\n",
    "test['molecule_index'] = test.molecule_name.str.replace('dsgdb9nsd_', ' ').astype('int32')\n",
    "test = test[['molecule_index', 'atom_index_0', 'atom_index_1', 'type']]\n",
    "sub = pd.read_csv(f'{file_folder}/sample_submission.csv')\n",
    "structures = pd.read_csv(f'{file_folder}/structures.csv', dtype=structure_dtypes)\n",
    "structures['molecule_index'] = structures.molecule_name.str.replace('dsgdb9nsd_', ' ').astype('int32')\n",
    "structures = structures[['molecule_index', 'atom_index', 'atom', 'x', 'y', 'z']]\n",
    "structures['atom'] = structures['atom'].replace(atomic_numbers).astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_index</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    molecule_index  atom_index_0  atom_index_1  type\n",
       "id                                                  \n",
       "0   1               1             0             0   \n",
       "1   1               1             2             3   \n",
       "2   1               1             3             3   \n",
       "3   1               1             4             3   \n",
       "4   1               2             0             0   "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_index</th>\n",
       "      <th>atom_index_0</th>\n",
       "      <th>atom_index_1</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4658147</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658148</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658149</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658150</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4658151</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         molecule_index  atom_index_0  atom_index_1  type\n",
       "id                                                       \n",
       "4658147  4               2             0             2   \n",
       "4658148  4               2             1             0   \n",
       "4658149  4               2             3             6   \n",
       "4658150  4               3             0             0   \n",
       "4658151  4               3             1             2   "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>molecule_index</th>\n",
       "      <th>atom_index</th>\n",
       "      <th>atom</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>dist_C_0</th>\n",
       "      <th>dist_C_1</th>\n",
       "      <th>dist_C_2</th>\n",
       "      <th>dist_C_3</th>\n",
       "      <th>dist_C_4</th>\n",
       "      <th>dist_F_0</th>\n",
       "      <th>dist_F_1</th>\n",
       "      <th>dist_F_2</th>\n",
       "      <th>dist_F_3</th>\n",
       "      <th>dist_F_4</th>\n",
       "      <th>dist_H_0</th>\n",
       "      <th>dist_H_1</th>\n",
       "      <th>dist_H_2</th>\n",
       "      <th>dist_H_3</th>\n",
       "      <th>dist_H_4</th>\n",
       "      <th>dist_N_0</th>\n",
       "      <th>dist_N_1</th>\n",
       "      <th>dist_N_2</th>\n",
       "      <th>dist_N_3</th>\n",
       "      <th>dist_N_4</th>\n",
       "      <th>dist_O_0</th>\n",
       "      <th>dist_O_1</th>\n",
       "      <th>dist_O_2</th>\n",
       "      <th>dist_O_3</th>\n",
       "      <th>dist_O_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.012698136270046</td>\n",
       "      <td>1.085804104804993</td>\n",
       "      <td>0.008000995963812</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.743646143903161</td>\n",
       "      <td>0.743643612607970</td>\n",
       "      <td>0.743635366509482</td>\n",
       "      <td>0.743632427238267</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002150415908545</td>\n",
       "      <td>-0.006031317636371</td>\n",
       "      <td>0.001976120285690</td>\n",
       "      <td>1.614539835628211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075901173046231</td>\n",
       "      <td>0.075897630886224</td>\n",
       "      <td>0.075896463024099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.011730790138245</td>\n",
       "      <td>1.463751196861267</td>\n",
       "      <td>0.000276574806776</td>\n",
       "      <td>1.614545880907262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075901264970942</td>\n",
       "      <td>0.075897598650848</td>\n",
       "      <td>0.075896417359001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.540815055370331</td>\n",
       "      <td>1.447526574134827</td>\n",
       "      <td>-0.876643717288971</td>\n",
       "      <td>1.614562970596873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075897722809510</td>\n",
       "      <td>0.075897669086822</td>\n",
       "      <td>0.075896417359001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.523813605308533</td>\n",
       "      <td>1.437932610511780</td>\n",
       "      <td>0.906397283077240</td>\n",
       "      <td>1.614557932820374</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075897577163561</td>\n",
       "      <td>0.075897506727611</td>\n",
       "      <td>0.075896463024099</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   molecule_index  atom_index  atom                  x                  y  \\\n",
       "0  1               0           6    -0.012698136270046  1.085804104804993   \n",
       "1  1               1           1     0.002150415908545 -0.006031317636371   \n",
       "2  1               2           1     1.011730790138245  1.463751196861267   \n",
       "3  1               3           1    -0.540815055370331  1.447526574134827   \n",
       "4  1               4           1    -0.523813605308533  1.437932610511780   \n",
       "\n",
       "                   z           dist_C_0  dist_C_1  dist_C_2  dist_C_3  \\\n",
       "0  0.008000995963812 NaN                NaN       NaN       NaN         \n",
       "1  0.001976120285690  1.614539835628211 NaN       NaN       NaN         \n",
       "2  0.000276574806776  1.614545880907262 NaN       NaN       NaN         \n",
       "3 -0.876643717288971  1.614562970596873 NaN       NaN       NaN         \n",
       "4  0.906397283077240  1.614557932820374 NaN       NaN       NaN         \n",
       "\n",
       "   dist_C_4  dist_F_0  dist_F_1  dist_F_2  dist_F_3  dist_F_4  \\\n",
       "0 NaN       NaN       NaN       NaN       NaN       NaN         \n",
       "1 NaN       NaN       NaN       NaN       NaN       NaN         \n",
       "2 NaN       NaN       NaN       NaN       NaN       NaN         \n",
       "3 NaN       NaN       NaN       NaN       NaN       NaN         \n",
       "4 NaN       NaN       NaN       NaN       NaN       NaN         \n",
       "\n",
       "            dist_H_0           dist_H_1           dist_H_2           dist_H_3  \\\n",
       "0  0.743646143903161  0.743643612607970  0.743635366509482  0.743632427238267   \n",
       "1  0.075901173046231  0.075897630886224  0.075896463024099 NaN                  \n",
       "2  0.075901264970942  0.075897598650848  0.075896417359001 NaN                  \n",
       "3  0.075897722809510  0.075897669086822  0.075896417359001 NaN                  \n",
       "4  0.075897577163561  0.075897506727611  0.075896463024099 NaN                  \n",
       "\n",
       "   dist_H_4  dist_N_0  dist_N_1  dist_N_2  dist_N_3  dist_N_4  dist_O_0  \\\n",
       "0 NaN       NaN       NaN       NaN       NaN       NaN       NaN         \n",
       "1 NaN       NaN       NaN       NaN       NaN       NaN       NaN         \n",
       "2 NaN       NaN       NaN       NaN       NaN       NaN       NaN         \n",
       "3 NaN       NaN       NaN       NaN       NaN       NaN       NaN         \n",
       "4 NaN       NaN       NaN       NaN       NaN       NaN       NaN         \n",
       "\n",
       "   dist_O_1  dist_O_2  dist_O_3  dist_O_4  \n",
       "0 NaN       NaN       NaN       NaN        \n",
       "1 NaN       NaN       NaN       NaN        \n",
       "2 NaN       NaN       NaN       NaN        \n",
       "3 NaN       NaN       NaN       NaN        \n",
       "4 NaN       NaN       NaN       NaN        "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_energy = pd.read_csv(f'{file_folder}/potential_energy.csv')\n",
    "mulliken_charges = pd.read_csv(f'{file_folder}/mulliken_charges.csv')\n",
    "scalar_coupling_contributions = pd.read_csv(f'{file_folder}/scalar_coupling_contributions.csv')\n",
    "magnetic_shielding_tensors = pd.read_csv(f'{file_folder}/magnetic_shielding_tensors.csv')\n",
    "dipole_moments = pd.read_csv(f'{file_folder}/dipole_moments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'parallelization-of-coulomb-yukawa-interaction'\n",
    "structures_yukawa = pd.read_csv(f'{path}/structures_yukawa.csv')\n",
    "structures = pd.concat([structures, structures_yukawa], axis=1)\n",
    "del structures_yukawa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['scalar_coupling_constant']\n",
    "train = train.drop(columns = ['scalar_coupling_constant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Distance Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_type_dataframes(base, structures, coupling_type):\n",
    "    base = base[base['type'] == coupling_type].copy()\n",
    "    base = base.reset_index()\n",
    "    base['id'] = base['id'].astype('int32')\n",
    "    structures = structures[structures['molecule_index'].isin(base['molecule_index'])]\n",
    "    return base, structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_atoms(base, atoms):\n",
    "    df = pd.merge(base, atoms, how='inner', on=['molecule_index', 'atom_index_0', 'atom_index_1'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_atoms(base, structures):\n",
    "    df = pd.merge(base, structures, how='left', left_on=['molecule_index'], right_on=['molecule_index'])\n",
    "    df = df[(df.atom_index_0 != df.atom_index) & (df.atom_index_1 != df.atom_index)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_center(df):\n",
    "    df['x_center'] = (df['x_1'] + df['x_0']) * 0.5\n",
    "    df['y_center'] = (df['y_1'] + df['y_0']) * 0.5\n",
    "    df['z_center'] = (df['z_1'] + df['z_0']) * 0.5\n",
    "    \n",
    "\n",
    "def add_distance_to_center(df):\n",
    "    df['d_center'] = ((df['x_center'] - df['x']) ** 2 + (df['y_center'] - df['y']) ** 2 + (df['z_center'] - df['z']) ** 2) ** 0.5\n",
    "    \n",
    "    \n",
    "def add_distance_between(df, suffix1, suffix2):\n",
    "    df[f'd_{suffix1}_{suffix2}'] = ((df[f'x_{suffix1}'] - df[f'x_{suffix2}']) ** 2 + (df[f'y_{suffix1}'] - df[f'y_{suffix2}']) ** 2 + (df[f'z_{suffix1}'] - df[f'z_{suffix2}']) ** 2) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distances(df):\n",
    "    n_atoms = 1 + max([int(c.split('_')[1]) for c in df.columns if c.startwith('x_')])\n",
    "    \n",
    "    for i in range(1, n_atoms):\n",
    "        for vi in range(min(4, i)):\n",
    "            add_distance_between(df, i, vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_n_atoms(base, structures):\n",
    "    dfs = structures['molecule_index'].value_counts().rename('n_atoms').to_frame()\n",
    "    return pd.merge(base, dfs, left_on='molecule_index', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_couple_dataframe(some_csv, structures_csv, coupling_type, n_atoms=10):\n",
    "    base, structures = build_type_dataframes(some_csv, structures_csv, coupling_type)\n",
    "    base = get_features(base, structures.copy())\n",
    "    base = base[good_columns].fillna(0.0)\n",
    "    \n",
    "    # base = base.drop(['atom_0', 'atom_1'], axis=1)\n",
    "    # atoms = base.drop('id', axis=1).copy()\n",
    "    if 'scalar_coupling_constant' in some_csv:\n",
    "        atoms = atoms.drop('scalar_coupling_constant', axis=1)\n",
    "        \n",
    "    add_center(atoms)\n",
    "    atoms = atoms.drop(['x_0', 'y_0', 'z_0', 'x_1', 'y_1', 'z_1'], axis=1)\n",
    "    \n",
    "    atoms = merge_all_atoms(atoms, structures)\n",
    "    \n",
    "    add_distance_to_center(atoms)\n",
    "    \n",
    "    atoms = atoms.drop(['x_center', 'y_center', 'z_center', 'atom_index'], axis=1)\n",
    "    atoms.sort_values(['molecule_index', 'atom_index_0', 'atom_index_1', 'd_center'], inplace=True)\n",
    "    atom_groups = atoms.groupby(['molecule_index', 'atom_index_0', 'atom_index_1'])\n",
    "    atoms['num'] = atom_groups.cumcount() + 2\n",
    "    atoms = atoms.drop(['d_center'], axis=1)\n",
    "    atoms = atoms[atoms['num'] < n_atoms]\n",
    "    \n",
    "    atoms = atoms.set_index(['molecule_index', 'atom_index_0', 'atom_index_1', 'num']).unstack()\n",
    "    atoms.columns = [f'{col[0]}_{col[1]}' for col in atoms.columns]\n",
    "    atoms = atoms.reset_index()\n",
    "    \n",
    "    for col in atoms.columns:\n",
    "        if col.startwith('atom_'):\n",
    "            atoms[col] = atoms[col].fillna(0).astype('int8')\n",
    "            \n",
    "    atoms['molecule_index'] = atoms['molecule_index'].astype('int32')\n",
    "    \n",
    "    full = add_atoms(base, atoms)\n",
    "    add_distances(full)\n",
    "    \n",
    "    full.sort_values('id', inplace=True)\n",
    "    \n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_n_atoms(df, n_atoms, four_start=4):\n",
    "    labels = []\n",
    "    for i in range(2, n_atoms):\n",
    "        label = f'atom_{i}'\n",
    "        labels.append(label)\n",
    "        \n",
    "    for i in range(n_atoms):\n",
    "        num = min(i, 4) if i < four_start else 4\n",
    "        for j in range(num):\n",
    "            labels.append(f'd_{i}_{j}')\n",
    "            \n",
    "    if 'scalar_coupling_constant' in df:\n",
    "        labels.append('scalar_coupling_constant')\n",
    "        \n",
    "    return df[labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature generation funcs\n",
    "\n",
    "The features here are:\n",
    "\n",
    "- First the `type` is encoded by a label encoder.\n",
    "- The merging template and selected features from [Andrew's brute force feature engineering](https://www.kaggle.com/artgor/brute-force-feature-engineering)\n",
    "- Cosine features originally from [Effective feature](https://www.kaggle.com/kmat2019/effective-feature) and expanded in [Keras Neural Net for CHAMPS](https://www.kaggle.com/todnewman/keras-neural-net-for-champs), I simplified the generation procedure by removing unnecessary `pandas` operations since vanilla `numpy` arrays operation is faster.\n",
    "- QM9 dataset from [Quantum Machine 9 - QM9](https://www.kaggle.com/zaharch/quantum-machine-9-qm9).\n",
    "- Parallelization computed [Yukawa potentials](https://www.kaggle.com/scaomath/parallelization-of-coulomb-yukawa-interaction).\n",
    "- Giba's features from [Giba R + data.table + Simple Features](https://www.kaggle.com/titericz/giba-r-data-table-simple-features-1-17-lb), which I now export the features to a dataset: [Giba molecular features](https://www.kaggle.com/scaomath/giba-molecular-features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# DataFrameの結合\n",
    "def map_atom_info(df_1, df_2, atom_idx):\n",
    "    # df_1とdf_2の'molecule_name'、df_1の'atom_index_i'とdf_2の'atom_index'が\n",
    "    # 共通している列をキーとして結合\n",
    "    # df_1のサンプルは全て残す（df_2の列を持たない部分はNanに）\n",
    "    df = pd.merge(df_1, df_2, how = 'left',\n",
    "                  left_on  = ['molecule_index', f'atom_index_{atom_idx}'],\n",
    "                  right_on = ['molecule_index',  'atom_index'])\n",
    "    \n",
    "    # 'atom_index'の列の値が重複するので削除\n",
    "    df = df.drop('atom_index', axis=1)\n",
    "    return df\n",
    "\n",
    "# 距離計算\n",
    "def find_dist(df):\n",
    "    # 注目する2原子について\n",
    "    # 1つ目の原子の座標\n",
    "    df_p_0 = df[['x_0', 'y_0', 'z_0']].values\n",
    "    # 2つ目の原子の座標\n",
    "    df_p_1 = df[['x_1', 'y_1', 'z_1']].values\n",
    "    \n",
    "    # 距離\n",
    "    df['dist'] = np.linalg.norm(df_p_0 - df_p_1, axis=1)\n",
    "    # 1/(距離)^2\n",
    "    df['dist_inv2'] = 1/df['dist']**2\n",
    "    # x, y, z方向の距離\n",
    "    df['dist_x'] = (df['x_0'] - df['x_1']) ** 2\n",
    "    df['dist_y'] = (df['y_0'] - df['y_1']) ** 2\n",
    "    df['dist_z'] = (df['z_0'] - df['z_1']) ** 2\n",
    "    return df\n",
    "\n",
    "# \n",
    "def find_closest_atom(df):    \n",
    "    df_temp = df.loc[:,[\"molecule_index\",\n",
    "                      \"atom_index_0\",\"atom_index_1\",\n",
    "                      \"dist\",\"x_0\",\"y_0\",\"z_0\",\"x_1\",\"y_1\",\"z_1\"]].copy()\n",
    "    df_temp_ = df_temp.copy()\n",
    "    # 2原子の順番を逆にして一列で全ての原子を網羅\n",
    "    df_temp_ = df_temp_.rename(columns={'atom_index_0': 'atom_index_1',\n",
    "                                       'atom_index_1': 'atom_index_0',\n",
    "                                       'x_0': 'x_1',\n",
    "                                       'y_0': 'y_1',\n",
    "                                       'z_0': 'z_1',\n",
    "                                       'x_1': 'x_0',\n",
    "                                       'y_1': 'y_0',\n",
    "                                       'z_1': 'z_0'})\n",
    "    df_temp_all = pd.concat((df_temp,df_temp_),axis=0)\n",
    "\n",
    "    # 各分子の一原子について、最も近い原子との距離(min)、最も遠い原子との距離(max)を求める\n",
    "    df_temp_all[\"min_distance\"]=df_temp_all.groupby(['molecule_index', \n",
    "                                                     'atom_index_0'])['dist'].transform('min')\n",
    "    df_temp_all[\"max_distance\"]=df_temp_all.groupby(['molecule_index', \n",
    "                                                     'atom_index_0'])['dist'].transform('max')\n",
    "    \n",
    "    # 各分子で原子間距離が最小の原子のみを取り出す\n",
    "    df_temp = df_temp_all[df_temp_all[\"min_distance\"]==df_temp_all[\"dist\"]].copy()\n",
    "    # 重複を削る\n",
    "    df_temp = df_temp.drop(['x_0','y_0','z_0','min_distance'], axis=1)\n",
    "    df_temp = df_temp.rename(columns={'atom_index_0': 'atom_index',\n",
    "                                         'atom_index_1': 'atom_index_closest',\n",
    "                                         'dist': 'distance_closest',\n",
    "                                         'x_1': 'x_closest',\n",
    "                                         'y_1': 'y_closest',\n",
    "                                         'z_1': 'z_closest'})\n",
    "    # 分子名と注目原子が重複しているものは削除\n",
    "    df_temp = df_temp.drop_duplicates(subset=['molecule_index', 'atom_index'])\n",
    "    \n",
    "    for atom_idx in [0,1]:\n",
    "        # 各原子について、最近傍の原子のindex, 距離、座標の情報を追加\n",
    "        df = map_atom_info(df,df_temp, atom_idx)\n",
    "        df = df.rename(columns={'atom_index_closest': f'atom_index_closest_{atom_idx}',\n",
    "                                        'distance_closest': f'distance_closest_{atom_idx}',\n",
    "                                        'x_closest': f'x_closest_{atom_idx}',\n",
    "                                        'y_closest': f'y_closest_{atom_idx}',\n",
    "                                        'z_closest': f'z_closest_{atom_idx}'})\n",
    "    \n",
    "    # 各分子で原子間距離が最大の原子のみを取り出す\n",
    "    df_temp= df_temp_all[df_temp_all[\"max_distance\"]==df_temp_all[\"dist\"]].copy()\n",
    "    df_temp = df_temp.drop(['x_0','y_0','z_0','max_distance'], axis=1)\n",
    "    df_temp= df_temp.rename(columns={'atom_index_0': 'atom_index',\n",
    "                                         'atom_index_1': 'atom_index_farthest',\n",
    "                                         'dist': 'distance_farthest',\n",
    "                                         'x_1': 'x_farthest',\n",
    "                                         'y_1': 'y_farthest',\n",
    "                                         'z_1': 'z_farthest'})\n",
    "    # 分子名と注目原子が重複しているものは削除\n",
    "    df_temp = df_temp.drop_duplicates(subset=['molecule_index', 'atom_index'])\n",
    "        \n",
    "    for atom_idx in [0,1]:\n",
    "        # 各原子について、分子内で最も遠い原子のindex, 距離、座標の情報を追加\n",
    "        df = map_atom_info(df,df_temp, atom_idx)\n",
    "        df = df.rename(columns={'atom_index_farthest': f'atom_index_farthest_{atom_idx}',\n",
    "                                        'distance_farthest': f'distance_farthest_{atom_idx}',\n",
    "                                        'x_farthest': f'x_farthest_{atom_idx}',\n",
    "                                        'y_farthest': f'y_farthest_{atom_idx}',\n",
    "                                        'z_farthest': f'z_farthest_{atom_idx}'})\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_cos_features(df):\n",
    "    \n",
    "    # 1原子目と分子の重心との距離\n",
    "    df[\"distance_center0\"] = np.sqrt((df['x_0']-df['c_x'])**2 \\\n",
    "                                   + (df['y_0']-df['c_y'])**2 \\\n",
    "                                   + (df['z_0']-df['c_z'])**2)\n",
    "    # 2原子目と分子の重心との距離\n",
    "    df[\"distance_center1\"] = np.sqrt((df['x_1']-df['c_x'])**2 \\\n",
    "                                   + (df['y_1']-df['c_y'])**2 \\\n",
    "                                   + (df['z_1']-df['c_z'])**2)\n",
    "    # 1原子目の最近傍原子との距離\n",
    "    df['distance_c0'] = np.sqrt((df['x_0']-df['x_closest_0'])**2 + \\\n",
    "                                (df['y_0']-df['y_closest_0'])**2 + \\\n",
    "                                (df['z_0']-df['z_closest_0'])**2)\n",
    "    # 2原子目の最近傍原子との距離\n",
    "    df['distance_c1'] = np.sqrt((df['x_1']-df['x_closest_1'])**2 + \\\n",
    "                                (df['y_1']-df['y_closest_1'])**2 + \\\n",
    "                                (df['z_1']-df['z_closest_1'])**2)\n",
    "    \n",
    "    # 1原子目の最も遠い原子との距離\n",
    "    df[\"distance_f0\"] = np.sqrt((df['x_0']-df['x_farthest_0'])**2 + \\\n",
    "                                (df['y_0']-df['y_farthest_0'])**2 + \\\n",
    "                                (df['z_0']-df['z_farthest_0'])**2)\n",
    "    # 2原子目の最も遠い原子との距離\n",
    "    df[\"distance_f1\"] = np.sqrt((df['x_1']-df['x_farthest_1'])**2 + \\\n",
    "                                (df['y_1']-df['y_farthest_1'])**2 + \\\n",
    "                                (df['z_1']-df['z_farthest_1'])**2)\n",
    "    \n",
    "    # 正規化した方向ベクトル\n",
    "    #　重心→各原子\n",
    "    vec_center0_x = (df['x_0']-df['c_x'])/(df[\"distance_center0\"]+1e-10)\n",
    "    vec_center0_y = (df['y_0']-df['c_y'])/(df[\"distance_center0\"]+1e-10)\n",
    "    vec_center0_z = (df['z_0']-df['c_z'])/(df[\"distance_center0\"]+1e-10)\n",
    "    \n",
    "    vec_center1_x = (df['x_1']-df['c_x'])/(df[\"distance_center1\"]+1e-10)\n",
    "    vec_center1_y = (df['y_1']-df['c_y'])/(df[\"distance_center1\"]+1e-10)\n",
    "    vec_center1_z = (df['z_1']-df['c_z'])/(df[\"distance_center1\"]+1e-10)\n",
    "    \n",
    "    # 最近傍原子→各原子\n",
    "    vec_c0_x = (df['x_0']-df['x_closest_0'])/(df[\"distance_c0\"]+1e-10)\n",
    "    vec_c0_y = (df['y_0']-df['y_closest_0'])/(df[\"distance_c0\"]+1e-10)\n",
    "    vec_c0_z = (df['z_0']-df['z_closest_0'])/(df[\"distance_c0\"]+1e-10)\n",
    "    \n",
    "    vec_c1_x = (df['x_1']-df['x_closest_1'])/(df[\"distance_c1\"]+1e-10)\n",
    "    vec_c1_y = (df['y_1']-df['y_closest_1'])/(df[\"distance_c1\"]+1e-10)\n",
    "    vec_c1_z = (df['z_1']-df['z_closest_1'])/(df[\"distance_c1\"]+1e-10)\n",
    "    \n",
    "    # 最も遠い原子→各原子\n",
    "    vec_f0_x = (df['x_0']-df['x_farthest_0'])/(df[\"distance_f0\"]+1e-10)\n",
    "    vec_f0_y = (df['y_0']-df['y_farthest_0'])/(df[\"distance_f0\"]+1e-10)\n",
    "    vec_f0_z = (df['z_0']-df['z_farthest_0'])/(df[\"distance_f0\"]+1e-10)\n",
    "    \n",
    "    vec_f1_x = (df['x_1']-df['x_farthest_1'])/(df[\"distance_f1\"]+1e-10)\n",
    "    vec_f1_y = (df['y_1']-df['y_farthest_1'])/(df[\"distance_f1\"]+1e-10)\n",
    "    vec_f1_z = (df['z_1']-df['z_farthest_1'])/(df[\"distance_f1\"]+1e-10)\n",
    "    \n",
    "    # 2番目原子→1番目原子\n",
    "    vec_x = (df['x_1']-df['x_0'])/df['dist']\n",
    "    vec_y = (df['y_1']-df['y_0'])/df['dist']\n",
    "    vec_z = (df['z_1']-df['z_0'])/df['dist']\n",
    "    \n",
    "    # 1原子目の最近傍原子→自身のベクトルと2原子目の最近傍原子→自身のベクトルにおけるcosθ\n",
    "    # 内積/各ベクトルの大きさ＝cosθ\n",
    "    df[\"cos_c0_c1\"] = vec_c0_x*vec_c1_x + vec_c0_y*vec_c1_y + vec_c0_z*vec_c1_z\n",
    "    # 1原子目の最も遠い原子→自身のベクトルと2原子目の最も遠い原子→自身のベクトルにおけるcosθ\n",
    "    df[\"cos_f0_f1\"] = vec_f0_x*vec_f1_x + vec_f0_y*vec_f1_y + vec_f0_z*vec_f1_z\n",
    "    \n",
    "    # 1原子目の最近傍原子→自身のベクトルと最も遠い原子→自身のベクトルにおけるcosθ\n",
    "    df[\"cos_c0_f0\"] = vec_c0_x*vec_f0_x + vec_c0_y*vec_f0_y + vec_c0_z*vec_f0_z\n",
    "    # 2原子目の...\n",
    "    df[\"cos_c1_f1\"] = vec_c1_x*vec_f1_x + vec_c1_y*vec_f1_y + vec_c1_z*vec_f1_z\n",
    "    \n",
    "    # 1原子目の重心→自身のベクトルと2原子目の重心→自身のベクトルにおけるcosθ\n",
    "    df[\"cos_center0_center1\"] = vec_center0_x*vec_center1_x \\\n",
    "                              + vec_center0_y*vec_center1_y \\\n",
    "                              + vec_center0_z*vec_center1_z\n",
    "    \n",
    "    # 1原子目の最近傍原子→自身のベクトルと2原子目→1原子目のベクトルにおけるcosθ\n",
    "    df[\"cos_c0\"] = vec_c0_x*vec_x + vec_c0_y*vec_y + vec_c0_z*vec_z\n",
    "    # 2原子目の最近傍原子→自身のベクトルと2原子目→1原子目のベクトルにおけるcosθ\n",
    "    df[\"cos_c1\"] = vec_c1_x*vec_x + vec_c1_y*vec_y + vec_c1_z*vec_z\n",
    "    \n",
    "    # 1原子目の最も遠い原子→自身のベクトルと2原子目→1原子目のベクトルにおけるcosθ\n",
    "    df[\"cos_f0\"] = vec_f0_x*vec_x + vec_f0_y*vec_y + vec_f0_z*vec_z\n",
    "    # 2原子目の最も遠い原子→自身のベクトルと2原子目→1原子目のベクトルにおけるcosθ\n",
    "    df[\"cos_f1\"] = vec_f1_x*vec_x + vec_f1_y*vec_y + vec_f1_z*vec_z\n",
    "    \n",
    "    # 1原子目の重心→自身のベクトルと2原子目→1原子目のベクトルにおけるcosθ\n",
    "    df[\"cos_center0\"] = vec_center0_x*vec_x + vec_center0_y*vec_y + vec_center0_z*vec_z\n",
    "    # 2原子目の重心→自身のベクトルと2原子目→1原子目のベクトルにおけるcosθ\n",
    "    df[\"cos_center1\"] = vec_center1_x*vec_x + vec_center1_y*vec_y + vec_center1_z*vec_z\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_dist_features(df):\n",
    "    # Andrew's features selected\n",
    "    # ある原子(atom_index_0)からの距離の平均\n",
    "    df[f'molecule_atom_index_0_dist_mean'] = df.groupby(['molecule_index', 'atom_index_0'])['dist'].transform('mean')\n",
    "    # 上の平均との差\n",
    "    df[f'molecule_atom_index_0_dist_mean_diff'] = df[f'molecule_atom_index_0_dist_mean'] - df['dist']\n",
    "    # ある原子(atom_index_0)からの距離の最小値\n",
    "    df[f'molecule_atom_index_0_dist_min'] = df.groupby(['molecule_index', 'atom_index_0'])['dist'].transform('min')\n",
    "    #　上の最小値との差\n",
    "    df[f'molecule_atom_index_0_dist_min_diff'] = df[f'molecule_atom_index_0_dist_min'] - df['dist']\n",
    "    # ある原子(atom_index_0)からの距離の標準誤差\n",
    "    df[f'molecule_atom_index_0_dist_std'] = df.groupby(['molecule_index', 'atom_index_0'])['dist'].transform('std')\n",
    "\n",
    "    df[f'molecule_atom_index_1_dist_mean'] = df.groupby(['molecule_index', 'atom_index_1'])['dist'].transform('mean')\n",
    "    df[f'molecule_atom_index_1_dist_mean_diff'] = df[f'molecule_atom_index_1_dist_mean'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_min'] = df.groupby(['molecule_index', 'atom_index_1'])['dist'].transform('min')\n",
    "    df[f'molecule_atom_index_1_dist_min_diff'] = df[f'molecule_atom_index_1_dist_min'] - df['dist']\n",
    "    df[f'molecule_atom_index_1_dist_std'] = df.groupby(['molecule_index', 'atom_index_1'])['dist'].transform('std')\n",
    "    \n",
    "    # あるタイプにおける注目2原子の距離の平均\n",
    "    df[f'molecule_type_dist_mean'] = df.groupby(['molecule_index', 'type'])['dist'].transform('mean')\n",
    "    # 上の平均との差\n",
    "    df[f'molecule_type_dist_mean_diff'] = df[f'molecule_type_dist_mean'] - df['dist']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def dummies(df, list_cols):\n",
    "    for col in list_cols:\n",
    "        # ダミー変数作成、列名は元のまま\n",
    "        df_dummies = pd.get_dummies(df[col], drop_first=True, \n",
    "                                    prefix=(str(col)))\n",
    "        df = pd.concat([df, df_dummies], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_qm9_features(df):\n",
    "    data_qm9 = pd.read_pickle('quantum-machine-9-qm9/data.covs.pickle')\n",
    "    data_qm9['molecule_index'] = data_qm9['molecule_name'].str.replace('dsgdb9nsd_', ' ').astype('int32')\n",
    "    to_drop = ['type', 'molecule_name',\n",
    "               'linear', \n",
    "               'atom_index_0', \n",
    "               'atom_index_1', \n",
    "               'scalar_coupling_constant', \n",
    "               'U', 'G', 'H', \n",
    "               'mulliken_mean', 'r2', 'U0']\n",
    "    data_qm9 = data_qm9.drop(columns = to_drop, axis=1)\n",
    "    data_qm9 = reduce_mem_usage(data_qm9,verbose=False)\n",
    "    df = pd.merge(df, data_qm9, how='left', on=['molecule_index','id'])\n",
    "    del data_qm9\n",
    "    \n",
    "    df = dummies(df, ['type', 'atom_1'])\n",
    "    return df\n",
    "\n",
    "def get_features(df, struct):\n",
    "    for atom_idx in [0,1]:\n",
    "        df = map_atom_info(df, struct, atom_idx)\n",
    "        df = df.rename(columns={'atom': f'atom_{atom_idx}',\n",
    "                            'x': f'x_{atom_idx}',\n",
    "                            'y': f'y_{atom_idx}',\n",
    "                            'z': f'z_{atom_idx}'})\n",
    "        # 分子の各軸の重心\n",
    "        struct['c_x'] = struct.groupby('molecule_index')['x'].transform('mean')\n",
    "        struct['c_y'] = struct.groupby('molecule_index')['y'].transform('mean')\n",
    "        struct['c_z'] = struct.groupby('molecule_index')['z'].transform('mean')\n",
    "\n",
    "    # 特徴量追加\n",
    "    df = find_dist(df)\n",
    "    df = find_closest_atom(df)\n",
    "    df = add_cos_features(df)\n",
    "    df = add_dist_features(df)\n",
    "    df = add_qm9_features(df)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "good_columns = ['type',\n",
    " 'dist_C_0_x',\n",
    " 'dist_C_1_x',\n",
    " 'dist_C_2_x',\n",
    " 'dist_C_3_x',\n",
    " 'dist_C_4_x',\n",
    " 'dist_F_0_x',\n",
    " 'dist_F_1_x',\n",
    " 'dist_F_2_x',\n",
    " 'dist_F_3_x',\n",
    " 'dist_F_4_x',\n",
    " 'dist_H_0_x',\n",
    " 'dist_H_1_x',\n",
    " 'dist_H_2_x',\n",
    " 'dist_H_3_x',\n",
    " 'dist_H_4_x',\n",
    " 'dist_N_0_x',\n",
    " 'dist_N_1_x',\n",
    " 'dist_N_2_x',\n",
    " 'dist_N_3_x',\n",
    " 'dist_N_4_x',\n",
    " 'dist_O_0_x',\n",
    " 'dist_O_1_x',\n",
    " 'dist_O_2_x',\n",
    " 'dist_O_3_x',\n",
    " 'dist_O_4_x',\n",
    " 'dist_C_0_y',\n",
    " 'dist_C_1_y',\n",
    " 'dist_C_2_y',\n",
    " 'dist_C_3_y',\n",
    " 'dist_C_4_y',\n",
    " 'dist_F_0_y',\n",
    " 'dist_F_1_y',\n",
    " 'dist_F_2_y',\n",
    " 'dist_F_3_y',\n",
    " 'dist_F_4_y',\n",
    " 'dist_H_0_y',\n",
    " 'dist_H_1_y',\n",
    " 'dist_H_2_y',\n",
    " 'dist_H_3_y',\n",
    " 'dist_H_4_y',\n",
    " 'dist_N_0_y',\n",
    " 'dist_N_1_y',\n",
    " 'dist_N_2_y',\n",
    " 'dist_N_3_y',\n",
    " 'dist_N_4_y',\n",
    " 'dist_O_0_y',\n",
    " 'dist_O_1_y',\n",
    " 'dist_O_2_y',\n",
    " 'dist_O_3_y',\n",
    " 'dist_O_4_y',\n",
    " 'dist_inv2',\n",
    " 'distance_closest_0',\n",
    " 'distance_closest_1',\n",
    " 'distance_farthest_0',\n",
    " 'distance_farthest_1',\n",
    " 'cos_c0_c1', 'cos_f0_f1','cos_c0_f0', 'cos_c1_f1',\n",
    " 'cos_center0_center1', 'cos_c0', 'cos_c1', 'cos_f0', 'cos_f1',\n",
    " 'cos_center0', 'cos_center1',\n",
    " 'molecule_atom_index_0_dist_mean',\n",
    " 'molecule_atom_index_0_dist_mean_diff',\n",
    " 'molecule_atom_index_0_dist_min',\n",
    " 'molecule_atom_index_0_dist_min_diff',\n",
    " 'molecule_atom_index_0_dist_std',\n",
    " 'molecule_atom_index_1_dist_mean',\n",
    " 'molecule_atom_index_1_dist_mean_diff',\n",
    " 'molecule_atom_index_1_dist_min',\n",
    " 'molecule_atom_index_1_dist_min_diff',\n",
    " 'molecule_atom_index_1_dist_std',\n",
    " # 'molecule_type_dist_mean',\n",
    " # 'molecule_type_dist_mean_diff',\n",
    " 'rc_A',\n",
    " 'rc_B',\n",
    " 'rc_C',\n",
    " 'mu',\n",
    " 'alpha',\n",
    " 'homo',\n",
    " 'lumo',\n",
    " 'gap',\n",
    " 'zpve',\n",
    " 'Cv',\n",
    " 'freqs_min',\n",
    " 'freqs_max',\n",
    " 'freqs_mean',\n",
    " 'mulliken_min',\n",
    " 'mulliken_max',\n",
    " 'mulliken_atom_0',\n",
    " 'mulliken_atom_1']\n",
    "\n",
    "giba_columns = ['inv_dist0',\n",
    " 'inv_dist1',\n",
    " 'inv_distP',\n",
    " 'inv_dist0R',\n",
    " 'inv_dist1R',\n",
    " 'inv_distPR',\n",
    " 'inv_dist0E',\n",
    " 'inv_dist1E',\n",
    " 'inv_distPE',\n",
    " 'linkM0',\n",
    " 'linkM1',\n",
    " 'min_molecule_atom_0_dist_xyz',\n",
    " 'mean_molecule_atom_0_dist_xyz',\n",
    " 'max_molecule_atom_0_dist_xyz',\n",
    " 'sd_molecule_atom_0_dist_xyz',\n",
    " 'min_molecule_atom_1_dist_xyz',\n",
    " 'mean_molecule_atom_1_dist_xyz',\n",
    " 'max_molecule_atom_1_dist_xyz',\n",
    " 'sd_molecule_atom_1_dist_xyz',\n",
    " 'coulomb_C.x',\n",
    " 'coulomb_F.x',\n",
    " 'coulomb_H.x',\n",
    " 'coulomb_N.x',\n",
    " 'coulomb_O.x',\n",
    " 'yukawa_C.x',\n",
    " 'yukawa_F.x',\n",
    " 'yukawa_H.x',\n",
    " 'yukawa_N.x',\n",
    " 'yukawa_O.x',\n",
    " 'vander_C.x',\n",
    " 'vander_F.x',\n",
    " 'vander_H.x',\n",
    " 'vander_N.x',\n",
    " 'vander_O.x',\n",
    " 'coulomb_C.y',\n",
    " 'coulomb_F.y',\n",
    " 'coulomb_H.y',\n",
    " 'coulomb_N.y',\n",
    " 'coulomb_O.y',\n",
    " 'yukawa_C.y',\n",
    " 'yukawa_F.y',\n",
    " 'yukawa_H.y',\n",
    " 'yukawa_N.y',\n",
    " 'yukawa_O.y',\n",
    " 'vander_C.y',\n",
    " 'vander_F.y',\n",
    " 'vander_H.y',\n",
    " 'vander_N.y',\n",
    " 'vander_O.y',\n",
    " 'distC0',\n",
    " 'distH0',\n",
    " 'distN0',\n",
    " 'distC1',\n",
    " 'distH1',\n",
    " 'distN1',\n",
    " 'adH1',\n",
    " 'adH2',\n",
    " 'adH3',\n",
    " 'adH4',\n",
    " 'adC1',\n",
    " 'adC2',\n",
    " 'adC3',\n",
    " 'adC4',\n",
    " 'adN1',\n",
    " 'adN2',\n",
    " 'adN3',\n",
    " 'adN4',\n",
    " 'NC',\n",
    " 'NH',\n",
    " 'NN',\n",
    " 'NF',\n",
    " 'NO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'atoms' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-e53f70f8b365>\u001b[0m in \u001b[0;36mbuild_couple_dataframe\u001b[0;34m(some_csv, structures_csv, coupling_type, n_atoms)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0matoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matoms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'scalar_coupling_constant'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0madd_center\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matoms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0matoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matoms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y_0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'z_0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x_1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y_1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'z_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'atoms' referenced before assignment"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "full = build_couple_dataframe(train, structures, 1, n_atoms=10)\n",
    "print(full.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# typeをlabel encode\n",
    "lbl = LabelEncoder()\n",
    "lbl.fit(list(train['type'].values) + list(test['type'].values))\n",
    "train['type'] = lbl.transform(list(train['type'].values))\n",
    "test['type'] = lbl.transform(list(test['type'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training by type with time seed\n",
    "We use different numbers of iterations for different type, after running the label encoder\n",
    "```\n",
    "train['type'].unique() = [0, 3, 1, 4, 2, 6, 5, 7]\n",
    "```\n",
    "Hence the current the number of iteration `N` config (in order) is:\n",
    "\n",
    "> Type 0 = `1JHC`. <br>\n",
    "> Type 1 = `1JHN`. <br>\n",
    "> Type 2 = `2JHC`. <br>\n",
    "> Type 3 = `2JHH`. <br>\n",
    "> Type 4 = `2JHN`. <br>\n",
    "> Type 5 = `3JHC`. <br>\n",
    "> Type 6 = `3JHH`. <br>\n",
    "> Type 7 = `3JHN`. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fold = 5\n",
    "# 時間ごとにseed値を変更\n",
    "seed = round(time.time())\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
    "\n",
    "params = {'num_leaves': 400,\n",
    "          'objective': 'huber',# 誤差が大きい場合は二乗誤差ではなく絶対誤差を考える\n",
    "          'max_depth': 9,\n",
    "          'learning_rate': 0.12,\n",
    "          \"boosting_type\": \"gbdt\",\n",
    "          \"subsample_freq\": 1,\n",
    "          \"subsample\": 0.8,\n",
    "          \"metric\": 'mae',\n",
    "          \"verbosity\": -1,\n",
    "          'lambda_l1': 0.8,\n",
    "          'lambda_l2': 0.2,\n",
    "          'feature_fraction': 0.6,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_short = pd.DataFrame({'ind': list(train.index), \n",
    "                        'type': train['type'].values,\n",
    "                        'oof': [0] * len(train), \n",
    "                        'target': y.values})\n",
    "X_short_test = pd.DataFrame({'ind': list(test.index), \n",
    "                             'type': test['type'].values, \n",
    "                             'prediction': [0] * len(test)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training of type 0: 0.\n",
      "Generating features...\n",
      "Done in 131.23 seconds for 168 features.\n",
      "Fold 1 started at Sat Aug 10 23:38:17 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.492667\tvalid_1's l1: 0.792688\n",
      "[4000]\ttraining's l1: 0.223089\tvalid_1's l1: 0.69096\n",
      "[6000]\ttraining's l1: 0.111362\tvalid_1's l1: 0.663107\n",
      "[8000]\ttraining's l1: 0.0593161\tvalid_1's l1: 0.653761\n",
      "[10000]\ttraining's l1: 0.0338727\tvalid_1's l1: 0.650104\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's l1: 0.0338727\tvalid_1's l1: 0.650104\n",
      "Fold 2 started at Sun Aug 11 00:02:51 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.500719\tvalid_1's l1: 0.792633\n",
      "[4000]\ttraining's l1: 0.225408\tvalid_1's l1: 0.687664\n",
      "[6000]\ttraining's l1: 0.111545\tvalid_1's l1: 0.659774\n",
      "[8000]\ttraining's l1: 0.0585975\tvalid_1's l1: 0.650183\n",
      "[10000]\ttraining's l1: 0.0336911\tvalid_1's l1: 0.646669\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's l1: 0.0336911\tvalid_1's l1: 0.646669\n",
      "Fold 3 started at Sun Aug 11 06:05:01 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.500035\tvalid_1's l1: 0.796036\n",
      "[4000]\ttraining's l1: 0.223903\tvalid_1's l1: 0.690722\n",
      "[6000]\ttraining's l1: 0.111078\tvalid_1's l1: 0.663485\n",
      "[8000]\ttraining's l1: 0.058869\tvalid_1's l1: 0.654663\n",
      "[10000]\ttraining's l1: 0.034151\tvalid_1's l1: 0.651312\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's l1: 0.034151\tvalid_1's l1: 0.651312\n",
      "Fold 4 started at Sun Aug 11 06:33:10 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.501333\tvalid_1's l1: 0.794659\n",
      "[4000]\ttraining's l1: 0.223531\tvalid_1's l1: 0.690613\n",
      "[6000]\ttraining's l1: 0.11077\tvalid_1's l1: 0.663256\n",
      "[8000]\ttraining's l1: 0.0583154\tvalid_1's l1: 0.653822\n",
      "[10000]\ttraining's l1: 0.0330972\tvalid_1's l1: 0.650339\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's l1: 0.0330972\tvalid_1's l1: 0.650339\n",
      "Fold 5 started at Sun Aug 11 06:55:38 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.484117\tvalid_1's l1: 0.782853\n",
      "[4000]\ttraining's l1: 0.21959\tvalid_1's l1: 0.684713\n",
      "[6000]\ttraining's l1: 0.109758\tvalid_1's l1: 0.658347\n",
      "[8000]\ttraining's l1: 0.059109\tvalid_1's l1: 0.649375\n",
      "[10000]\ttraining's l1: 0.0349049\tvalid_1's l1: 0.645788\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[10000]\ttraining's l1: 0.0349049\tvalid_1's l1: 0.645788\n",
      "CV mean score: 0.6488, std: 0.0022.\n",
      "\n",
      "Training of type 3: 3.\n",
      "Generating features...\n",
      "Done in 100.32 seconds for 168 features.\n",
      "Fold 1 started at Sun Aug 11 07:19:27 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.0401944\tvalid_1's l1: 0.166554\n",
      "[4000]\ttraining's l1: 0.0165315\tvalid_1's l1: 0.162636\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.012572\tvalid_1's l1: 0.162148\n",
      "Fold 2 started at Sun Aug 11 07:26:54 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.0403818\tvalid_1's l1: 0.165477\n",
      "[4000]\ttraining's l1: 0.0160913\tvalid_1's l1: 0.161653\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.0122757\tvalid_1's l1: 0.161188\n",
      "Fold 3 started at Sun Aug 11 07:34:20 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.040173\tvalid_1's l1: 0.165583\n",
      "[4000]\ttraining's l1: 0.0162597\tvalid_1's l1: 0.161722\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.0122833\tvalid_1's l1: 0.161244\n",
      "Fold 4 started at Sun Aug 11 07:41:44 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.0406098\tvalid_1's l1: 0.167075\n",
      "[4000]\ttraining's l1: 0.016406\tvalid_1's l1: 0.163171\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.0124721\tvalid_1's l1: 0.16272\n",
      "Fold 5 started at Sun Aug 11 07:49:04 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.0403864\tvalid_1's l1: 0.165615\n",
      "[4000]\ttraining's l1: 0.0163059\tvalid_1's l1: 0.1618\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.0123919\tvalid_1's l1: 0.161313\n",
      "CV mean score: 0.1617, std: 0.0006.\n",
      "\n",
      "Training of type 1: 1.\n",
      "Generating features...\n",
      "Done in 71.57 seconds for 168 features.\n",
      "Fold 1 started at Sun Aug 11 07:57:33 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.0147832\tvalid_1's l1: 0.34881\n",
      "[4000]\ttraining's l1: 0.00864803\tvalid_1's l1: 0.348128\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.00764812\tvalid_1's l1: 0.348039\n",
      "Fold 2 started at Sun Aug 11 07:58:33 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.0149886\tvalid_1's l1: 0.360049\n",
      "[4000]\ttraining's l1: 0.00863983\tvalid_1's l1: 0.35928\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.00769912\tvalid_1's l1: 0.359187\n",
      "Fold 3 started at Sun Aug 11 07:59:34 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.0143619\tvalid_1's l1: 0.357795\n",
      "[4000]\ttraining's l1: 0.00861204\tvalid_1's l1: 0.357128\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.00761663\tvalid_1's l1: 0.357002\n",
      "Fold 4 started at Sun Aug 11 08:00:33 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.0146329\tvalid_1's l1: 0.361918\n",
      "[4000]\ttraining's l1: 0.00850413\tvalid_1's l1: 0.361305\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.00753151\tvalid_1's l1: 0.361211\n",
      "Fold 5 started at Sun Aug 11 08:01:32 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.0148763\tvalid_1's l1: 0.355103\n",
      "[4000]\ttraining's l1: 0.00879486\tvalid_1's l1: 0.354303\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.00779681\tvalid_1's l1: 0.354171\n",
      "CV mean score: 0.3559, std: 0.0046.\n",
      "\n",
      "Training of type 4: 4.\n",
      "Generating features...\n",
      "Done in 77.45 seconds for 168 features.\n",
      "Fold 1 started at Sun Aug 11 08:03:49 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.0171245\tvalid_1's l1: 0.212907\n",
      "[4000]\ttraining's l1: 0.00908251\tvalid_1's l1: 0.21191\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.00793926\tvalid_1's l1: 0.211792\n",
      "Fold 2 started at Sun Aug 11 08:06:23 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.0176907\tvalid_1's l1: 0.210021\n",
      "[4000]\ttraining's l1: 0.00911659\tvalid_1's l1: 0.209098\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.00787946\tvalid_1's l1: 0.208958\n",
      "Fold 3 started at Sun Aug 11 08:08:56 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.0173986\tvalid_1's l1: 0.215655\n",
      "[4000]\ttraining's l1: 0.00909768\tvalid_1's l1: 0.214689\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.00790034\tvalid_1's l1: 0.214546\n",
      "Fold 4 started at Sun Aug 11 08:11:28 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.0173671\tvalid_1's l1: 0.214665\n",
      "[4000]\ttraining's l1: 0.009109\tvalid_1's l1: 0.213717\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.00788014\tvalid_1's l1: 0.213584\n",
      "Fold 5 started at Sun Aug 11 08:14:00 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.0173398\tvalid_1's l1: 0.220642\n",
      "[4000]\ttraining's l1: 0.00907782\tvalid_1's l1: 0.219585\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.00787549\tvalid_1's l1: 0.219447\n",
      "CV mean score: 0.2137, std: 0.0035.\n",
      "\n",
      "Training of type 2: 2.\n",
      "Generating features...\n",
      "Done in 163.69 seconds for 168 features.\n",
      "Fold 1 started at Sun Aug 11 08:19:15 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.204143\tvalid_1's l1: 0.347106\n",
      "[4000]\ttraining's l1: 0.106257\tvalid_1's l1: 0.313927\n",
      "[6000]\ttraining's l1: 0.0617668\tvalid_1's l1: 0.303984\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0617668\tvalid_1's l1: 0.303984\n",
      "Fold 2 started at Sun Aug 11 08:39:23 2019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.204593\tvalid_1's l1: 0.349139\n",
      "[4000]\ttraining's l1: 0.106148\tvalid_1's l1: 0.316033\n",
      "[6000]\ttraining's l1: 0.06189\tvalid_1's l1: 0.305915\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.06189\tvalid_1's l1: 0.305915\n",
      "Fold 3 started at Sun Aug 11 08:59:15 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.204936\tvalid_1's l1: 0.346362\n",
      "[4000]\ttraining's l1: 0.106998\tvalid_1's l1: 0.314038\n",
      "[6000]\ttraining's l1: 0.0621063\tvalid_1's l1: 0.304094\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0621063\tvalid_1's l1: 0.304094\n",
      "Fold 4 started at Sun Aug 11 09:19:00 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.203971\tvalid_1's l1: 0.34591\n",
      "[4000]\ttraining's l1: 0.106103\tvalid_1's l1: 0.312979\n",
      "[6000]\ttraining's l1: 0.0619804\tvalid_1's l1: 0.302971\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0619804\tvalid_1's l1: 0.302971\n",
      "Fold 5 started at Sun Aug 11 09:38:54 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.203339\tvalid_1's l1: 0.348081\n",
      "[4000]\ttraining's l1: 0.105366\tvalid_1's l1: 0.315371\n",
      "[6000]\ttraining's l1: 0.0612633\tvalid_1's l1: 0.305449\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0612633\tvalid_1's l1: 0.305449\n",
      "CV mean score: 0.3045, std: 0.0011.\n",
      "\n",
      "Training of type 6: 6.\n",
      "Generating features...\n",
      "Done in 113.47 seconds for 168 features.\n",
      "Fold 1 started at Sun Aug 11 10:00:45 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.0722472\tvalid_1's l1: 0.211321\n",
      "[4000]\ttraining's l1: 0.0291947\tvalid_1's l1: 0.202606\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.0209835\tvalid_1's l1: 0.201396\n",
      "Fold 2 started at Sun Aug 11 10:11:05 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.0719493\tvalid_1's l1: 0.210225\n",
      "[4000]\ttraining's l1: 0.0291737\tvalid_1's l1: 0.201564\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.0210639\tvalid_1's l1: 0.20035\n",
      "Fold 3 started at Sun Aug 11 10:21:21 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.0719678\tvalid_1's l1: 0.210942\n",
      "[4000]\ttraining's l1: 0.0290916\tvalid_1's l1: 0.202309\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.0209207\tvalid_1's l1: 0.201096\n",
      "Fold 4 started at Sun Aug 11 10:31:44 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.0713992\tvalid_1's l1: 0.210708\n",
      "[4000]\ttraining's l1: 0.0290984\tvalid_1's l1: 0.202139\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.0209851\tvalid_1's l1: 0.200949\n",
      "Fold 5 started at Sun Aug 11 10:42:01 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.07229\tvalid_1's l1: 0.212135\n",
      "[4000]\ttraining's l1: 0.0293296\tvalid_1's l1: 0.203514\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.0211092\tvalid_1's l1: 0.202295\n",
      "CV mean score: 0.2012, std: 0.0006.\n",
      "\n",
      "Training of type 5: 5.\n",
      "Generating features...\n",
      "Done in 190.07 seconds for 168 features.\n",
      "Fold 1 started at Sun Aug 11 10:55:29 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.271905\tvalid_1's l1: 0.432013\n",
      "[4000]\ttraining's l1: 0.151024\tvalid_1's l1: 0.389523\n",
      "[6000]\ttraining's l1: 0.0926159\tvalid_1's l1: 0.375443\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0926159\tvalid_1's l1: 0.375443\n",
      "Fold 2 started at Sun Aug 11 11:22:42 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.274776\tvalid_1's l1: 0.436362\n",
      "[4000]\ttraining's l1: 0.152063\tvalid_1's l1: 0.391538\n",
      "[6000]\ttraining's l1: 0.0932822\tvalid_1's l1: 0.37715\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0932822\tvalid_1's l1: 0.37715\n",
      "Fold 3 started at Sun Aug 11 11:50:01 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.27287\tvalid_1's l1: 0.434733\n",
      "[4000]\ttraining's l1: 0.151598\tvalid_1's l1: 0.391111\n",
      "[6000]\ttraining's l1: 0.0929367\tvalid_1's l1: 0.377011\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0929367\tvalid_1's l1: 0.377011\n",
      "Fold 4 started at Sun Aug 11 12:17:26 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.273888\tvalid_1's l1: 0.432776\n",
      "[4000]\ttraining's l1: 0.151839\tvalid_1's l1: 0.389855\n",
      "[6000]\ttraining's l1: 0.0927332\tvalid_1's l1: 0.375727\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0927332\tvalid_1's l1: 0.375727\n",
      "Fold 5 started at Sun Aug 11 12:44:38 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.272599\tvalid_1's l1: 0.434138\n",
      "[4000]\ttraining's l1: 0.1519\tvalid_1's l1: 0.390828\n",
      "[6000]\ttraining's l1: 0.0925703\tvalid_1's l1: 0.376483\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[6000]\ttraining's l1: 0.0925703\tvalid_1's l1: 0.376483\n",
      "CV mean score: 0.3764, std: 0.0007.\n",
      "\n",
      "Training of type 7: 7.\n",
      "Generating features...\n",
      "Done in 78.83 seconds for 168 features.\n",
      "Fold 1 started at Sun Aug 11 13:13:20 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.0161068\tvalid_1's l1: 0.168607\n",
      "[4000]\ttraining's l1: 0.00858519\tvalid_1's l1: 0.167596\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.00745683\tvalid_1's l1: 0.167464\n",
      "Fold 2 started at Sun Aug 11 13:17:01 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.0159812\tvalid_1's l1: 0.169693\n",
      "[4000]\ttraining's l1: 0.00857942\tvalid_1's l1: 0.168696\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.00745894\tvalid_1's l1: 0.168559\n",
      "Fold 3 started at Sun Aug 11 13:20:37 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.0159145\tvalid_1's l1: 0.16629\n",
      "[4000]\ttraining's l1: 0.00858302\tvalid_1's l1: 0.165385\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.00746584\tvalid_1's l1: 0.165246\n",
      "Fold 4 started at Sun Aug 11 13:24:12 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.015967\tvalid_1's l1: 0.16781\n",
      "[4000]\ttraining's l1: 0.00857458\tvalid_1's l1: 0.166823\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.00746203\tvalid_1's l1: 0.166688\n",
      "Fold 5 started at Sun Aug 11 13:27:45 2019\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[2000]\ttraining's l1: 0.0160015\tvalid_1's l1: 0.169236\n",
      "[4000]\ttraining's l1: 0.00860253\tvalid_1's l1: 0.168253\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\ttraining's l1: 0.00748114\tvalid_1's l1: 0.168121\n",
      "CV mean score: 0.1672, std: 0.0012.\n",
      "CPU times: user 2d 12h 12min 49s, sys: 1h 43min 19s, total: 2d 13h 56min 9s\n",
      "Wall time: 13h 55min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CV_score = 0\n",
    "###Iters###    [1JHC, 1JHN, 2JHC, 2JHH, 2JHN, 3JHC, 3JHH, 3JHN]\n",
    "n_estimators = [10000, 5000, 6000, 5000, 5000, 6000, 5000, 5000]\n",
    "\n",
    "\n",
    "for t in train['type'].unique():\n",
    "    # typeの最初の数字を取得\n",
    "    type_ = lbl.inverse_transform([t])[0]\n",
    "    print(f'\\nTraining of type {t}: {type_}.')\n",
    "    # 注目typeのindex取得\n",
    "    index_type = (train['type'] == t)\n",
    "    index_type_test = (test['type'] == t)\n",
    "    \n",
    "    X_t = train.loc[index_type].copy()\n",
    "    X_test_t = test.loc[index_type_test].copy()\n",
    "    y_t = y[index_type]\n",
    "    \n",
    "    print(f'Generating features...')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    ## Generating features from the public kernels, just by type\n",
    "    ## no memory reduction is needed\n",
    "    X_t = get_features(X_t, structures.copy())\n",
    "    X_t = X_t[good_columns].fillna(0.0)\n",
    "    \n",
    "    X_test_t = get_features(X_test_t, structures.copy())\n",
    "    X_test_t = X_test_t[good_columns].fillna(0.0)\n",
    "    \n",
    "    ## load Giba's features just for type t by getting rows to be excluded when initiating read_csv\n",
    "    rows_to_exclude = np.where(index_type==False)[0]+1 # retain the header row\n",
    "    rows_to_exclude_test = np.where(index_type_test==False)[0]+1\n",
    "    train_giba_t = pd.read_csv('giba-molecular-features/train_giba.csv',\n",
    "                        header=0, skiprows=rows_to_exclude, usecols=giba_columns)\n",
    "    test_giba_t = pd.read_csv('giba-molecular-features/test_giba.csv',\n",
    "                       header=0, skiprows=rows_to_exclude_test, usecols=giba_columns)\n",
    "    \n",
    "    X_t = pd.concat((X_t, train_giba_t), axis=1)\n",
    "\n",
    "    X_test_t = pd.concat((X_test_t,test_giba_t), axis=1) \n",
    "    \n",
    "    del train_giba_t, test_giba_t\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f'Done in {(time.time() - start_time):.2f} seconds for {X_t.shape[1]} features.')\n",
    "    ## feature generation done\n",
    "    \n",
    "    \n",
    "    result_dict_lgb = train_model_regression(X=X_t, X_test=X_test_t, \n",
    "                                              y=y_t, params=params, \n",
    "                                              folds=folds, \n",
    "                                              model_type='lgb', \n",
    "                                              eval_metric='mae', \n",
    "                                              plot_feature_importance=False,\n",
    "                                              verbose=2000, early_stopping_rounds=200, \n",
    "                                              n_estimators=n_estimators[t])\n",
    "    del X_t, X_test_t\n",
    "    gc.collect()\n",
    "    \n",
    "    X_short.loc[X_short['type'] == t, 'oof'] = result_dict_lgb['oof']\n",
    "    X_short_test.loc[X_short_test['type'] == t, 'prediction'] = result_dict_lgb['prediction']\n",
    "    \n",
    "    ## manually computing the cv score\n",
    "    CV_score += np.log(np.array(result_dict_lgb['scores']).mean())/8 # total 8 types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['scalar_coupling_constant'] = X_short_test['prediction']\n",
    "today = str(datetime.date.today())\n",
    "sub.to_csv(f'LGB_{today}_{CV_score:.4f}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
